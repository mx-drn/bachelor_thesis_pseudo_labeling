{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "first_implementation_of_BERT_on_kaggle_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tcuqFPNNMzaq",
        "m32Ndw0wMzar",
        "qzUqxLvuMzas",
        "dwPSGTb2bIA_",
        "eHa6eFazbgbx",
        "tZ4SBdmxbpua",
        "0k9CLe9PbyqG",
        "8W9fZSmS0LIC",
        "oaBx7Lsa3NxS",
        "opPjR9d_cJaC",
        "fDI95MW1eize",
        "lvakzoK-cQuP",
        "HYnMDrgtzaWt",
        "rFMWLRx41d-4",
        "GABnMc4a10tP",
        "gBzofDQZ18Ey",
        "AzpqoPt5_qk-"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mx-drn/bachelor_thesis_pseudo_labeling/blob/main/first_implementation_of_BERT_on_kaggle_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hZO7R9TMzaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc182a05-3909-4070-8097-f3e00c65b905"
      },
      "source": [
        "!pip install transformers\n",
        "#!pip install wandb\n",
        "!pip install easynmt\n",
        "#!pip install pydeepl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: easynmt in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: transformers<5,>=4.4 in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.19.5)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (from easynmt) (0.9.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from easynmt) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->easynmt) (3.7.4.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (4.6.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5,>=4.4->easynmt) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (57.2.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (2.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=4.4->easynmt) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5,>=4.4->easynmt) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5,>=4.4->easynmt) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLNtikhwX99e"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCt7gXW-i9yc"
      },
      "source": [
        "#print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMT5LWhkMzal"
      },
      "source": [
        "# needed imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import time\n",
        "#import wandb\n",
        "import pickle\n",
        "from easynmt import EasyNMT\n",
        "import requests\n",
        "import copy\n",
        "import collections\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from textwrap import wrap\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings, BertPooler, BertLayer\n",
        "#import pytorch_lightning as pl\n",
        "\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZ7vslbMzal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef22a19-cb72-47fd-beda-436ae43a922d"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# Use GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 12 10:49:57 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRiZccwMzam"
      },
      "source": [
        "# Get an Overview over the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMmaEOMh1Fdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570de918-429d-4acc-e052-37d9bf667208"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MIORSrKMzam",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0010c9d-6a56-4c70-9b2a-d626c2abbbb4"
      },
      "source": [
        "# import data\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Data/kaggle_data.csv\", names=['label', 'article'], encoding='latin-1')\n",
        "\n",
        "print(\"Overall distribution of the data and labels:\")\n",
        "print(data['label'].value_counts(ascending=True))\n",
        "print()\n",
        "\n",
        "all_lens = {\n",
        "    'positive': [],\n",
        "    'negative': [],\n",
        "    'neutral': []\n",
        "}\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    all_lens[row['label']].append(len(row['article']))\n",
        "print(\"General dataset size: \" + str(len(data)))\n",
        "\n",
        "print(\"\\nPositive Sentiment Mean Article Length: \" + str(sum(all_lens['positive'])/len(all_lens['positive'])))\n",
        "print(\"Positive Sentiment Max Article Length: \" + str(max(all_lens['positive'])))\n",
        "print()\n",
        "print(\"Neutral Sentiment Mean Article Length: \" + str(sum(all_lens['neutral'])/len(all_lens['neutral'])))\n",
        "print(\"Neutral Sentiment Max Article Length: \" + str(max(all_lens['neutral'])))\n",
        "print()\n",
        "print(\"Negative Sentiment Mean Article Length: \" + str(sum(all_lens['negative'])/len(all_lens['negative'])))\n",
        "print(\"Negative Sentiment Max Article Length: \" + str(max(all_lens['negative'])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall distribution of the data and labels:\n",
            "negative     604\n",
            "positive    1363\n",
            "neutral     2879\n",
            "Name: label, dtype: int64\n",
            "\n",
            "General dataset size: 4846\n",
            "\n",
            "Positive Sentiment Mean Article Length: 135.64783565663976\n",
            "Positive Sentiment Max Article Length: 298\n",
            "\n",
            "Neutral Sentiment Mean Article Length: 125.07224730809308\n",
            "Neutral Sentiment Max Article Length: 315\n",
            "\n",
            "Negative Sentiment Mean Article Length: 125.75662251655629\n",
            "Negative Sentiment Max Article Length: 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC3riTPlMzan"
      },
      "source": [
        "## Have a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvpwUFhMzan",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8337f9f-b835-4417-b56b-ab6a708adab9"
      },
      "source": [
        "print('Article:')\n",
        "print(data.iloc[13].article + \"\\n\")\n",
        "print('Label:')\n",
        "print(data.iloc[13].label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article:\n",
            "Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\n",
            "\n",
            "Label:\n",
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m91CGvaCWnjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9060a5c-650d-4460-8034-90c72fba826b"
      },
      "source": [
        "label_names = list(set(data['label'].values))\n",
        "label_names.sort()\n",
        "\n",
        "print(\"All possible labels: \" + str(label_names))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All possible labels: ['negative', 'neutral', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcQOnkcvMzan"
      },
      "source": [
        "### Test Data if BERT cased or uncased makes sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuqn6Oc2Mzao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f57891-7cf1-4369-c6bd-d3b3323a2621"
      },
      "source": [
        "uppercased_words_found = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    temp_article = row['article']\n",
        "    \n",
        "    for word in temp_article:\n",
        "        if len(word)>1 and word == word.upper():\n",
        "            uppercased_words_found.append(word)\n",
        "            \n",
        "print(uppercased_words_found)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_89-2sLiMzao"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "606mN-qUMzao"
      },
      "source": [
        "# normalize whitespace\n",
        "data['article'] = data['article'].apply(lambda x: \" \".join(x.split()))\n",
        "\n",
        "# remove punctuations except ?\n",
        "data['article'] = data['article'].apply(lambda x: re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLYjtJIMzap"
      },
      "source": [
        "## Convert labels to more understandable 0, 1, 2 format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIT7oXs_Mzap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "4b92c033-4c27-4259-c690-e1a2a39437ec"
      },
      "source": [
        "new_labels = {\n",
        "    'positive': 2,\n",
        "    'negative': 0,\n",
        "    'neutral': 1\n",
        "}\n",
        "for index, row in data.iterrows():\n",
        "  data['label'].iloc[index] = new_labels[row['label']]\n",
        "data.head(2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>According to Gran  ,  the company has no plans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            article\n",
              "0     1  According to Gran  ,  the company has no plans...\n",
              "1     1  Technopolis plans to develop in stages an area..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ErARovQHVgA"
      },
      "source": [
        "## Build Pseudo Labeling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgjsgVpfHbI8"
      },
      "source": [
        "def get_dataclass_distribution_of_unlabeled_data(data_class_size, X, y, random_state):\n",
        "  labeled_data_distribution = data_class_size/len(X)\n",
        "\n",
        "  X_unlabeled, X_labeled, y_unlabeled, y_labeled = train_test_split(X, y, test_size=labeled_data_distribution, random_state=random_state)\n",
        "  len(X_labeled)\n",
        "  return {\n",
        "      'labeled_data': X_labeled,\n",
        "      'labeled_data_labels': y_labeled,\n",
        "      'unlabeled_data': X_unlabeled,\n",
        "      'unlabeled_data_labels': y_unlabeled\n",
        "  }"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpqv6yEqMzap"
      },
      "source": [
        "## Set tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc5vTsC1Mzap"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Uncased -> Because case may express the sentiment but no uppercase word are given in the dataset as mentioned earlier"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMinlZ7PMzap"
      },
      "source": [
        "### Choose maximum token length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ3EHTwyMzaq",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "6faff02e-f131-40f1-d6de-63cac1b5804b"
      },
      "source": [
        "token_lens = []\n",
        "for txt in data.article:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))\n",
        "    \n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW2I38KQMzaq"
      },
      "source": [
        "MAX_LEN = 100\n",
        "BATCH_SIZE = 8\n",
        "SEED = 1210"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcuqFPNNMzaq"
      },
      "source": [
        "# Split data into training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRjOsFIKMzaq"
      },
      "source": [
        "def split_data_into_train_test_val(X, y, val_size, random_state):\n",
        "    X_reduced, X_test, y_reduced, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_reduced, y_reduced, test_size=val_size, random_state=random_state, stratify=y_reduced)\n",
        "    \n",
        "    return {\n",
        "        'train': {\n",
        "            'X': list(X_train),\n",
        "            'y': list(y_train)\n",
        "        },\n",
        "        'val': {\n",
        "            'X': list(X_val),\n",
        "            'y': list(y_val)\n",
        "        },\n",
        "        'test': {\n",
        "            'X': list(X_test),\n",
        "            'y': list(y_test)\n",
        "        }\n",
        "    }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijpna4dYMzaq",
        "scrolled": true
      },
      "source": [
        "split_data = split_data_into_train_test_val(data['article'].values, data['label'].values, val_size=0.2, random_state=SEED)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpvWmDwhMzar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d720e5-8b98-4e8e-9f56-7f4fd62c86aa"
      },
      "source": [
        "X_train = split_data['train']['X']\n",
        "X_test = split_data['test']['X']\n",
        "X_val = split_data['val']['X']\n",
        "y_train = split_data['train']['y']\n",
        "y_test = split_data['test']['y']\n",
        "y_val = split_data['val']['y']\n",
        "\n",
        "import collections, numpy\n",
        "\n",
        "\n",
        "\n",
        "print(\"Distribution train: \")\n",
        "print(collections.Counter(y_train))\n",
        "print(\"\\nDistribution val: \")\n",
        "print(collections.Counter(y_val))\n",
        "print(\"\\nDistribution test: \")\n",
        "print(collections.Counter(y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution train: \n",
            "Counter({1: 2072, 2: 982, 0: 434})\n",
            "\n",
            "Distribution val: \n",
            "Counter({1: 519, 2: 245, 0: 109})\n",
            "\n",
            "Distribution test: \n",
            "Counter({1: 288, 2: 136, 0: 61})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32Ndw0wMzar"
      },
      "source": [
        "### Build data class for pytorch-Dataloader usage (Map-style dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxuAYV89Mzar"
      },
      "source": [
        "class BertData():\n",
        "    def __init__(self, article, label):\n",
        "        self.article = article\n",
        "        self.label = label\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.article)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        article = str(self.article[idx])\n",
        "        article = ' '.join(article.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            article,\n",
        "            None,\n",
        "            add_special_tokens = True,\n",
        "            max_length = MAX_LEN,\n",
        "            padding='max_length',\n",
        "            truncation='longest_first'\n",
        "        )\n",
        "\n",
        "        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
        "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
        "        labels = torch.tensor(self.label[idx], dtype=torch.long)\n",
        "\n",
        "        return {'article': article,\n",
        "                'ids': ids,\n",
        "                'mask': mask,\n",
        "                'token_type_ids': token_type_ids,\n",
        "                'targets': labels\n",
        "                }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzGprxRZh9"
      },
      "source": [
        "training_set = BertData(article=X_train, label=y_train)\n",
        "validation_set = BertData(article=X_val, label=y_val)\n",
        "test_set = BertData(article=X_test, label=y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzUqxLvuMzas"
      },
      "source": [
        "### Set pytorch data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NQd1PdkMzas"
      },
      "source": [
        "def get_dataloader(training_set, validation_set, test_set, sampler):\n",
        "    train_dataloader = DataLoader(\n",
        "            training_set,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            sampler=sampler\n",
        "            )\n",
        "    val_dataloader = DataLoader(\n",
        "            validation_set,\n",
        "            shuffle=False,\n",
        "            batch_size=BATCH_SIZE\n",
        "            )\n",
        "    test_dataloader = DataLoader(\n",
        "            test_set,\n",
        "            shuffle=False,\n",
        "            batch_size=BATCH_SIZE\n",
        "            )\n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3dkrCVMzas"
      },
      "source": [
        "# BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYDrusWdMzat"
      },
      "source": [
        "## Build PyTorch Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUL8xhXYMzat"
      },
      "source": [
        "#train_dataloader, val_dataloader, test_dataloader = get_dataloader(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyE_LGsMzat"
      },
      "source": [
        "## Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGsOiF2sMzat"
      },
      "source": [
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.2\n",
        "learning_rate = 3e-5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fa2zJvybBxc"
      },
      "source": [
        "## Define standard BERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFLpR6laMzat"
      },
      "source": [
        "class ClassicalBertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClassicalBertClassifier, self).__init__()\n",
        "        \n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        #for param in self.bert.parameters():\n",
        "         # param.requires_grad = False\n",
        "        #self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "#        self.classifier = nn.Sequential(\n",
        " #           nn.Linear(768, 384),\n",
        "  #          #nn.ReLU(),\n",
        "   #         nn.ReLU(),\n",
        "    #        nn.Dropout(0.3),\n",
        "     #       nn.Linear(384, 128),\n",
        "      #      nn.ReLU(),\n",
        "       #     nn.Dropout(0.3),\n",
        "        #    nn.Linear(128, 3)\n",
        "       # )\n",
        "\n",
        "#        self.classifier = nn.Sequential(\n",
        " #           nn.Linear(768, 384),\n",
        "  #          nn.Tanh(),\n",
        "   #         nn.Dropout(0.5),\n",
        "    #        nn.Linear(384, 3),\n",
        "     #       nn.Softmax(1)\n",
        "      #  )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(768, 3)\n",
        "        )\n",
        "        #self.all_targets = []\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids) -> torch.Tensor:\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ezNBumeXSn"
      },
      "source": [
        "# MixTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwPSGTb2bIA_"
      },
      "source": [
        "## Define MixText BERT Classifier (According to publishers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bGpO-XebIX_"
      },
      "source": [
        "class BertModel4Mix(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertModel4Mix, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder4Mix(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _resize_token_embeddings(self, new_num_tokens):\n",
        "        old_embeddings = self.embeddings.word_embeddings\n",
        "        new_embeddings = self._get_resized_embeddings(\n",
        "            old_embeddings, new_num_tokens)\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(self, input_ids,  input_ids2=None, l=None, mix_layer=1000, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            if input_ids2 is not None:\n",
        "                attention_mask2 = torch.ones_like(input_ids2)\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "            if input_ids2 is not None:\n",
        "                token_type_ids2 = torch.zeros_like(input_ids2)\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "\n",
        "            extended_attention_mask2 = attention_mask2.unsqueeze(\n",
        "                1).unsqueeze(2)\n",
        "\n",
        "            extended_attention_mask2 = extended_attention_mask2.to(\n",
        "                dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "            extended_attention_mask2 = (\n",
        "                1.0 - extended_attention_mask2) * -10000.0\n",
        "\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(\n",
        "                    0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand(\n",
        "                    self.config.num_hidden_layers, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                # We can specify head_mask for each layer\n",
        "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "            # switch to fload if need + fp16 compatibility\n",
        "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)\n",
        "        else:\n",
        "            head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "            embedding_output2 = self.embeddings(\n",
        "                input_ids2, position_ids=position_ids, token_type_ids=token_type_ids2)\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "            encoder_outputs = self.encoder(embedding_output, embedding_output2, l, mix_layer,\n",
        "                                           extended_attention_mask, extended_attention_mask2, head_mask=head_mask)\n",
        "        else:\n",
        "            encoder_outputs = self.encoder(\n",
        "                embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        # add hidden_states and attentions if they are here\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
        "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        return outputs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa6eFazbgbx"
      },
      "source": [
        "## Define BERT Encoder for Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkNgukRjbmOs"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class BertEncoder4Mix(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder4Mix, self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.layer = nn.ModuleList([BertLayer(config)\n",
        "                                    for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, hidden_states2=None, l=None, mix_layer=1000, attention_mask=None, attention_mask2=None, head_mask=None):\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "\n",
        "        # Perform mix at till the mix_layer\n",
        "        if mix_layer == -1:\n",
        "            if hidden_states2 is not None:\n",
        "                hidden_states = l * hidden_states + (1-l)*hidden_states2\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if i <= mix_layer:\n",
        "\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states, attention_mask, head_mask[i])\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "                if hidden_states2 is not None:\n",
        "                    layer_outputs2 = layer_module(\n",
        "                        hidden_states2, attention_mask2, head_mask[i])\n",
        "                    hidden_states2 = layer_outputs2[0]\n",
        "\n",
        "            if i == mix_layer:\n",
        "                if hidden_states2 is not None:\n",
        "                    hidden_states = l * hidden_states + (1-l)*hidden_states2\n",
        "\n",
        "            if i > mix_layer:\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states, attention_mask, head_mask[i])\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        # last-layer hidden state, (all hidden states), (all attentions)\n",
        "        return outputs\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ4SBdmxbpua"
      },
      "source": [
        "## Define Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOide6Vbrqg"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class MixText(nn.Module):\n",
        "    def __init__(self, num_labels=3, mix_option=False):\n",
        "        super(MixText, self).__init__()\n",
        "\n",
        "        if mix_option:\n",
        "            self.bert = BertModel4Mix.from_pretrained('bert-base-uncased')\n",
        "        else:\n",
        "            self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(768, 128),\n",
        "                                    nn.Tanh(),\n",
        "                                    nn.Dropout(0.5),\n",
        "                                    nn.Linear(128, num_labels))\n",
        "\n",
        "    def forward(self, x, x2=None, l=None, mix_layer=1000):\n",
        "\n",
        "        if x2 is not None:\n",
        "            all_hidden, pooler = self.bert(x, x2, l, mix_layer)\n",
        "\n",
        "            pooled_output = torch.mean(all_hidden, 1)\n",
        "\n",
        "        else:\n",
        "            all_hidden, pooler = self.bert(x)\n",
        "            pooled_output = torch.mean(all_hidden, 1)\n",
        "\n",
        "        predict = self.linear(pooled_output)\n",
        "\n",
        "        return predict"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9CLe9PbyqG"
      },
      "source": [
        "## Define Mix Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot2qTP6sb2mt"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "def mix_validate(valloader, model, criterion, epoch, mode):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_total = 0\n",
        "        total_sample = 0\n",
        "        correct = 0\n",
        "        overall_targets = []\n",
        "        overall_preds = []\n",
        "\n",
        "        for batch_idx, (inputs, targets, length) in enumerate(valloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                print(\"Sample some true labeles and predicted labels\")\n",
        "                print(predicted[:20])\n",
        "                print(targets[:20])\n",
        "\n",
        "            new_targets = [int(target) for target in targets]\n",
        "            overall_targets = overall_targets + new_targets\n",
        "\n",
        "            new_preds = [int(pred) for pred in predicted]\n",
        "            overall_preds = overall_preds + new_preds\n",
        "\n",
        "            loss_total += loss.item() * inputs.shape[0]\n",
        "            total_sample += inputs.shape[0]\n",
        "\n",
        "        loss_total = loss_total/total_sample\n",
        "        \n",
        "        accuracy = accuracy_score(overall_targets, overall_preds)\n",
        "        precision = precision_score(overall_targets, overall_preds, average='macro')\n",
        "        recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "        f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "    return loss_total, accuracy, precision, recall, f1\n",
        "\n",
        "def linear_rampup(current, rampup_length=EPOCHS):\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
        "        return float(current)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W9fZSmS0LIC"
      },
      "source": [
        "## Define translation model for MixText augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCV-ASmL0Th5"
      },
      "source": [
        "from easynmt import EasyNMT\n",
        "#TRANSLATION_MODEL = EasyNMT('mbart50_m2m') # von Facebook reasearch\n",
        "TRANSLATION_MODEL = EasyNMT('opus-mt', device=device) # smaller model -> worse performance - better runtime"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaBx7Lsa3NxS"
      },
      "source": [
        "## Pre translate all possible input texts to increase training speed for MixText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaXdTQJZ3Od3"
      },
      "source": [
        "#german_translate_dict = {article: TRANSLATION_MODEL.translate(TRANSLATION_MODEL.translate(str(article), target_lang='de', source_lang='en'), target_lang='en', source_lang='de') for article in data['article']}\n",
        "#russian_translate_dict = {article: TRANSLATION_MODEL.translate(TRANSLATION_MODEL.translate(str(article), target_lang='ru', source_lang='en'), target_lang='en', source_lang='ru') for article in data['article']}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YAuC3djgqX8"
      },
      "source": [
        "def save_translator_dicts():\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/german_translate_dict.pickle', 'wb') as f:\n",
        "    pickle.dump(german_translate_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/russian_translate_dict.pickle', 'wb') as f:\n",
        "    pickle.dump(russian_translate_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_translator_dicts():\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/german_translate_dict.pickle', 'rb') as f:\n",
        "    ger_translate_dict = pickle.load(f)\n",
        "\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/russian_translate_dict.pickle', 'rb') as f:\n",
        "    rus_translate_dict = pickle.load(f)\n",
        "  return ger_translate_dict, rus_translate_dict\n",
        "german_translate_dict, russian_translate_dict = load_translator_dicts()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opPjR9d_cJaC"
      },
      "source": [
        "## Define Loss for Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms_pSP5PcLfg"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class SemiLoss(object):\n",
        "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, outputs_u_2, epoch, mixed=1):\n",
        "        temp_change = 1000000\n",
        "        T = 0.5\n",
        "        co = False\n",
        "        alpha = 0.75\n",
        "        seperate_mix = False\n",
        "        mix_layers_set = [7, 9, 12]\n",
        "        mix_method = 0\n",
        "        margin = 0.7\n",
        "        lambda_u_hinge = 1\n",
        "        lambda_u = 1\n",
        "\n",
        "        if mix_method == 0 or mix_method == 1:\n",
        "\n",
        "            Lx = - \\\n",
        "                torch.mean(torch.sum(F.log_softmax(\n",
        "                    outputs_x, dim=1) * targets_x, dim=1))\n",
        "\n",
        "            probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "            Lu = F.kl_div(probs_u.log(), targets_u, None, None, 'batchmean')\n",
        "\n",
        "            Lu2 = torch.mean(torch.clamp(torch.sum(-F.softmax(outputs_u, dim=1)\n",
        "                                                   * F.log_softmax(outputs_u, dim=1), dim=1) - margin, min=0))\n",
        "\n",
        "        elif mix_method == 2:\n",
        "            if mixed == 0:\n",
        "                Lx = - \\\n",
        "                    torch.mean(torch.sum(F.logsigmoid(\n",
        "                        outputs_x) * targets_x, dim=1))\n",
        "\n",
        "                probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "                Lu = F.kl_div(probs_u.log(), targets_u,\n",
        "                              None, None, 'batchmean')\n",
        "\n",
        "                Lu2 = torch.mean(torch.clamp(margin - torch.sum(\n",
        "                    F.softmax(outputs_u_2, dim=1) * F.softmax(outputs_u_2, dim=1), dim=1), min=0))\n",
        "            else:\n",
        "                Lx = - \\\n",
        "                    torch.mean(torch.sum(F.log_softmax(\n",
        "                        outputs_x, dim=1) * targets_x, dim=1))\n",
        "\n",
        "                probs_u = torch.softmax(outputs_u, dim=1)\n",
        "                Lu = F.kl_div(probs_u.log(), targets_u,\n",
        "                              None, None, 'batchmean')\n",
        "\n",
        "                Lu2 = torch.mean(torch.clamp(margin - torch.sum(\n",
        "                    F.softmax(outputs_u, dim=1) * F.softmax(outputs_u, dim=1), dim=1), min=0))\n",
        "\n",
        "        return Lx, Lu, lambda_u * linear_rampup(epoch), Lu2, lambda_u_hinge * linear_rampup(epoch)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDI95MW1eize"
      },
      "source": [
        "## Define loaders for MixText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUamR2nkemf0"
      },
      "source": [
        "temp_change = 1000000\n",
        "T = 0.5\n",
        "co = False\n",
        "alpha = 0.75\n",
        "seperate_mix = False\n",
        "mix_layers_set = [7, 9, 12]\n",
        "mix_method = 0\n",
        "margin = 0.7\n",
        "lambda_u_hinge = 1\n",
        "lambda_u = 1\n",
        "mix_path = '/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/'\n",
        "\n",
        "# define augmentation translator \n",
        "class Translator:\n",
        "    \"\"\"Backtranslation. Here to save time, we pre-processing and save all the translated data into pickle files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, transform_type='BackTranslation'):\n",
        "        # Translator for eng to de to eng\n",
        "        # self.translator = TRANSLATION_MODEL\n",
        "        self.translator = None\n",
        "\n",
        "    def __call__(self, ori):\n",
        "        return german_translate_dict[ori], russian_translate_dict[ori], ori\n",
        "        \n",
        "class mixtext_loader_labeled():\n",
        "    # Data loader for labeled data\n",
        "    def __init__(self, dataset_text, dataset_label, tokenizer, max_seq_len, aug=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.labels = dataset_label\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.aug = aug\n",
        "        self.trans_dist = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def augment(self, text):\n",
        "        if text not in self.trans_dist:\n",
        "            self.trans_dist[text] = german_translate_dict[str(text)]\n",
        "        return self.trans_dist[text]\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug:\n",
        "            text = self.text[idx]\n",
        "            text_aug = self.augment(text)\n",
        "            text_result, text_length = self.get_tokenized(text)\n",
        "            text_result2, text_length2 = self.get_tokenized(text_aug)\n",
        "            return ((torch.tensor(text_result), torch.tensor(text_result2).to(device)), (self.labels[idx], self.labels[idx]), (text_length, text_length2))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "            if len(tokens) > self.max_seq_len:\n",
        "                tokens = tokens[:self.max_seq_len]\n",
        "            length = len(tokens)\n",
        "            encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "            encode_result += padding\n",
        "            return (torch.tensor(encode_result), self.labels[idx], length)\n",
        "\n",
        "\n",
        "class mixtext_loader_unlabeled():\n",
        "    # Data loader for unlabeled data\n",
        "    def __init__(self, dataset_text, tokenizer, max_seq_len, aug=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.ids = [n for n in range (0, len(dataset_text)+1)]\n",
        "        self.aug = aug\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug is not None:\n",
        "            u, v, ori = self.aug(self.text[idx])\n",
        "            encode_result_u, length_u = self.get_tokenized(u)\n",
        "            encode_result_v, length_v = self.get_tokenized(v)\n",
        "            encode_result_ori, length_ori = self.get_tokenized(ori)\n",
        "            return ((torch.tensor(encode_result_u), torch.tensor(encode_result_v), torch.tensor(encode_result_ori)), (length_u, length_v, length_ori))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            encode_result, length = self.get_tokenized(text)\n",
        "            return (torch.tensor(encode_result), length)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvakzoK-cQuP"
      },
      "source": [
        "# Set up MixTest default parameters setp up by the publishers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shjAaQwccXYb"
      },
      "source": [
        "# recommended default sets by publisher of MixText\n",
        "temp_change = 1000000\n",
        "T = 0.5\n",
        "co = False\n",
        "alpha = 0.75\n",
        "seperate_mix = False\n",
        "mix_layers_set = [0, 1, 2, 3]\n",
        "mix_method = 0\n",
        "margin = 0.7\n",
        "lambda_u_hinge = 1\n",
        "mix_path = '/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzl2n2sgMzau"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYnMDrgtzaWt"
      },
      "source": [
        "## Define traditional training epoch (supervised)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dm68sAkMzau"
      },
      "source": [
        "def traditional_train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples):\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"ids\"].to(device)\n",
        "    attention_mask = d[\"mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "    \n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    new_targets = [float(target) for target in targets]\n",
        "    overall_targets = overall_targets + new_targets\n",
        "\n",
        "    new_preds = [float(pred) for pred in preds]\n",
        "    overall_preds = overall_preds + new_preds\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # avoid exploding gradient\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = accuracy_score(overall_targets, overall_preds)\n",
        "  precision = precision_score(overall_targets, overall_preds, average='macro')\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "  return accuracy, precision, recall, f1, np.mean(losses)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFMWLRx41d-4"
      },
      "source": [
        "## Define traditional eval epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr60XpqfPca-"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  pseudo_labels = []\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"ids\"].to(device)\n",
        "      attention_mask = d[\"mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids)\n",
        "      \n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      new_targets = [float(target) for target in targets]\n",
        "      overall_targets = overall_targets + new_targets\n",
        "\n",
        "      new_preds = [float(pred) for pred in preds]\n",
        "      overall_preds = overall_preds + new_preds\n",
        "\n",
        "      for item in preds:\n",
        "        pseudo_labels.append(item.item())\n",
        "  frequency = {}\n",
        "\n",
        "  # iterating over the list\n",
        "  for item in pseudo_labels:\n",
        "  # checking the element in dictionary\n",
        "    if item in frequency:\n",
        "      # incrementing the count\n",
        "      frequency[item] += 1\n",
        "    else:\n",
        "    # initializing the count\n",
        "      frequency[item] = 1\n",
        "\n",
        "  # printing the frequency\n",
        "  print(frequency)\n",
        "  \n",
        "  accuracy = accuracy_score(overall_targets, overall_preds)\n",
        "  precision = precision_score(overall_targets, overall_preds, average='macro')\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "\n",
        "  return accuracy, precision, recall, f1, np.mean(losses)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdIM8YECp3Bn"
      },
      "source": [
        "## Define Pseudo Labeling Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5IJhJRJp5ui"
      },
      "source": [
        "def generate_pseudo_label(unlabeled_dataset, model):\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "  \n",
        "  valid_pseudo_label = []\n",
        "  valid_articles = []\n",
        "  valid_probas = []\n",
        "\n",
        "  for unlabeled_article in unlabeled_dataset:\n",
        "    # prepare data from model\n",
        "    inputs = tokenizer.encode_plus(\n",
        "            unlabeled_article,\n",
        "            max_length=MAX_LEN,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    token_type_ids = inputs['token_type_ids'].to(device)\n",
        "\n",
        "    # calculate model outputs\n",
        "    output = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "  \n",
        "    top_p, top_class = output.topk(1)\n",
        "\n",
        "    # calculate pseudo labels\n",
        "    _, pred = torch.max(output, dim=1)\n",
        "\n",
        "    valid_pseudo_label.append(pred.item())\n",
        "    valid_articles.append(unlabeled_article) \n",
        "    valid_probas.append(float(top_p[0]))\n",
        "\n",
        "  select_top_500_pl = []\n",
        "  select_top_500_art = []\n",
        "  \n",
        "  # pick only the most confident Pseudo Label\n",
        "  if len(valid_probas) > 400:\n",
        "    for i in range(400):\n",
        "      max_val_index = valid_probas.index(max(valid_probas))\n",
        "      select_top_500_pl.append(valid_pseudo_label[max_val_index])\n",
        "      select_top_500_art.append(valid_articles[max_val_index])\n",
        "\n",
        "      # reset highest proba without destroying the order\n",
        "      valid_probas[max_val_index] = -1\n",
        "  else:\n",
        "    select_top_500_pl = valid_pseudo_label\n",
        "    select_top_500_art = valid_articles\n",
        "\n",
        "  frequency = {}\n",
        "\n",
        "  # iterating over the list\n",
        "  for item in select_top_500_pl:\n",
        "  # checking the element in dictionary\n",
        "    if item in frequency:\n",
        "      # incrementing the count\n",
        "      frequency[item] += 1\n",
        "    else:\n",
        "    # initializing the count\n",
        "      frequency[item] = 1\n",
        "  \n",
        "  return (select_top_500_pl, select_top_500_art)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GABnMc4a10tP"
      },
      "source": [
        "## Define Naive Semi supervised deep learning training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so8Bu_QR17x8"
      },
      "source": [
        "def nssdl_train_epoch(\n",
        "    model,\n",
        "    pseudo_label_dataloader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    n_examples,\n",
        "    unlabeled_dataset):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  for d in pseudo_label_dataloader:\n",
        "    input_ids = d[\"ids\"].to(device)\n",
        "    attention_mask = d[\"mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "    \n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    new_targets = [float(target) for target in targets]\n",
        "    overall_targets = overall_targets + new_targets\n",
        "\n",
        "    new_preds = [float(pred) for pred in preds]\n",
        "    overall_preds = overall_preds + new_preds\n",
        "\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # avoid exploding gradient\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  # create new pseudo labels for next iteration\n",
        "  accuracy = accuracy_score(overall_targets, overall_preds)\n",
        "  precision = precision_score(overall_targets, overall_preds, average='macro')\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "\n",
        "  return accuracy, precision, recall, f1, np.mean(losses)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBzofDQZ18Ey"
      },
      "source": [
        "## Define Naive Semi supervised deep eval training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzPEI95Y1-Zy"
      },
      "source": [
        "# Sharing same eval loader as traditional approach"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpqoPt5_qk-"
      },
      "source": [
        "## Define MixText Model Class and Train Epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhrIUD4__0y7"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "def mixtext_train_epoch(labeled_trainloader, unlabeled_trainloader, model, optimizer, scheduler, criterion, epoch, n_labels, train_aug, len_labeled_data):\n",
        "  temp_change = 1000000\n",
        "  T = 0.5\n",
        "  co = False\n",
        "  alpha = 0.75\n",
        "  separate_mix = False\n",
        "  mix_layers_set = [7, 9, 12]\n",
        "  mix_method = 0\n",
        "  margin = 0.7\n",
        "  lambda_u_hinge = 1\n",
        "  lambda_u = 1\n",
        "\n",
        "  train_aug=True\n",
        "  labeled_train_iter = iter(labeled_trainloader)\n",
        "  unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "  model.train()\n",
        "\n",
        "  val_iteration = len_labeled_data\n",
        "\n",
        "  global total_steps\n",
        "  global flag\n",
        "  if flag == 0 and total_steps > temp_change:\n",
        "      print('Change T!')\n",
        "      T = 0.9\n",
        "      flag = 1\n",
        "\n",
        "  for batch_idx in range(val_iteration):\n",
        "\n",
        "      total_steps += 1\n",
        "\n",
        "      if not train_aug:\n",
        "          try:\n",
        "              inputs_x, targets_x, inputs_x_length = labeled_train_iter.next()\n",
        "          except:\n",
        "              labeled_train_iter = iter(labeled_trainloader)\n",
        "              inputs_x, targets_x, inputs_x_length = labeled_train_iter.next()\n",
        "      else:\n",
        "          try:\n",
        "              (inputs_x, inputs_x_aug), (targets_x, _), (inputs_x_length,\n",
        "                                                          inputs_x_length_aug) = labeled_train_iter.next()\n",
        "          except:\n",
        "              labeled_train_iter = iter(labeled_trainloader)\n",
        "              (inputs_x, inputs_x_aug), (targets_x, _), (inputs_x_length,\n",
        "                                                          inputs_x_length_aug) = labeled_train_iter.next()\n",
        "      try:\n",
        "          (inputs_u, inputs_u2,  inputs_ori), (length_u,\n",
        "                                                length_u2,  length_ori) = unlabeled_train_iter.next()\n",
        "      except:\n",
        "          unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "          (inputs_u, inputs_u2, inputs_ori), (length_u,\n",
        "                                              length_u2, length_ori) = unlabeled_train_iter.next()\n",
        "\n",
        "      batch_size = inputs_x.size(0)\n",
        "      batch_size_2 = inputs_ori.size(0)\n",
        "      targets_x = torch.zeros(batch_size, n_labels).scatter_(\n",
        "          1, targets_x.view(-1, 1), 1)\n",
        "\n",
        "      inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
        "      inputs_u = inputs_u.to(device)\n",
        "      inputs_u2 = inputs_u2.to(device)\n",
        "      inputs_ori = inputs_ori.to(device)\n",
        "\n",
        "      mask = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Predict labels for unlabeled data.\n",
        "          outputs_u = model(inputs_u)\n",
        "          outputs_u2 = model(inputs_u2)\n",
        "          outputs_ori = model(inputs_ori)\n",
        "\n",
        "          # Based on translation qualities, choose different weights here.\n",
        "          # For AG News: German: 1, Russian: 0, ori: 1\n",
        "          # For DBPedia: German: 1, Russian: 1, ori: 1\n",
        "          # For IMDB: German: 0, Russian: 0, ori: 1\n",
        "          # For Yahoo Answers: German: 1, Russian: 0, ori: 1 / German: 0, Russian: 0, ori: 1\n",
        "          p = (0 * torch.softmax(outputs_u, dim=1) + 0 * torch.softmax(outputs_u2,\n",
        "                                                                        dim=1) + 1 * torch.softmax(outputs_ori, dim=1)) / (1)\n",
        "          # Do a sharpen here.\n",
        "          pt = p**(1/T)\n",
        "          targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
        "          targets_u = targets_u.detach()\n",
        "\n",
        "      mixed = 1\n",
        "\n",
        "      if co:\n",
        "          mix_ = np.random.choice([0, 1], 1)[0]\n",
        "      else:\n",
        "          mix_ = 1\n",
        "\n",
        "      if mix_ == 1:\n",
        "          l = np.random.beta(alpha, alpha)\n",
        "          if separate_mix:\n",
        "              l = l\n",
        "          else:\n",
        "              l = max(l, 1-l)\n",
        "      else:\n",
        "          l = 1\n",
        "\n",
        "      mix_layer = np.random.choice(mix_layers_set, 1)[0]\n",
        "      mix_layer = mix_layer - 1\n",
        "\n",
        "      if not train_aug:\n",
        "          all_inputs = torch.cat(\n",
        "              [inputs_x, inputs_u, inputs_u2, inputs_ori, inputs_ori], dim=0)\n",
        "\n",
        "          all_lengths = torch.cat(\n",
        "              [inputs_x_length, length_u, length_u2, length_ori, length_ori], dim=0)\n",
        "\n",
        "          all_targets = torch.cat(\n",
        "              [targets_x, targets_u, targets_u, targets_u, targets_u], dim=0)\n",
        "\n",
        "      else:\n",
        "          all_inputs = torch.cat(\n",
        "              [inputs_x, inputs_x_aug, inputs_u, inputs_u2, inputs_ori], dim=0)\n",
        "          all_lengths = torch.cat(\n",
        "              [inputs_x_length, inputs_x_length, length_u, length_u2, length_ori], dim=0)\n",
        "          all_targets = torch.cat(\n",
        "              [targets_x, targets_x, targets_u, targets_u, targets_u], dim=0)\n",
        "\n",
        "      if separate_mix:\n",
        "          idx1 = torch.randperm(batch_size)\n",
        "          idx2 = torch.randperm(all_inputs.size(0) - batch_size) + batch_size\n",
        "          idx = torch.cat([idx1, idx2], dim=0)\n",
        "\n",
        "      else:\n",
        "          idx1 = torch.randperm(all_inputs.size(0) - batch_size_2)\n",
        "          idx2 = torch.arange(batch_size_2) + \\\n",
        "              all_inputs.size(0) - batch_size_2\n",
        "          idx = torch.cat([idx1, idx2], dim=0)\n",
        "\n",
        "      input_a, input_b = all_inputs, all_inputs[idx]\n",
        "      target_a, target_b = all_targets, all_targets[idx]\n",
        "      length_a, length_b = all_lengths, all_lengths[idx]\n",
        "\n",
        "      if mix_method == 0:\n",
        "          # Mix sentences' hidden representations\n",
        "          logits = model(input_a, input_b, l, mix_layer)\n",
        "          mixed_target = l * target_a + (1 - l) * target_b\n",
        "\n",
        "      elif mix_method == 1:\n",
        "          # Concat snippet of two training sentences, the snippets are selected based on l\n",
        "          # For example: \"I lova you so much\" and \"He likes NLP\" could be mixed as \"He likes NLP so much\".\n",
        "          # The corresponding labels are mixed with coefficient as well\n",
        "          mixed_input = []\n",
        "          if l != 1:\n",
        "              for i in range(input_a.size(0)):\n",
        "                  length1 = math.floor(int(length_a[i]) * l)\n",
        "                  idx1 = torch.randperm(int(length_a[i]) - length1 + 1)[0]\n",
        "                  length2 = math.ceil(int(length_b[i]) * (1-l))\n",
        "                  if length1 + length2 > 256:\n",
        "                      length2 = 256-length1 - 1\n",
        "                  idx2 = torch.randperm(int(length_b[i]) - length2 + 1)[0]\n",
        "                  try:\n",
        "                      mixed_input.append(\n",
        "                          torch.cat((input_a[i][idx1: idx1 + length1], torch.tensor([102]).to(device), input_b[i][idx2:idx2 + length2], torch.tensor([0]*(256-1-length1-length2)).to(device)), dim=0).unsqueeze(0))\n",
        "                  except:\n",
        "                      print(256 - 1 - length1 - length2,\n",
        "                            idx2, length2, idx1, length1)\n",
        "\n",
        "              mixed_input = torch.cat(mixed_input, dim=0)\n",
        "\n",
        "          else:\n",
        "              mixed_input = input_a\n",
        "\n",
        "          logits = model(mixed_input)\n",
        "          mixed_target = l * target_a + (1 - l) * target_b\n",
        "\n",
        "      elif mix_method == 2:\n",
        "          # Concat two training sentences\n",
        "          # The corresponding labels are averaged\n",
        "          if l == 1:\n",
        "              mixed_input = []\n",
        "              for i in range(input_a.size(0)):\n",
        "                  mixed_input.append(\n",
        "                      torch.cat((input_a[i][:length_a[i]], torch.tensor([102]).to(device), input_b[i][:length_b[i]], torch.tensor([0]*(512-1-int(length_a[i])-int(length_b[i]))).to(device)), dim=0).unsqueeze(0))\n",
        "\n",
        "              mixed_input = torch.cat(mixed_input, dim=0)\n",
        "              logits = model(mixed_input, sent_size=512)\n",
        "\n",
        "              #mixed_target = torch.clamp(target_a + target_b, max = 1)\n",
        "              mixed = 0\n",
        "              mixed_target = (target_a + target_b)/2\n",
        "          else:\n",
        "              mixed_input = input_a\n",
        "              mixed_target = target_a\n",
        "              logits = model(mixed_input, sent_size=256)\n",
        "              mixed = 1\n",
        "\n",
        "      Lx, Lu, w, Lu2, w2 = criterion(logits[:batch_size], mixed_target[:batch_size], logits[batch_size:-batch_size_2],\n",
        "                                      mixed_target[batch_size:-batch_size_2], logits[-batch_size_2:], epoch+batch_idx/val_iteration, mixed)\n",
        "\n",
        "      if mix_ == 1:\n",
        "          loss = Lx + w * Lu\n",
        "      else:\n",
        "          loss = Lx + w * Lu + w2 * Lu2\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "      optimizer.step()\n",
        "      # scheduler.step()\n",
        "          \n",
        "      #precision = accuracy_score(overall_targets, overall_preds)\n",
        "      #recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "      #f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "      #return precision, recall, f1, np.mean(losses), new_pseudo_label_dataloader"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2gpM7HR-YUb"
      },
      "source": [
        "## Generic Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtVBC5SS-b6N"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def run_model_training(kind, model, train_dataloader, val_dataloader, loss_fn, \n",
        "                       optimizer, scheduler, training_set, validation_set, \n",
        "                       unlabeled_train_set, unlabeled_scheduler, train_criterion, fold, mix_train_loader_for_train_vali_if_aug,\n",
        "                       data_class, epochs): # kind == 'traditional', 'NSSDL', 'NSSDL Strong', 'MixText'\n",
        "  best_f1 = 0\n",
        "  history = defaultdict(list)\n",
        "  best_model = None\n",
        "\n",
        "  # for early stopping: save val_loss from last epoch\n",
        "  last_val_loss = None\n",
        "\n",
        "  pseudo_label_dataloader = None\n",
        "  ret_str = \"\"\n",
        "\n",
        "  # for precise analysis save pl acc/loss and finetune acc/loss as well\n",
        "  if kind == 'NSSDL':\n",
        "    train_acc_pl = 0\n",
        "    train_acc_fine_tune = 0\n",
        "    train_loss_pl = 0\n",
        "    train_loss_fine_tune = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f'({kind}) Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "    \n",
        "    if kind == 'traditional' or kind == 'nssdl_strong_supervised':\n",
        "      train_acc, train_precision, train_recall, train_f1, train_loss = traditional_train_epoch(\n",
        "        model=model,\n",
        "        data_loader=train_dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        scheduler=scheduler,\n",
        "        n_examples=len(training_set))\n",
        "      \n",
        "    elif kind == 'NSSDL' or kind == 'NSSDL Strong':\n",
        "      if epoch == 0:\n",
        "        if kind !='NSSDL Strong':\n",
        "          # train initial model with labeled data\n",
        "          temp_acc, temp_precision, temp_recall, temp_f1, temp_loss = traditional_train_epoch(\n",
        "                  model=model,\n",
        "                  data_loader=train_dataloader,\n",
        "                  loss_fn=loss_fn,\n",
        "                  optimizer=optimizer,\n",
        "                  device=device,\n",
        "                  scheduler=scheduler,\n",
        "                  n_examples=len(training_set)\n",
        "                )\n",
        "          \n",
        "          print(f'Initial acc: {temp_precision} Initial loss: {temp_loss}' )\n",
        "          \n",
        "        pseudo_labels = generate_pseudo_label(unlabeled_dataset=unlabeled_train_set, \n",
        "                                        model=model)\n",
        "        \n",
        "        pseudo_label_list = pseudo_labels[0]\n",
        "        unlabeled_dataset = pseudo_labels[1]\n",
        "        pseudo_labeled_dataset = BertData(article=unlabeled_dataset, \n",
        "                                          label=pseudo_label_list)\n",
        "        \n",
        "        # Create PseudoLabel DataLoader\n",
        "        pseudo_label_dataloader = DataLoader(\n",
        "                  pseudo_labeled_dataset,\n",
        "                  shuffle=False,\n",
        "                  batch_size=BATCH_SIZE\n",
        "                  )\n",
        "\n",
        "      # first train with pseudo labels\n",
        "      train_acc_pl, train_precision_pl, train_recall_pl, train_f1_pl, train_loss_pl = nssdl_train_epoch(\n",
        "                model=model,\n",
        "                pseudo_label_dataloader=pseudo_label_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device,\n",
        "                scheduler=unlabeled_scheduler,\n",
        "                n_examples=len(unlabeled_train_set),\n",
        "                unlabeled_dataset=unlabeled_train_set\n",
        "              )\n",
        "      \n",
        "      # fine tune model with labeled samples\n",
        "      train_acc_fine_tune, train_precision_fine_tune, train_recall_fine_tune, train_f1_fine_tune, train_loss_fine_tune = traditional_train_epoch(\n",
        "                model=model,\n",
        "                data_loader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device,\n",
        "                scheduler=scheduler,\n",
        "                n_examples=len(training_set)\n",
        "              )\n",
        "      \n",
        "      ################## Pseudo Labeling ####################\n",
        "      # 1. guess pseudo label\n",
        "      # 2. BertData()\n",
        "      # 3. DataLoader(BertData(PL))\n",
        "      pseudo_labels = generate_pseudo_label(unlabeled_dataset=unlabeled_dataset, \n",
        "                                            model=model)\n",
        "      \n",
        "      pseudo_label_list = pseudo_labels[0]\n",
        "\n",
        "      unlabeled_dataset = pseudo_labels[1]\n",
        "      \n",
        "      # Create WeightedRandomSampler to counter inbalanced datasets\n",
        "      ctr = collections.Counter(pseudo_label_list)\n",
        "      class_weights = [round(ctr[0]/len(pseudo_label_list), 1), round(ctr[1]/len(pseudo_label_list), 1), round(ctr[2]/len(pseudo_label_list), 1)]\n",
        "      class_weights_initialize = [class_weights[i] for i in pseudo_label_list]\n",
        "\n",
        "      # set sampler\n",
        "      weighted_sampler=WeightedRandomSampler(weights=class_weights_initialize, num_samples=len(class_weights_initialize), replacement=True)\n",
        "\n",
        "      # Convert to BertData\n",
        "      pseudo_labeled_dataset = BertData(article=unlabeled_dataset, \n",
        "                                    label=pseudo_label_list)\n",
        "\n",
        "      # Create PseudoLabel DataLoader\n",
        "      pseudo_label_dataloader = DataLoader(\n",
        "            pseudo_labeled_dataset,\n",
        "            sampler=weighted_sampler,\n",
        "            batch_size=BATCH_SIZE\n",
        "            )\n",
        "      ##################               ####################\n",
        "\n",
        "      train_loss = float((train_loss_pl + train_loss_fine_tune)/2)\n",
        "      train_acc = float((float(train_acc_pl) + float(train_acc_fine_tune))/2)\n",
        "      train_precision = float((float(train_precision_pl) + float(train_precision_fine_tune))/2)\n",
        "      train_recall = float((float(train_recall_pl) + float(train_recall_fine_tune))/2)\n",
        "      train_f1 = float((float(train_f1_pl) + float(train_f1_fine_tune))/2)\n",
        "      print(f\"FT-Accuracy: {train_acc_pl} Precision: {train_precision_pl} FT-Recall: {train_recall_pl} FT-F1: {train_f1_pl} FT-Loss: {train_loss_pl}\")\n",
        "      print(f\"PL-Accuracy: {train_acc_fine_tune} Precision: {train_precision_fine_tune} PL-Recall: {train_recall_fine_tune} PL-F1: {train_f1_fine_tune} PL-Loss: {train_loss_fine_tune}\")\n",
        "\n",
        "    if kind == 'MixText':\n",
        "      mixtext_train_epoch(labeled_trainloader=train_dataloader,\n",
        "                          unlabeled_trainloader=unlabeled_train_set,\n",
        "                          model=model,\n",
        "                          optimizer=optimizer,\n",
        "                          scheduler=None,\n",
        "                          criterion=train_criterion,\n",
        "                          epoch=epoch,\n",
        "                          n_labels=3,\n",
        "                          train_aug=False,\n",
        "                          len_labeled_data=len(training_set))\n",
        "\n",
        "      train_loss, train_acc, train_precision, train_recall, train_f1 = mix_validate(mix_train_loader_for_train_vali_if_aug,\n",
        "                              model,  loss_fn, epoch, mode='Train Stats')\n",
        "\n",
        "      val_loss, val_acc, val_precision, val_recall, val_f1 = mix_validate(\n",
        "          val_dataloader, model, loss_fn, epoch, mode='Valid Stats')\n",
        "    \n",
        "    print(f'({kind}) Train loss {train_loss}, accuracy {train_acc}, precision {train_precision}, recall {train_recall}, f1 {train_f1}')\n",
        "\n",
        "    if kind != 'MixText':\n",
        "      val_acc, val_precision, val_recall, val_f1, val_loss = eval_model(\n",
        "        model,\n",
        "        val_dataloader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(validation_set))\n",
        "      \n",
        "\n",
        "    \n",
        "    print(f'({kind}) Val   loss {val_loss}, accuracy {val_acc}, precision {val_precision}, recall {val_recall}, f1 {val_f1}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_precision'].append(train_precision)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_recall'].append(train_recall)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_precision'].append(val_precision)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_recall'].append(val_recall)\n",
        "    history['train_f1'].append(val_f1)\n",
        "\n",
        "    ret_str += f\"({kind}) Epoch {epoch+1}: \\n Training(acc: {round(train_acc, 4)}, precision: {round(train_precision, 4)}, recall: {train_recall}, f1: {train_f1}, loss: {round(train_loss, 4)}) \\n Validation(acc: {round(val_acc, 4)}, precision: {round(val_precision, 4)}, recall: {val_recall}, f1: {val_f1}, loss: {round(val_loss, 4)})\\n\"\n",
        "\n",
        "    if kind == 'NSSDL':\n",
        "      history['train_acc_pl'].append(train_precision_pl)\n",
        "      history['train_acc_fine_tune'].append(train_precision_fine_tune)\n",
        "      history['train_loss_pl'].append(train_loss_pl)\n",
        "      history['train_loss_fine_tune'].append(train_loss_fine_tune)\n",
        "\n",
        "      ret_str += f\"PL-Training(acc: {round(train_acc_pl, 4)}, precision: {round(train_precision_pl, 4)}, recall: {train_recall_pl}, f1: {train_f1_pl}, loss: {round(train_loss_pl, 4)}) \\n Finetune-Training(acc: {round(train_acc_fine_tune, 4)}, precision: {round(train_precision_fine_tune, 4)}, recall: {train_recall_fine_tune}, f1: {train_f1_fine_tune}, loss: {round(train_loss_fine_tune, 4)})\\n\"\n",
        "\n",
        "    ret_str += \"\\n\"\n",
        "      \n",
        "    # early stopping:\n",
        "#    if (last_val_loss is not None) and (last_val_loss < val_loss):\n",
        " #     print(\"Early stopping executed due to increasing val_loss from \" + str(last_val_loss) + \" to \" + str(val_loss))\n",
        "  #    break\n",
        "\n",
        "   # last_val_loss = val_loss\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "      best_model = model.state_dict()\n",
        "      torch.save(model.state_dict(), f'/content/drive/My Drive/Colab Notebooks/Data/Model/dataclasses/{data_class}/fold_{fold+1}/{kind}_best_model_state.pt')\n",
        "      best_f1 = val_f1\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "    if unlabeled_scheduler:\n",
        "        unlabeled_scheduler.step()\n",
        "  \n",
        "  ret_str += \"-----------------------------------------------------------------\"\n",
        "\n",
        "  return best_model, history, best_f1, ret_str"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfaRm4ZxGtm9"
      },
      "source": [
        "## Overall Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXGzec0p3v5B"
      },
      "source": [
        "#### Build function to save results to excel each fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhN_uAnb33w0"
      },
      "source": [
        "def save_results_to_excel(saving_dict, fold, dataclass):\n",
        "  data_test = [{\n",
        "              'fold': fold,\n",
        "              'model': kind,\n",
        "              'test_acc': saving_dict[kind]['test']['acc'],\n",
        "              'test_precision': saving_dict[kind]['test']['precision'],\n",
        "              'test_recall': saving_dict[kind]['test']['recall'],\n",
        "              'test_f1': saving_dict[kind]['test']['f1'],\n",
        "              'test_loss': saving_dict[kind]['test']['loss'],\n",
        "              'train_time': saving_dict[kind]['time']\n",
        "          } for kind in list(saving_dict.keys())]\n",
        "\n",
        "  for kind in list(saving_dict.keys()):\n",
        "    list_to_df = [{\n",
        "        'epoch': epoch+1,\n",
        "        'model': kind,\n",
        "        'train_acc': saving_dict[kind]['hist']['train_acc'][epoch],\n",
        "        'train_precision': saving_dict[kind]['hist']['train_precision'][epoch],\n",
        "        'train_recall': saving_dict[kind]['hist']['train_recall'][epoch],\n",
        "        'train_f1': saving_dict[kind]['hist']['train_f1'][epoch],\n",
        "        'train_loss': saving_dict[kind]['hist']['train_loss'][epoch],\n",
        "        'val_acc': saving_dict[kind]['hist']['val_acc'][epoch],\n",
        "        'val_precision': saving_dict[kind]['hist']['val_precision'][epoch],\n",
        "        'val_recall': saving_dict[kind]['hist']['train_recall'][epoch],\n",
        "        'val_f1': saving_dict[kind]['hist']['train_f1'][epoch],\n",
        "        'val_loss': saving_dict[kind]['hist']['val_loss'][epoch]\n",
        "      } for epoch in range(EPOCHS)] \n",
        "\n",
        "    df_to_safe = pd.DataFrame(list_to_df)\n",
        "    df_to_safe.to_excel(f\"/content/drive/My Drive/Colab Notebooks/BA/histories/data_classes/{data_class}/fold_{1}/{kind}/training_data_from_fold.xlsx\")\n",
        "\n",
        "  df_test = pd.DataFrame(data_test)\n",
        "\n",
        "  # to excel\n",
        "  df_test.to_excel(f\"/content/drive/My Drive/Colab Notebooks/BA/class_{data_class}_test_data_from_fold_{fold}.xlsx\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxx7eEm36dk"
      },
      "source": [
        "## Loop itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHeiGlkTGs7u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5904709d-31d8-4065-e91a-82d35f22874f"
      },
      "source": [
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "    \n",
        "FOLDS = 10\n",
        "RANDOM_STATES = ['1210', '505', '2506', '1807', '1402', '107', '1803', '2405', '208', '2209']\n",
        "\n",
        "overall_saving_dict = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "best_models = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "\n",
        "test_txt_string = \"\"\n",
        "\n",
        "total_steps = 0\n",
        "flag = 0\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "\n",
        "# set reasonable data manipulation classes\n",
        "data_manipultation_classes = [25, 50, 250, 500, 2000]\n",
        "\n",
        "for data_class in data_manipultation_classes:\n",
        "  print(\"Data class of \" + str(data_class) + \" labeled samples per fold\")\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "  for fold in range(FOLDS):\n",
        "    print(f'Fold {fold + 1}/{FOLDS}')\n",
        "    print('-' * 10)\n",
        "    print()\n",
        "\n",
        "    # clear CUDA memory\n",
        "    tradi_model = None\n",
        "    NSSDL_model = None\n",
        "    NSSDL_strong_model = None\n",
        "    MixText_model = None\n",
        "\n",
        "    torch.cuda.empty_cache() \n",
        "    \n",
        "\n",
        "    tradi_model = ClassicalBertClassifier()\n",
        "    tradi_model = nn.DataParallel(tradi_model)\n",
        "    tradi_model = tradi_model.to(device)\n",
        "\n",
        "    MixText_model = MixText(num_labels=3, mix_option=True)\n",
        "    MixText_model = nn.DataParallel(MixText_model)\n",
        "    MixText_model = MixText_model.to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # set random_state\n",
        "    random_state = int(str(data_manipultation_classes.index(data_class)+1) + RANDOM_STATES[fold])\n",
        "\n",
        "    # set labeled- and unlabeled data and train-, val-, test data\n",
        "    split_data = split_data_into_train_test_val(data['article'], data['label'], 0.2, random_state)\n",
        "\n",
        "    X_train = split_data['train']['X']\n",
        "    X_test = split_data['test']['X']\n",
        "    X_val = split_data['val']['X']\n",
        "\n",
        "    y_train = split_data['train']['y']\n",
        "    y_test = split_data['test']['y']\n",
        "    y_val = split_data['val']['y']\n",
        "\n",
        "    data_dict = get_dataclass_distribution_of_unlabeled_data(data_class_size=data_class,\n",
        "                                                             X=X_train, \n",
        "                                                             y=y_train, \n",
        "                                                             random_state=random_state)\n",
        "    \n",
        "    X_train_labeled = data_dict['labeled_data']\n",
        "    X_train_unlabeled = data_dict['unlabeled_data']\n",
        "\n",
        "    y_train_labeled = data_dict['labeled_data_labels']\n",
        "    y_train_unlabeled = data_dict['unlabeled_data_labels'] # for testing purposes (maybe)\n",
        "\n",
        "    # initialise BertData\n",
        "    training_set = BertData(article=X_train_labeled, label=y_train_labeled)\n",
        "    validation_set = BertData(article=X_val, label=y_val)\n",
        "    test_set = BertData(article=X_test, label=y_test)\n",
        "\n",
        "    # set Class weight for sampler\n",
        "    ctr = collections.Counter(y_train_labeled)\n",
        "    class_weights = [round(ctr[0]/len(y_train_labeled), 1), round(ctr[1]/len(y_train_labeled), 1), round(ctr[2]/len(y_train_labeled), 1)]\n",
        "    class_weights_initialize = [class_weights[i] for i in y_train_labeled]\n",
        "\n",
        "    # set sampler\n",
        "    weighted_sampler=WeightedRandomSampler(weights=class_weights_initialize, num_samples=len(class_weights_initialize), replacement=True)\n",
        "\n",
        "\n",
        "    # initialize dataloaders\n",
        "    train_dataloader, val_dataloader, test_dataloader = get_dataloader(training_set, \n",
        "                                                                       validation_set, \n",
        "                                                                       test_set,\n",
        "                                                                       weighted_sampler)\n",
        "    # for MixText\n",
        "    mix_train_loader = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_train_labeled,\n",
        "            dataset_label=y_train_labeled, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN,\n",
        "            aug=True\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         sampler=weighted_sampler)\n",
        "    \n",
        "    mix_train_loader_for_train_vali_if_aug = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_train_labeled,\n",
        "            dataset_label=y_train_labeled, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         sampler=weighted_sampler)\n",
        "    \n",
        "    mix_train_unlabeled = DataLoader(\n",
        "        dataset=mixtext_loader_unlabeled(\n",
        "            dataset_text=X_train_unlabeled,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_len=MAX_LEN,\n",
        "            aug=Translator(mix_path)),\n",
        "         batch_size=8, \n",
        "         sampler=weighted_sampler)\n",
        "    \n",
        "    mix_val_loader = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_val,\n",
        "            dataset_label=y_val, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE)\n",
        "\n",
        "    # define optimizers, schedulers and loss function\n",
        "    tradi_optimizer = transformers.AdamW(\n",
        "        [\n",
        "            {\"params\": tradi_model.module.bert.parameters(), \"lr\": learning_rate},\n",
        "            {\"params\": tradi_model.module.classifier.parameters(), \"lr\": 0.0001}, # as proposed by authors\n",
        "        ])\n",
        "    \n",
        "    mix_optimizer = transformers.AdamW(\n",
        "        [\n",
        "            {\"params\": MixText_model.module.bert.parameters(), \"lr\": 0.000005},\n",
        "            {\"params\": MixText_model.module.linear.parameters(), \"lr\": 0.0005}, # as proposed by authors\n",
        "        ])\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "    tradi_labeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      tradi_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    train_criterion = SemiLoss()\n",
        "\n",
        "    # train trad\n",
        "    start = time.time()\n",
        "    tradi_best_model, tradi_history, tradi_best_f1, tradi_str = run_model_training('traditional', \n",
        "                                                         tradi_model, \n",
        "                                                         train_dataloader, \n",
        "                                                         val_dataloader, \n",
        "                                                         loss_fn, \n",
        "                                                         tradi_optimizer, \n",
        "                                                         tradi_labeled_scheduler, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         X_train_unlabeled,\n",
        "                                                         None,\n",
        "                                                         None,\n",
        "                                                         fold,\n",
        "                                                         None,\n",
        "                                                         data_class,\n",
        "                                                         10)\n",
        "    time_for_tradi = time.time()-start\n",
        "\n",
        "    # train nssdl\n",
        "    start = time.time()\n",
        "\n",
        "    NSSDL_model = ClassicalBertClassifier()\n",
        "    NSSDL_model = nn.DataParallel(NSSDL_model)\n",
        "    NSSDL_model = NSSDL_model.to(device)\n",
        "\n",
        "    nssdl_optimizer = transformers.AdamW(\n",
        "        [\n",
        "            {\"params\": NSSDL_model.module.bert.parameters(), \"lr\": learning_rate},\n",
        "            {\"params\": NSSDL_model.module.classifier.parameters(), \"lr\": 0.0001}, # as proposed by authors\n",
        "        ])\n",
        "\n",
        "    nssdl_labeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    nssdl_unlabeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=(len(X_train_unlabeled)/BATCH_SIZE)*EPOCHS\n",
        "    )\n",
        "\n",
        "    nssdl_best_model, nssdl_history, nssdl_best_f1, nssdl_str = run_model_training('NSSDL', \n",
        "                                                         NSSDL_model, \n",
        "                                                         train_dataloader, \n",
        "                                                         val_dataloader, \n",
        "                                                         loss_fn, \n",
        "                                                         nssdl_optimizer, \n",
        "                                                         nssdl_labeled_scheduler, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         X_train_unlabeled,\n",
        "                                                         nssdl_unlabeled_scheduler,\n",
        "                                                         None,\n",
        "                                                         fold,\n",
        "                                                         None,\n",
        "                                                         data_class,\n",
        "                                                         10)\n",
        "    time_for_nssdl = time.time()-start\n",
        "    \n",
        "    # train nssdl with strong initial Classifier\n",
        "\n",
        "    # set initial model to best model from supervisedly trained model beforehand\n",
        "    start = time.time()\n",
        "    NSSDL_Strong_model = ClassicalBertClassifier()\n",
        "    NSSDL_Strong_model = nn.DataParallel(NSSDL_Strong_model)\n",
        "    NSSDL_Strong_model = NSSDL_Strong_model.to(device)\n",
        "    NSSDL_Strong_model.load_state_dict(tradi_best_model)\n",
        "\n",
        "    nssdl_strong_optimizer = transformers.AdamW(\n",
        "        [\n",
        "            {\"params\": NSSDL_Strong_model.module.bert.parameters(), \"lr\": learning_rate},\n",
        "            {\"params\": NSSDL_Strong_model.module.classifier.parameters(), \"lr\": 0.000001}, # as proposed by authors\n",
        "        ])\n",
        "\n",
        "    nssdl_strong_labeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_strong_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    nssdl_strong_unlabeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_strong_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=(len(X_train_unlabeled)/BATCH_SIZE)*EPOCHS\n",
        "    )\n",
        "\n",
        "    nssdl_strong_best_model, nssdl_strong_history, nssdl_strong_best_f1, nssdl_strong_str = run_model_training('NSSDL Strong', \n",
        "                                                         NSSDL_Strong_model, \n",
        "                                                         train_dataloader, \n",
        "                                                         val_dataloader, \n",
        "                                                         loss_fn, \n",
        "                                                         nssdl_strong_optimizer, \n",
        "                                                         nssdl_strong_labeled_scheduler, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         X_train_unlabeled,\n",
        "                                                         nssdl_strong_unlabeled_scheduler,\n",
        "                                                         None,\n",
        "                                                         fold,\n",
        "                                                         None,\n",
        "                                                         data_class,\n",
        "                                                         10)\n",
        "    time_for_nssdl_strong = time.time()-start\n",
        "    \n",
        "    # train mixtext\n",
        "    start = time.time()\n",
        "    mixtext_best_model, mixtext_history, mixtext_best_f1, mixtext_str = run_model_training('MixText', \n",
        "                                                         MixText_model, \n",
        "                                                         mix_train_loader, \n",
        "                                                         mix_val_loader, \n",
        "                                                         loss_fn, \n",
        "                                                         mix_optimizer, \n",
        "                                                         None, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         mix_train_unlabeled,\n",
        "                                                         None,\n",
        "                                                         train_criterion,\n",
        "                                                         fold,\n",
        "                                                         mix_train_loader_for_train_vali_if_aug,\n",
        "                                                         data_class,\n",
        "                                                         20)\n",
        "    time_for_mixtext = time.time()-start\n",
        "    \n",
        "\n",
        "\n",
        "    # execute evaluation with test set\n",
        "    # traditional\n",
        "    tradi_model = ClassicalBertClassifier()\n",
        "    tradi_model = nn.DataParallel(tradi_model)\n",
        "    tradi_model = tradi_model.to(device)\n",
        "    tradi_model.load_state_dict(tradi_best_model)\n",
        "\n",
        "    tradi_test_acc, tradi_test_precision, tradi_test_recall, tradi_test_f1, tradi_test_loss = eval_model(\n",
        "      tradi_model,\n",
        "      test_dataloader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(validation_set))\n",
        "    \n",
        "\n",
        "    NSSDL_model = ClassicalBertClassifier()\n",
        "    NSSDL_model = nn.DataParallel(NSSDL_model)\n",
        "    NSSDL_model = NSSDL_model.to(device)\n",
        "    NSSDL_model.load_state_dict(nssdl_best_model)\n",
        "\n",
        "    nssdl_test_acc, nssdl_test_precision, nssdl_test_recall, nssdl_test_f1, nssdl_test_loss = eval_model(\n",
        "      NSSDL_model,\n",
        "      test_dataloader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(validation_set))\n",
        "    \n",
        "    NSSDL_Strong_model = ClassicalBertClassifier()\n",
        "    NSSDL_Strong_model = nn.DataParallel(NSSDL_Strong_model)\n",
        "    NSSDL_Strong_model = NSSDL_Strong_model.to(device)\n",
        "    NSSDL_Strong_model.load_state_dict(nssdl_strong_best_model)\n",
        "\n",
        "    nssdl_strong_test_acc, nssdl_strong_test_precision, nssdl_strong_test_recall, nssdl_strong_test_f1, nssdl_strong_test_loss = eval_model(\n",
        "      NSSDL_Strong_model,\n",
        "      test_dataloader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(validation_set))\n",
        "    \n",
        "    MixText_model = MixText(num_labels=3, mix_option=True)\n",
        "    MixText_model = nn.DataParallel(MixText_model)\n",
        "    MixText_model = MixText_model.to(device)\n",
        "    MixText_model.load_state_dict(mixtext_best_model)\n",
        "\n",
        "    mix_text_loader = DataLoader(\n",
        "            dataset=mixtext_loader_labeled(\n",
        "                dataset_text=X_test,\n",
        "                dataset_label=y_test, \n",
        "                tokenizer=tokenizer, \n",
        "                max_seq_len=MAX_LEN\n",
        "                ),\n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=True)\n",
        "\n",
        "\n",
        "    mixtext_test_loss, mixtext_test_acc, mixtext_test_precision, mixtext_test_recall, mixtext_test_f1 = mix_validate(\n",
        "      mix_text_loader,\n",
        "      MixText_model,\n",
        "      loss_fn,\n",
        "      11,\n",
        "      mode='Test Stats')\n",
        "    \n",
        "    torch.cuda.empty_cache() \n",
        "    \n",
        "    # build dict for saving\n",
        "    saving_dict = {\n",
        "      'tradi': {\n",
        "          'hist': tradi_history,\n",
        "          'time': time_for_tradi,\n",
        "          'test': {\n",
        "              'acc': tradi_test_acc,\n",
        "              'precision': tradi_test_precision,\n",
        "              'recall': tradi_test_recall, \n",
        "              'f1': tradi_test_f1, \n",
        "              'loss': tradi_test_loss\n",
        "          }\n",
        "      },\n",
        "      'nssdl': {\n",
        "          'hist': nssdl_history,\n",
        "          'time': time_for_nssdl,\n",
        "          'test': {\n",
        "              'acc': nssdl_test_acc,\n",
        "              'precision': nssdl_test_precision,\n",
        "              'recall': nssdl_test_recall, \n",
        "              'f1': nssdl_test_f1, \n",
        "              'loss': nssdl_test_loss\n",
        "          }\n",
        "      },\n",
        "      'nssdl_strong': {\n",
        "          'hist': nssdl_strong_history,\n",
        "          'time': time_for_nssdl_strong,\n",
        "          'test': {\n",
        "              'acc': nssdl_strong_test_acc,\n",
        "              'precision': nssdl_strong_test_precision,\n",
        "              'recall': nssdl_strong_test_recall, \n",
        "              'f1': nssdl_strong_test_f1, \n",
        "              'loss': nssdl_strong_test_loss\n",
        "          }\n",
        "      },\n",
        "      'mixtext': {\n",
        "          'hist': mixtext_history,\n",
        "          'time': time_for_mixtext,\n",
        "          'test': {\n",
        "              'acc': mixtext_test_acc,\n",
        "              'precision': mixtext_test_precision,\n",
        "              'recall': mixtext_test_recall, \n",
        "              'f1': mixtext_test_f1, \n",
        "              'loss': mixtext_test_loss\n",
        "          }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    save_results_to_excel(saving_dict=saving_dict, fold=fold, dataclass=data_class)\n",
        "\n",
        "    # ---> Log locally\n",
        "    test_txt_string += f\"Fold: {fold}\\n\\n\" + \\\n",
        "    f\"Traditional: Test(acc: {round(tradi_test_acc, 4)}, precision: {round(tradi_test_precision, 4)}, recall: {round(tradi_test_recall, 4)}, f1: {round(tradi_test_f1, 4)}, loss: {round(tradi_test_loss, 4)})\\n\"+\\\n",
        "    f\"NSSDL: Test(acc: {round(nssdl_test_acc, 4)}, precision: {round(nssdl_test_precision, 4)}, recall: {round(nssdl_test_recall, 4)}, f1: {round(nssdl_test_f1, 4)}, loss: {round(nssdl_test_loss, 4)})\\n\"+\\\n",
        "    f\"NSSDL Strong: Test(acc: {round(nssdl_strong_test_acc, 4)}, precision: {round(nssdl_strong_test_precision, 4)}, recall: {round(nssdl_strong_test_recall, 4)}, f1: {round(nssdl_strong_test_f1, 4)}, loss: {round(nssdl_strong_test_loss, 4)})\\n\"+\\\n",
        "    f\"MixText: Test(acc: {round(mixtext_test_acc, 4)}, precision: {round(mixtext_test_precision, 4)}, recall: {round(mixtext_test_recall, 4)}, f1: {round(mixtext_test_f1, 4)}, loss: {round(mixtext_test_loss, 4)})\\n\\n+++++++++++++++++++++++++++++++++++\"\n",
        "\n",
        "    overall_saving_dict['trad'].append(tradi_history)\n",
        "    overall_saving_dict['nssdl'].append(nssdl_history)\n",
        "\n",
        "    with open(f\"/content/drive/My Drive/Colab Notebooks/BA/dataclasses/{data_class}/history_fold_{fold}.txt\", \"w\") as text_file:\n",
        "      append_str=f\"Fold: {fold}\\n\\n\"+\\\n",
        "      \"Traditional:\\n\"+\\\n",
        "      \"-----------\\n\" + tradi_str + \"\\n\\nNSSDL:\\n\"+\\\n",
        "      \"-----------\\n\" + nssdl_str + \"\\n-----------\\nNSSDL_Strong:\\n\" + nssdl_strong_str + \\\n",
        "      \"\\n-----------\\nMixText:\\n\" + mixtext_str + \"\\n\\n\\n+++++++++++++++++++++++++++++++++++\"\n",
        "      text_file.write(append_str)\n",
        "\n",
        "  with open(f\"/content/drive/My Drive/Colab Notebooks/BA/testing_history_data_class_{data_class}.txt\", \"w\") as text_file:\n",
        "        text_file.write(test_txt_string)\n",
        "\n",
        "\n",
        "  # calc mean of logged results\n",
        "\n",
        "  # significance testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data class of 25 labeled samples per fold\n",
            "--------------------------------------------------\n",
            "\n",
            "Fold 1/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 1.0726601055690221, accuracy 0.44, precision 0.21568627450980393, recall 0.21568627450980393, f1 0.21568627450980393\n",
            "{2: 11, 1: 862}\n",
            "(traditional) Val   loss 0.9532792609031886, accuracy 0.5967926689576174, precision 0.2912184489910708, recall 0.3367727052285268, f1 0.25787284274680183\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.6524205016238349, accuracy 0.68, precision 0.8333333333333333, recall 0.5555555555555556, f1 0.5\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.0129491780718711, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.3255565996680941, accuracy 0.84, precision 0.9047619047619048, recall 0.75, f1 0.7807017543859649\n",
            "{2: 196, 1: 677}\n",
            "(traditional) Val   loss 1.005103072313141, accuracy 0.6437571592210768, precision 0.3818341221274329, recall 0.4234412069259303, f1 0.39629073480005156\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.375850112842662, accuracy 0.84, precision 0.5671296296296297, recall 0.5757575757575758, f1 0.5655172413793103\n",
            "{2: 345, 1: 528}\n",
            "(traditional) Val   loss 1.0724493435936975, accuracy 0.6048109965635738, precision 0.3795454545454546, recall 0.438236797609217, f1 0.4042607611739757\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.09021695437175888, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 209, 1: 664}\n",
            "(traditional) Val   loss 1.138486564252951, accuracy 0.6517754868270332, precision 0.38946695874406717, recall 0.43368329990955923, f1 0.4066182321011193\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.04613066817234669, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 166, 1: 707}\n",
            "(traditional) Val   loss 1.4027567478641016, accuracy 0.6563573883161512, precision 0.39308009974835695, recall 0.42547809104373924, f1 0.39784527981858325\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.01553303808239954, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 197, 1: 676}\n",
            "(traditional) Val   loss 1.5800569168723626, accuracy 0.6586483390607102, precision 0.3950329899178005, recall 0.43610029753712665, f1 0.4091696801024884\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.15024744762506867, accuracy 0.96, precision 0.6491228070175438, recall 0.6666666666666666, f1 0.6576576576576577\n",
            "{2: 241, 1: 632}\n",
            "(traditional) Val   loss 1.638699151940227, accuracy 0.6575028636884307, precision 0.3985809828947599, recall 0.44910542251582714, f1 0.42122851364412645\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.08201195984812719, accuracy 0.96, precision 0.6428571428571429, recall 0.6666666666666666, f1 0.654320987654321\n",
            "{2: 293, 1: 580}\n",
            "(traditional) Val   loss 1.6905159073080187, accuracy 0.6357388316151202, precision 0.38989643403554197, recall 0.44695843655381223, f1 0.4163704077041988\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.0025854440838364618, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 357, 1: 516}\n",
            "(traditional) Val   loss 1.7758624300740224, accuracy 0.6013745704467354, precision 0.3800023885523201, recall 0.4391831491748915, f1 0.40395247189989353\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.2596491228070175 Initial loss: 0.9425103834697178\n",
            "FT-Accuracy: 0.9925 Precision: 0.49625 FT-Recall: 0.5 FT-F1: 0.4981179422835633 FT-Loss: 0.06911854607998975\n",
            "PL-Accuracy: 0.72 Precision: 0.36 PL-Recall: 0.5 PL-F1: 0.41860465116279066 PL-Loss: 1.880518454792244\n",
            "(NSSDL) Train loss 0.9748185004361168, accuracy 0.85625, precision 0.428125, recall 0.5, f1 0.458361296723177\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.1210304957965485, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0024351890353136695\n",
            "PL-Accuracy: 0.56 Precision: 0.7708333333333333 PL-Recall: 0.5416666666666666 PL-F1: 0.4282744282744283 PL-Loss: 2.0867402468408858\n",
            "(NSSDL) Train loss 1.0445877179380998, accuracy 0.78, precision 0.8854166666666666, recall 0.7708333333333333, f1 0.7141372141372142\n",
            "{1: 867, 2: 6}\n",
            "(NSSDL) Val   loss 1.9427125519907025, accuracy 0.5967926689576174, precision 0.3106497500961169, recall 0.33605442176870753, f1 0.2549513346326095\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.002351641201439634\n",
            "PL-Accuracy: 0.68 Precision: 0.8333333333333333 PL-Recall: 0.5555555555555556 PL-F1: 0.5 PL-Loss: 1.1578174054988526\n",
            "(NSSDL) Train loss 0.5800845233501462, accuracy 0.8400000000000001, precision 0.9166666666666666, recall 0.7777777777777778, f1 0.75\n",
            "{1: 858, 2: 15}\n",
            "(NSSDL) Val   loss 2.8561366606186374, accuracy 0.6002290950744559, precision 0.31274281274281274, recall 0.3401360544217687, f1 0.2640913915423719\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.001180686746520223\n",
            "PL-Accuracy: 0.88 Precision: 0.9166666666666667 PL-Recall: 0.85 PL-F1: 0.8663101604278074 PL-Loss: 0.21216364071837493\n",
            "(NSSDL) Train loss 0.10667216373244758, accuracy 0.94, precision 0.9583333333333334, recall 0.925, f1 0.9331550802139037\n",
            "{1: 762, 2: 111}\n",
            "(NSSDL) Val   loss 2.546847977175118, accuracy 0.6265750286368843, precision 0.3701260315433544, recall 0.3879490910044172, f1 0.35363728594526167\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.003223186658196937\n",
            "PL-Accuracy: 0.96 Precision: 0.9736842105263157 PL-Recall: 0.9285714285714286 PL-F1: 0.9480249480249481 PL-Loss: 0.05469437358052736\n",
            "(NSSDL) Train loss 0.02895878011936215, accuracy 0.97875, precision 0.7368421052631579, recall 0.7136607142857143, f1 0.7236995828985817\n",
            "{1: 738, 2: 135}\n",
            "(NSSDL) Val   loss 3.2025651751584903, accuracy 0.6277205040091638, precision 0.370581150255947, recall 0.3957741863604787, f1 0.3653044704043322\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0008714565573609434\n",
            "PL-Accuracy: 0.84 Precision: 0.5441176470588235 PL-Recall: 0.6190476190476191 PL-F1: 0.5791666666666666 PL-Loss: 0.7843861996547535\n",
            "(NSSDL) Train loss 0.39262882810605726, accuracy 0.9199999999999999, precision 0.7720588235294117, recall 0.8095238095238095, f1 0.7895833333333333\n",
            "{1: 776, 2: 97}\n",
            "(NSSDL) Val   loss 3.0014462314315904, accuracy 0.6219931271477663, precision 0.3655498281786942, recall 0.38035206375421077, f1 0.34265543739227944\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011301489028483048\n",
            "PL-Accuracy: 0.96 Precision: 0.9722222222222222 PL-Recall: 0.9375 PL-F1: 0.9523809523809523 PL-Loss: 0.035368830734244384\n",
            "(NSSDL) Train loss 0.01774092281226461, accuracy 0.98, precision 0.9861111111111112, recall 0.96875, f1 0.9761904761904762\n",
            "{1: 787, 2: 86}\n",
            "(NSSDL) Val   loss 3.1392513461453095, accuracy 0.6139747995418099, precision 0.36166681441643767, recall 0.37226482115003995, f1 0.33225071673228684\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.3875787878278064e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6296296296296297 PL-Recall: 0.6666666666666666 PL-F1: 0.6470588235294118 PL-Loss: 0.05124941632519559\n",
            "(NSSDL) Train loss 0.025646646056536933, accuracy 0.98, precision 0.8148148148148149, recall 0.8333333333333333, f1 0.8235294117647058\n",
            "{1: 791, 2: 82}\n",
            "(NSSDL) Val   loss 3.3341109156977904, accuracy 0.6071019473081328, precision 0.3435858694870135, recall 0.3641015558439175, f1 0.3202044961131732\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.556349478458287e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0003606793216022197\n",
            "(NSSDL) Train loss 0.0001981214081934013, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 775, 2: 96, 0: 2}\n",
            "(NSSDL) Val   loss 3.36943759754237, accuracy 0.6013745704467354, precision 0.5076433691756271, recall 0.36689751257134345, f1 0.3306837048672817\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.7804551632470974e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.9777777777777779 PL-Recall: 0.8333333333333334 PL-F1: 0.8773946360153256 PL-Loss: 0.40655124066636616\n",
            "(NSSDL) Train loss 0.20328952260899932, accuracy 0.98, precision 0.9888888888888889, recall 0.9166666666666667, f1 0.9386973180076628\n",
            "{1: 739, 2: 123, 0: 11}\n",
            "(NSSDL) Val   loss 3.1661255548208467, accuracy 0.6128293241695304, precision 0.5401541738384539, recall 0.39617358813564846, f1 0.3816000783392088\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00022496927365864394\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0026518306965174687\n",
            "(NSSDL Strong) Train loss 0.0014383999850880564, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 154, 1: 719}\n",
            "(NSSDL Strong) Val   loss 2.5404851550024246, accuracy 0.6586483390607102, precision 0.39754288363467777, recall 0.42460776218001656, f1 0.39682944571984624\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0001490716140688164\n",
            "PL-Accuracy: 0.92 Precision: 0.9166666666666667 PL-Recall: 0.9333333333333333 PL-F1: 0.9188311688311688 PL-Loss: 0.6195922610890453\n",
            "(NSSDL Strong) Train loss 0.3098706663515571, accuracy 0.96, precision 0.9583333333333334, recall 0.9666666666666667, f1 0.9594155844155844\n",
            "{2: 184, 1: 689}\n",
            "(NSSDL Strong) Val   loss 2.6939057658504075, accuracy 0.6506300114547537, precision 0.3876574956353463, recall 0.4258582045534977, f1 0.3986464234808606\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0004479293689109909\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0012613300599956087\n",
            "(NSSDL Strong) Train loss 0.0008546297144532998, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 279, 1: 594}\n",
            "(NSSDL Strong) Val   loss 2.9542922855689207, accuracy 0.6300114547537228, precision 0.38340393179102855, recall 0.4380008650859188, f1 0.4088873342798159\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.465876527523505e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00018771350317235504\n",
            "(NSSDL Strong) Train loss 0.00012118613422379504, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 254, 1: 619}\n",
            "(NSSDL Strong) Val   loss 3.2356370199757034, accuracy 0.6345933562428407, precision 0.38190460441233215, recall 0.43338707351919575, f1 0.4055656714718247\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.00928862291039e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6363636363636364 PL-Recall: 0.6666666666666666 PL-F1: 0.6507936507936508 PL-Loss: 0.22749499308182358\n",
            "(NSSDL Strong) Train loss 0.11376754298402635, accuracy 0.98, precision 0.8181818181818181, recall 0.8333333333333333, f1 0.8253968253968254\n",
            "{2: 193, 1: 680}\n",
            "(NSSDL Strong) Val   loss 3.1956791939750033, accuracy 0.6460481099656358, precision 0.3853195164075993, recall 0.42544401190148506, f1 0.39860715994937435\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.004050976378330233\n",
            "PL-Accuracy: 0.96 Precision: 0.9642857142857143 PL-Recall: 0.9583333333333333 PL-F1: 0.9597423510466989 PL-Loss: 0.2814345128151347\n",
            "(NSSDL Strong) Train loss 0.14274274459673247, accuracy 0.97875, precision 0.7321428571428572, recall 0.7285416666666666, f1 0.7295582844094571\n",
            "{2: 283, 1: 590}\n",
            "(NSSDL Strong) Val   loss 3.244476651197724, accuracy 0.6174112256586484, precision 0.3743886127248408, recall 0.4280628629100966, f1 0.39942481624176845\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0011901854132429434\n",
            "PL-Accuracy: 0.96 Precision: 0.6363636363636364 PL-Recall: 0.6666666666666666 PL-F1: 0.6507936507936508 PL-Loss: 0.38402352267231826\n",
            "(NSSDL Strong) Train loss 0.1926068540427806, accuracy 0.98, precision 0.8181818181818181, recall 0.8333333333333333, f1 0.8253968253968254\n",
            "{2: 452, 1: 421}\n",
            "(NSSDL Strong) Val   loss 3.6048002591342083, accuracy 0.5429553264604811, precision 0.36671711545064073, recall 0.4179203858807492, f1 0.37523733935712317\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000197126947423385\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0001595352410471865\n",
            "(NSSDL Strong) Train loss 0.00017833109423528575, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 351, 1: 522}\n",
            "(NSSDL Strong) Val   loss 3.576698733649886, accuracy 0.5853379152348225, precision 0.36737400530503983, recall 0.42300866396655, f1 0.3903663445276118\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.79611594932794e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.5674603174603174 PL-Recall: 0.6666666666666666 PL-F1: 0.610917537746806 PL-Loss: 0.17915086916764267\n",
            "(NSSDL Strong) Train loss 0.08960941516356798, accuracy 0.96, precision 0.7837301587301587, recall 0.8333333333333333, f1 0.805458768873403\n",
            "{2: 322, 1: 551}\n",
            "(NSSDL Strong) Val   loss 3.135900830478599, accuracy 0.5967926689576174, precision 0.3685337782236701, recall 0.4236850038666719, f1 0.3931486151631531\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.999540317134233e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.011164144668977574\n",
            "(NSSDL Strong) Train loss 0.005612070036074458, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 251, 1: 622}\n",
            "(NSSDL Strong) Val   loss 3.5026376066216054, accuracy 0.6334478808705613, precision 0.38111220712007277, recall 0.4320265293015087, f1 0.4044073299218758\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6131977748870849, accuracy 0.64, precision 0.34782608695652173, recall 0.4444444444444444, f1 0.3902439024390244\n",
            "(MixText) Val   loss 0.9718716790449442, accuracy 0.6048109965635738, precision 0.39134229923703606, recall 0.35419763281034955, f1 0.2990918845358666\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.1970200490951538, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.0631946368827851, accuracy 0.5773195876288659, precision 0.355944721323803, recall 0.4091751536838242, f1 0.38027894654823124\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.2565135610103607, accuracy 0.92, precision 0.9188311688311688, recall 0.9188311688311688, f1 0.9188311688311688\n",
            "(MixText) Val   loss 1.1494438410880639, accuracy 0.5990836197021764, precision 0.366461116820829, recall 0.41850497424403293, f1 0.39075535538765566\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.1030262491852045, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.0599326481370581, accuracy 0.6311569301260023, precision 0.3815909401009687, recall 0.41350320475010816, f1 0.3881141500697787\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.2890038971602917, accuracy 0.92, precision 0.6153846153846154, recall 0.6666666666666666, f1 0.6388888888888888\n",
            "(MixText) Val   loss 1.0977960116009122, accuracy 0.6128293241695304, precision 0.3737192715687339, recall 0.42693038679826456, f1 0.39855604251398574\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04932772234082222, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.259277443686353, accuracy 0.6403207331042382, precision 0.3833033592057382, recall 0.42007785773268846, f1 0.39367953606364203\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.159977093860507, accuracy 0.96, precision 0.6458333333333334, recall 0.6666666666666666, f1 0.6559139784946236\n",
            "(MixText) Val   loss 1.3209250026808161, accuracy 0.5990836197021764, precision 0.37369471171775936, recall 0.43143407652078175, f1 0.3991020308187572\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.044581578075885774, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2252369419162727, accuracy 0.6323024054982818, precision 0.37530957054539454, recall 0.4040894970705045, f1 0.37562978699159205\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.10457602713257075, accuracy 0.96, precision 0.6481481481481481, recall 0.6666666666666666, f1 0.6571428571428571\n",
            "(MixText) Val   loss 1.2847436466120943, accuracy 0.6254295532646048, precision 0.37494437027147304, recall 0.42178443631787976, f1 0.3953307421159678\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.02205783274024725, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2140628523433332, accuracy 0.6242840778923253, precision 0.3711996336996337, recall 0.41395934096181825, f1 0.38773541753674207\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.22201562099158764, accuracy 0.92, precision 0.6296296296296297, recall 0.6666666666666666, f1 0.6470588235294118\n",
            "(MixText) Val   loss 1.3639931784748587, accuracy 0.6071019473081328, precision 0.3596707818930041, recall 0.4050437130536222, f1 0.37901632968088145\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 0, 1, 1], device='cuda:0')\n",
            "tensor([2, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.07334051847457886, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2092135461710847, accuracy 0.6311569301260023, precision 0.37252182845403187, recall 0.4063203701519143, f1 0.37841122176502945\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 2], device='cuda:0')\n",
            "tensor([1, 0, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03318334393203259, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.350708833871087, accuracy 0.6265750286368843, precision 0.37572294723555794, recall 0.4224266970757475, f1 0.3959754808708518\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.013814839497208596, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4493374595928505, accuracy 0.5922107674684994, precision 0.3655231268134494, recall 0.42039767737538175, f1 0.39031591280894906\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.12981184296309947, accuracy 0.92, precision 0.9444444444444445, recall 0.8333333333333334, f1 0.8585858585858586\n",
            "(MixText) Val   loss 1.382154175252115, accuracy 0.6162657502863689, precision 0.36586740586740585, recall 0.41090008257638316, f1 0.38482594309698137\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03554203763604164, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.438227061843681, accuracy 0.5784650630011455, precision 0.3703527423633097, recall 0.4292110678568151, f1 0.3922808462557881\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.02953462492674589, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3421926094775962, accuracy 0.5956471935853379, precision 0.3598101027149441, recall 0.41011364083205537, f1 0.38329154608224375\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03523125201463699, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3825354753072796, accuracy 0.6048109965635738, precision 0.3593433535870376, recall 0.405914041917345, f1 0.37998037998038\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03006066732108593, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3075652520032546, accuracy 0.5441008018327605, precision 0.438750004487863, recall 0.4291406007525069, f1 0.39007187034450697\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.021866103783249855, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3658100676563871, accuracy 0.6174112256586484, precision 0.4786190399397947, recall 0.4168313203911958, f1 0.3938721392270453\n",
            "\n",
            "{1: 271, 2: 214}\n",
            "{1: 405, 2: 74, 0: 6}\n",
            "{1: 339, 2: 146}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Fold 2/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 1.005512569631849, accuracy 0.48, precision 0.5128205128205128, recall 0.5166666666666667, f1 0.4507359307359307\n",
            "{1: 830, 2: 43}\n",
            "(traditional) Val   loss 0.9627779060847139, accuracy 0.584192439862543, precision 0.30037358737274683, recall 0.33689067149017604, f1 0.2757066276803119\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.8620018277849469, accuracy 0.64, precision 0.4444444444444445, recall 0.5555555555555555, f1 0.4743083003952569\n",
            "{1: 120, 2: 753}\n",
            "(traditional) Val   loss 1.0353488233535801, accuracy 0.36769759450171824, precision 0.36393315626383355, recall 0.3684977651947099, f1 0.25008180157916254\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.5785448040281024, accuracy 0.8, precision 0.5942028985507246, recall 0.5, f1 0.5149051490514905\n",
            "{2: 862, 1: 11}\n",
            "(traditional) Val   loss 1.4807446662965975, accuracy 0.2920962199312715, precision 0.39777121563664486, recall 0.3397559409120103, f1 0.16012453625584472\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.6262255587748119, accuracy 0.68, precision 0.5454545454545454, recall 0.43333333333333335, f1 0.4131054131054131\n",
            "{2: 802, 1: 71}\n",
            "(traditional) Val   loss 1.237669249511745, accuracy 0.33906071019473083, precision 0.37549904581269833, recall 0.36034236430603067, f1 0.2175740210124164\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.399135520947831, accuracy 0.88, precision 0.8825396825396825, recall 0.8333333333333334, f1 0.8218390804597702\n",
            "{1: 731, 2: 128, 0: 14}\n",
            "(traditional) Val   loss 0.8735422758751263, accuracy 0.5761741122565864, precision 0.5064175379454107, recall 0.372551812261204, f1 0.3600274577330224\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.3109968272703035, accuracy 0.88, precision 0.9184149184149185, recall 0.7777777777777778, f1 0.8176328502415459\n",
            "{1: 374, 2: 485, 0: 14}\n",
            "(traditional) Val   loss 1.0149509653653184, accuracy 0.4856815578465063, precision 0.5056875608725214, recall 0.40415481743067555, f1 0.3759217361766647\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.05389254992561681, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 229, 2: 644}\n",
            "(traditional) Val   loss 1.400864795428705, accuracy 0.44100801832760594, precision 0.36530237688392236, recall 0.39667335142149346, f1 0.3137345131323825\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.0228525082181607, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 172, 2: 701}\n",
            "(traditional) Val   loss 1.8716795516102553, accuracy 0.4009163802978236, precision 0.36093786285373053, recall 0.38065877603449855, f1 0.2812410443750262\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.06877259284790073, accuracy 0.96, precision 0.9696969696969697, recall 0.8888888888888888, f1 0.9174603174603174\n",
            "{1: 254, 2: 605, 0: 14}\n",
            "(traditional) Val   loss 1.7369908446522608, accuracy 0.44902634593356244, precision 0.5219257716752348, recall 0.4101789611922264, f1 0.3545890362792221\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.05460611501309488, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 323, 2: 430, 0: 120}\n",
            "(traditional) Val   loss 1.4706359652963947, accuracy 0.4662084765177549, precision 0.4454800361597107, recall 0.43835221454900397, f1 0.4201669661238789\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.34139194139194134 Initial loss: 1.0375499810491289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FT-Accuracy: 0.8225 Precision: 0.543859649122807 FT-Recall: 0.5332204027856201 FT-F1: 0.5377328195247271 FT-Loss: 0.47084182404447344\n",
            "PL-Accuracy: 0.6 Precision: 0.40972222222222215 PL-Recall: 0.5555555555555555 PL-F1: 0.46222222222222226 PL-Loss: 1.7032665205853326\n",
            "(NSSDL) Train loss 1.087054172314903, accuracy 0.7112499999999999, precision 0.4767909356725146, recall 0.5443879791705878, f1 0.4999775208734747\n",
            "{1: 559, 2: 314}\n",
            "(NSSDL) Val   loss 1.7791740649196928, accuracy 0.5005727376861397, precision 0.30478485618464884, recall 0.34890487987102353, f1 0.32480033808937464\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 0.9675 Precision: 0.9693595877806405 FT-Recall: 0.9533013418643917 FT-F1: 0.9608360007531538 FT-Loss: 0.13844591065571876\n",
            "PL-Accuracy: 0.8 Precision: 0.5281385281385281 PL-Recall: 0.6410256410256411 PL-F1: 0.5769980506822613 PL-Loss: 0.8892237401035216\n",
            "(NSSDL) Train loss 0.5138348253796201, accuracy 0.88375, precision 0.7487490579595844, recall 0.7971634914450164, f1 0.7689170257177076\n",
            "{1: 322, 2: 551}\n",
            "(NSSDL) Val   loss 3.329487117200869, accuracy 0.39862542955326463, precision 0.2922711576542556, recall 0.3369955303894197, f1 0.28294265620611975\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 0.985 Precision: 0.9821428571428572 FT-Recall: 0.9821428571428572 FT-F1: 0.9821428571428572 FT-Loss: 0.02980575774134195\n",
            "PL-Accuracy: 0.84 Precision: 0.5333333333333333 PL-Recall: 0.6666666666666666 PL-F1: 0.5833333333333334 PL-Loss: 0.764997824693897\n",
            "(NSSDL) Train loss 0.3974017912176195, accuracy 0.9125, precision 0.7577380952380952, recall 0.8244047619047619, f1 0.7827380952380953\n",
            "{1: 280, 2: 587, 0: 6}\n",
            "(NSSDL) Val   loss 4.00611809485424, accuracy 0.41122565864833904, precision 0.3709202022660285, recall 0.36084191114001946, f1 0.29861304124581595\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 0.9825 Precision: 0.9715447154471545 FT-Recall: 0.9876760563380282 FT-F1: 0.9791167893555291 FT-Loss: 0.09078698140503548\n",
            "PL-Accuracy: 0.72 Precision: 0.474025974025974 PL-Recall: 0.6666666666666666 PL-F1: 0.5525925925925926 PL-Loss: 1.4731180559361488\n",
            "(NSSDL) Train loss 0.7819525186705921, accuracy 0.8512500000000001, precision 0.7227853447365642, recall 0.8271713615023475, f1 0.7658546909740609\n",
            "{1: 349, 2: 521, 0: 3}\n",
            "(NSSDL) Val   loss 4.069446904945706, accuracy 0.40778923253150057, precision 0.29366786009565765, recall 0.3342324983943481, f1 0.2884596102401207\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 0.99 Precision: 0.9880379197942522 FT-Recall: 0.9880379197942522 FT-F1: 0.9880379197942522 FT-Loss: 0.016836411653421236\n",
            "PL-Accuracy: 0.8 Precision: 0.532051282051282 PL-Recall: 0.6666666666666666 PL-F1: 0.5912698412698413 PL-Loss: 0.6097453482159055\n",
            "(NSSDL) Train loss 0.31329087993466337, accuracy 0.895, precision 0.760044600922767, recall 0.8273522932304593, f1 0.7896538805320468\n",
            "{1: 308, 2: 559, 0: 6}\n",
            "(NSSDL) Val   loss 4.78895629110732, accuracy 0.413516609392898, precision 0.4734451594916711, recall 0.35833871757325725, f1 0.30981271202126864\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 0.995 Precision: 0.9942790125575675 FT-Recall: 0.9942790125575675 FT-F1: 0.9942790125575675 FT-Loss: 0.02449387471151567\n",
            "PL-Accuracy: 0.92 Precision: 0.9361111111111112 PL-Recall: 0.8472222222222222 PL-F1: 0.8801724137931034 PL-Loss: 0.45894420006827985\n",
            "(NSSDL) Train loss 0.24171903738989775, accuracy 0.9575, precision 0.9651950618343393, recall 0.9207506173898948, f1 0.9372257131753354\n",
            "{1: 261, 2: 595, 0: 17}\n",
            "(NSSDL) Val   loss 4.809850051350602, accuracy 0.4032073310423826, precision 0.43049250351481594, recall 0.36986186884292177, f1 0.3164835164835165\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.9981751824817517 FT-Recall: 0.9960629921259843 FT-F1: 0.9971096386325701 FT-Loss: 0.003120465278698248\n",
            "PL-Accuracy: 0.96 Precision: 0.6428571428571429 PL-Recall: 0.6666666666666666 PL-F1: 0.654320987654321 PL-Loss: 0.04481314738014979\n",
            "(NSSDL) Train loss 0.02396680632942402, accuracy 0.97875, precision 0.8205161626694473, recall 0.8313648293963254, f1 0.8257153131434456\n",
            "{1: 388, 2: 485}\n",
            "(NSSDL) Val   loss 4.463205710371825, accuracy 0.4570446735395189, precision 0.31666666666666665, recall 0.36544112828175584, f1 0.320363685792391\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0019815144979838805\n",
            "PL-Accuracy: 0.92 Precision: 0.9629629629629629 PL-Recall: 0.7777777777777777 PL-F1: 0.8137254901960785 PL-Loss: 0.08282008775287457\n",
            "(NSSDL) Train loss 0.042400801125429224, accuracy 0.96, precision 0.9814814814814814, recall 0.8888888888888888, f1 0.9068627450980393\n",
            "{1: 329, 2: 519, 0: 25}\n",
            "(NSSDL) Val   loss 5.173453459130376, accuracy 0.41580756013745707, precision 0.42265513330327004, recall 0.36334404649770674, f1 0.33612996707659965\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011477134903543628\n",
            "PL-Accuracy: 0.96 Precision: 0.9696969696969697 PL-Recall: 0.8888888888888888 PL-F1: 0.9174603174603174 PL-Loss: 0.0379874322674628\n",
            "(NSSDL) Train loss 0.01905110180824912, accuracy 0.98, precision 0.9848484848484849, recall 0.9444444444444444, f1 0.9587301587301587\n",
            "{1: 240, 2: 578, 0: 55}\n",
            "(NSSDL) Val   loss 5.709157736593791, accuracy 0.3974799541809851, precision 0.40135612177134666, recall 0.3792517668183393, f1 0.33432907329135725\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 7.633107397850836e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0012673022803418072\n",
            "(NSSDL) Train loss 0.0006718166771601578, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 427, 2: 321, 0: 125}\n",
            "(NSSDL) Val   loss 4.558509867360833, accuracy 0.4753722794959908, precision 0.403325886853388, recall 0.4081650209711786, f1 0.4012322231567856\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0029170068766688926\n",
            "PL-Accuracy: 0.88 Precision: 0.6041666666666666 PL-Recall: 0.6333333333333333 PL-F1: 0.6146400483968543 PL-Loss: 0.7023373052340632\n",
            "(NSSDL Strong) Train loss 0.35262715605536604, accuracy 0.94, precision 0.8020833333333333, recall 0.8166666666666667, f1 0.8073200241984271\n",
            "{1: 99, 2: 772, 0: 2}\n",
            "(NSSDL Strong) Val   loss 5.36086741122752, accuracy 0.3722794959908362, precision 0.7222920046754243, recall 0.3845178961793481, f1 0.25972006889611493\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 0.9925 Precision: 0.9818627450980393 FT-Recall: 0.9885928724091655 FT-F1: 0.985192314811387 FT-Loss: 0.04342559338423598\n",
            "PL-Accuracy: 0.96 Precision: 0.6388888888888888 PL-Recall: 0.6666666666666666 PL-F1: 0.6521739130434783 PL-Loss: 0.23882041949296504\n",
            "(NSSDL Strong) Train loss 0.1411230064386005, accuracy 0.9762500000000001, precision 0.8103758169934641, recall 0.827629769537916, f1 0.8186831139274326\n",
            "{1: 387, 2: 486}\n",
            "(NSSDL Strong) Val   loss 3.636726753400813, accuracy 0.4753722794959908, precision 0.32972852266564584, recall 0.38002700116655524, f1 0.3332044866819873\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0015106548703624867\n",
            "PL-Accuracy: 0.96 Precision: 0.9743589743589745 PL-Recall: 0.8888888888888888 PL-F1: 0.92 PL-Loss: 0.08534631485651646\n",
            "(NSSDL Strong) Train loss 0.043428484863439476, accuracy 0.98, precision 0.9871794871794872, recall 0.9444444444444444, f1 0.96\n",
            "{0: 30, 2: 721, 1: 122}\n",
            "(NSSDL Strong) Val   loss 5.628855273821064, accuracy 0.3974799541809851, precision 0.558498745670479, recall 0.42215247662410144, f1 0.33780657957331\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 0.995 Precision: 0.9901960784313726 FT-Recall: 0.9901960784313726 FT-F1: 0.9901960784313726 FT-Loss: 0.035521669261888746\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.003070088043906643\n",
            "(NSSDL Strong) Train loss 0.019295878652897693, accuracy 0.9975, precision 0.9950980392156863, recall 0.9950980392156863, f1 0.9950980392156863\n",
            "{1: 180, 2: 622, 0: 71}\n",
            "(NSSDL Strong) Val   loss 5.321939116805398, accuracy 0.422680412371134, precision 0.4938505871479921, recall 0.43961042513910337, f1 0.3840882702207085\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 0.99 Precision: 0.6554122524182405 FT-Recall: 0.6584821428571429 FT-F1: 0.6569246789309268 FT-Loss: 0.06032880097322049\n",
            "PL-Accuracy: 0.96 Precision: 0.9761904761904763 PL-Recall: 0.9166666666666666 PL-F1: 0.9400352733686067 PL-Loss: 0.06063586481884288\n",
            "(NSSDL Strong) Train loss 0.060482332896031686, accuracy 0.975, precision 0.8158013643043585, recall 0.7875744047619048, f1 0.7984799761497667\n",
            "{1: 169, 2: 656, 0: 48}\n",
            "(NSSDL Strong) Val   loss 5.414944052902455, accuracy 0.41809851088201605, precision 0.4954879652346819, recall 0.4270523213439448, f1 0.36106160099162904\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.223476148079499e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0020427563106620384\n",
            "(NSSDL Strong) Train loss 0.0010524955360714167, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 169, 2: 533, 0: 171}\n",
            "(NSSDL Strong) Val   loss 5.373261525140853, accuracy 0.39862542955326463, precision 0.46983311938382544, recall 0.4471450421040948, f1 0.3828124332772507\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.541791017298238e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0001207245877594687\n",
            "(NSSDL Strong) Train loss 9.307124896622554e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 121, 2: 639, 0: 113}\n",
            "(NSSDL Strong) Val   loss 6.251714851047259, accuracy 0.37915234822451316, precision 0.4846562383529965, recall 0.42682644180685836, f1 0.3498144354762001\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.434809443206177e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00023774351653368546\n",
            "(NSSDL Strong) Train loss 0.0001360458054828736, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 123, 2: 638, 0: 112}\n",
            "(NSSDL Strong) Val   loss 6.3388973304855805, accuracy 0.37915234822451316, precision 0.48307075015443385, recall 0.42610815834703897, f1 0.35034080420796826\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.9363888743318966e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 8.521686764392403e-05\n",
            "(NSSDL Strong) Train loss 5.7290378193621496e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 126, 2: 634, 0: 113}\n",
            "(NSSDL Strong) Val   loss 6.379730092458279, accuracy 0.38258877434135163, precision 0.4863469434810268, recall 0.42973250037849003, f1 0.3554112092885012\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.5622122220738673e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 6.300477967410448e-05\n",
            "(NSSDL Strong) Train loss 4.4313450947421576e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 126, 2: 637, 0: 110}\n",
            "(NSSDL Strong) Val   loss 6.432230008594792, accuracy 0.38258877434135163, precision 0.4859389287960716, recall 0.42803494062064207, f1 0.35394400583496294\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 1.1537662982940673, accuracy 0.48, precision 0.16, recall 0.3333333333333333, f1 0.21621621621621623\n",
            "(MixText) Val   loss 0.9898035105697615, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 1.3668165874481202, accuracy 0.28, precision 0.09333333333333334, recall 0.3333333333333333, f1 0.14583333333333334\n",
            "(MixText) Val   loss 1.4061709005739818, accuracy 0.2806414662084765, precision 0.0935471554028255, recall 0.3333333333333333, f1 0.14609421586165774\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([0, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.8170243716239929, accuracy 0.44, precision 0.4444444444444444, recall 0.42857142857142855, f1 0.3148148148148148\n",
            "(MixText) Val   loss 0.9112512045299884, accuracy 0.6036655211912944, precision 0.36214310285517715, recall 0.3578650728113982, f1 0.3092905391439509\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4880754232406616, accuracy 0.84, precision 0.9215686274509803, recall 0.7037037037037037, f1 0.7472222222222222\n",
            "(MixText) Val   loss 1.046832341133251, accuracy 0.4306987399770905, precision 0.39231003288511274, recall 0.3694091958609114, f1 0.32211590346842883\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.20389048337936402, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.389429305188298, accuracy 0.3711340206185567, precision 0.452578891060756, recall 0.38903300974983335, f1 0.28798596250110486\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 2], device='cuda:0')\n",
            "tensor([1, 1, 0, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.15206214368343354, accuracy 0.92, precision 0.8333333333333334, recall 0.9333333333333332, f1 0.8518518518518517\n",
            "(MixText) Val   loss 1.9993328429579325, accuracy 0.26689576174112256, precision 0.4950497372372373, recall 0.3888909932819356, f1 0.24032585845293988\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 0, 1, 1], device='cuda:0')\n",
            "tensor([2, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.09202030569314956, accuracy 0.96, precision 0.9761904761904763, recall 0.9333333333333332, f1 0.9506172839506174\n",
            "(MixText) Val   loss 1.6650031242276384, accuracy 0.3665521191294387, precision 0.5061381591182915, recall 0.39318951069494634, f1 0.28554934104789453\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04098373100161552, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.06119181610055, accuracy 0.30126002290950743, precision 0.4666107227179617, recall 0.3933380688189437, f1 0.2745389899040675\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 2, 2, 2], device='cuda:0')\n",
            "tensor([0, 2, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.057629323825240136, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.1054918602757575, accuracy 0.31844215349369986, precision 0.5044664581160933, recall 0.3605998939145402, f1 0.21220103374735108\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04156975984573364, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6109924295923554, accuracy 0.3665521191294387, precision 0.49496352413019085, recall 0.4024602879988148, f1 0.3122156806004206\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03921308554708958, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.228362065103046, accuracy 0.286368843069874, precision 0.5053358802929003, recall 0.38498867896666367, f1 0.2587845053185644\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 1], device='cuda:0')\n",
            "tensor([2, 2, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03530129432678222, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.3242908758276655, accuracy 0.30698739977090495, precision 0.5486868686868687, recall 0.3633833685849231, f1 0.2201069007481817\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 0, 1, 2], device='cuda:0')\n",
            "tensor([2, 0, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04527453973889351, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5668287234890774, accuracy 0.34822451317296677, precision 0.4627262354733011, recall 0.42978921076482424, f1 0.3425710933727328\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0484472668915987, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.692965981886559, accuracy 0.3344788087056128, precision 0.47455617382087967, recall 0.41588073598922165, f1 0.3164424611611571\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 0], device='cuda:0')\n",
            "tensor([2, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.009350953679531813, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.077466364920071, accuracy 0.36769759450171824, precision 0.49118942731277526, recall 0.4287622429078046, f1 0.33691782706164\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.029260498434305192, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.475249033681579, accuracy 0.38831615120274915, precision 0.4731044799446287, recall 0.4524646350735943, f1 0.3765957081250107\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04088887110352516, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.436822259002808, accuracy 0.38258877434135163, precision 0.4973076433091365, recall 0.420722715912831, f1 0.3474242957427646\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 0, 2, 1], device='cuda:0')\n",
            "tensor([2, 0, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03438792623579502, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5080622949073132, accuracy 0.3940435280641466, precision 0.4913968688250494, recall 0.40357578466503535, f1 0.3130807600472744\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 0], device='cuda:0')\n",
            "tensor([2, 2, 2, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.013752535171806812, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.9769612410770385, accuracy 0.35967926689576174, precision 0.5064514360208764, recall 0.4155176259752809, f1 0.3197996841632854\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.031422788351774214, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.458787486861798, accuracy 0.38946162657502864, precision 0.5096435185185185, recall 0.4240542947836185, f1 0.35041528638402\n",
            "\n",
            "{1: 181, 2: 251, 0: 53}\n",
            "{1: 256, 0: 56, 2: 173}\n",
            "{0: 55, 2: 368, 1: 62}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 0, 2], device='cuda:0')\n",
            "Fold 3/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.8245418689080647, accuracy 0.6, precision 0.3857142857142857, recall 0.4222222222222222, f1 0.3833333333333333\n",
            "{1: 873}\n",
            "(traditional) Val   loss 0.9772924829155343, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.44567463653428213, accuracy 0.8, precision 0.5972222222222222, recall 0.39999999999999997, f1 0.4056847545219638\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.4020556367287353, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.8917436426106308, accuracy 0.72, precision 0.24, recall 0.3333333333333333, f1 0.27906976744186046\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.6198098400361054, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.42519725885774406, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.4717119830962457, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.11358675287504282, accuracy 0.96, precision 0.9791666666666667, recall 0.75, f1 0.8226950354609929\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.580756369518907, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.3322038353819932, accuracy 0.72, precision 0.24, recall 0.3333333333333333, f1 0.27906976744186046\n",
            "{1: 788, 0: 82, 2: 3}\n",
            "(traditional) Val   loss 1.3410935764962204, accuracy 0.6185567010309279, precision 0.4556594170002613, recall 0.4248460756737335, f1 0.3729885148073713\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.1007499785960785, accuracy 0.96, precision 0.9824561403508771, recall 0.8333333333333334, f1 0.8798798798798799\n",
            "{1: 658, 0: 187, 2: 28}\n",
            "(traditional) Val   loss 1.2208890090845355, accuracy 0.5853379152348225, precision 0.43919618137390354, recall 0.4610019291872942, f1 0.39585603137005004\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.03270945851025837, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 640, 0: 191, 2: 42}\n",
            "(traditional) Val   loss 1.3202923801408646, accuracy 0.5876288659793815, precision 0.465561113188731, recall 0.47044856159925214, f1 0.4138317872516825\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.012270259820590062, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 663, 0: 157, 2: 53}\n",
            "(traditional) Val   loss 1.4008794004442042, accuracy 0.6105383734249714, precision 0.4904155284058717, recall 0.4762425448870043, f1 0.4356764280362156\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.03325235953421465, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 673, 0: 155, 2: 45}\n",
            "(traditional) Val   loss 1.495938750919482, accuracy 0.6162657502863689, precision 0.4996419321108963, recall 0.47971484151455207, f1 0.4354977687076738\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.45634920634920634 Initial loss: 0.9213277229240963\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0069409050207468685\n",
            "PL-Accuracy: 0.76 Precision: 0.25333333333333335 PL-Recall: 0.3333333333333333 PL-F1: 0.2878787878787879 PL-Loss: 1.9493718315955317\n",
            "(NSSDL) Train loss 0.9781563683081393, accuracy 0.88, precision 0.6266666666666667, recall 0.6666666666666666, f1 0.6439393939393939\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.336577522230322, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006776397176145111\n",
            "PL-Accuracy: 0.64 Precision: 0.21333333333333335 PL-Recall: 0.3333333333333333 PL-F1: 0.26016260162601623 PL-Loss: 2.641925008967519\n",
            "(NSSDL) Train loss 1.3213013243425666, accuracy 0.8200000000000001, precision 0.6066666666666667, recall 0.6666666666666666, f1 0.6300813008130082\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 1.5222501262648194, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0017129960191232384\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 1.5655272914113343\n",
            "(NSSDL) Train loss 0.7836201437152288, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 872, 2: 1}\n",
            "(NSSDL) Val   loss 3.093254095308066, accuracy 0.5933562428407789, precision 0.19801223241590216, recall 0.33269107257546565, f1 0.24826264078600527\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0007410158561833669\n",
            "PL-Accuracy: 0.88 Precision: 0.44 PL-Recall: 0.5 PL-F1: 0.46808510638297873 PL-Loss: 1.5523584553136192\n",
            "(NSSDL) Train loss 0.7765497355849013, accuracy 0.94, precision 0.72, recall 0.75, f1 0.7340425531914894\n",
            "{1: 872, 2: 1}\n",
            "(NSSDL) Val   loss 3.2460447536435506, accuracy 0.5933562428407789, precision 0.19801223241590216, recall 0.33269107257546565, f1 0.24826264078600527\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0004857139032537816\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 0.9727988421842123\n",
            "(NSSDL) Train loss 0.48664227804373306, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 870, 2: 2, 0: 1}\n",
            "(NSSDL) Val   loss 3.2475251795708884, accuracy 0.5933562428407789, precision 0.19846743295019156, recall 0.33269107257546565, f1 0.2486201103911687\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006416987175907707\n",
            "PL-Accuracy: 0.92 Precision: 0.9523809523809523 PL-Recall: 0.8333333333333333 PL-F1: 0.875 PL-Loss: 0.547383633161579\n",
            "(NSSDL) Train loss 0.2740126659395849, accuracy 0.96, precision 0.9761904761904762, recall 0.9166666666666666, f1 0.9375\n",
            "{1: 654, 2: 219}\n",
            "(NSSDL) Val   loss 2.3671266291254374, accuracy 0.586483390607102, precision 0.3551101057070643, recall 0.3956378697914619, f1 0.37175652369893486\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00021777844303869643\n",
            "PL-Accuracy: 0.84 Precision: 0.6111111111111112 PL-Recall: 0.6666666666666666 PL-F1: 0.6363636363636364 PL-Loss: 1.3300652917563898\n",
            "(NSSDL) Train loss 0.6651415350997142, accuracy 0.9199999999999999, precision 0.8055555555555556, recall 0.8333333333333333, f1 0.8181818181818181\n",
            "{1: 833, 2: 40}\n",
            "(NSSDL) Val   loss 3.6491776815751544, accuracy 0.5967926689576174, precision 0.3592136854741897, recall 0.34826524058563696, f1 0.2919789612097305\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0004855318806949072\n",
            "PL-Accuracy: 0.92 Precision: 0.6388888888888888 PL-Recall: 0.5 PL-F1: 0.5410628019323672 PL-Loss: 0.6586708966923263\n",
            "(NSSDL) Train loss 0.3295782142865106, accuracy 0.96, precision 0.8194444444444444, recall 0.75, f1 0.7705314009661837\n",
            "{1: 782, 2: 91}\n",
            "(NSSDL) Val   loss 2.977845049612054, accuracy 0.6059564719358533, precision 0.37114377523584685, recall 0.37207869660388243, f1 0.3373000622231983\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0004919618067651755\n",
            "PL-Accuracy: 0.96 Precision: 0.5833333333333334 PL-Recall: 0.6666666666666666 PL-F1: 0.6190476190476191 PL-Loss: 0.05939850830535371\n",
            "(NSSDL) Train loss 0.029945235056059442, accuracy 0.98, precision 0.7916666666666667, recall 0.8333333333333333, f1 0.8095238095238095\n",
            "{1: 871, 2: 2}\n",
            "(NSSDL) Val   loss 4.056018247574585, accuracy 0.5922107674684994, precision 0.19785686949866055, recall 0.33204881181759793, f1 0.2479616306954436\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00010365554819145472\n",
            "PL-Accuracy: 0.96 Precision: 0.5833333333333334 PL-Recall: 0.6666666666666666 PL-F1: 0.6190476190476191 PL-Loss: 0.1862864594430513\n",
            "(NSSDL) Train loss 0.09319505749562138, accuracy 0.98, precision 0.7916666666666667, recall 0.8333333333333333, f1 0.8095238095238095\n",
            "{1: 858, 2: 15}\n",
            "(NSSDL) Val   loss 4.053196907629471, accuracy 0.5933562428407789, precision 0.3540792540792541, recall 0.3377190567942013, f1 0.2653464424706255\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00020350714719825192\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.020086738271077462\n",
            "(NSSDL Strong) Train loss 0.010145122709137857, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 474, 0: 253, 2: 146}\n",
            "(NSSDL Strong) Val   loss 1.787578859432392, accuracy 0.54524627720504, precision 0.46858346052788197, recall 0.49689784807172055, f1 0.45400932383319437\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00024190323634684318\n",
            "PL-Accuracy: 0.92 Precision: 0.3194444444444445 PL-Recall: 0.3333333333333333 PL-F1: 0.326241134751773 PL-Loss: 0.46726650999698904\n",
            "(NSSDL Strong) Train loss 0.23375420661666793, accuracy 0.96, precision 0.6597222222222222, recall 0.6666666666666666, f1 0.6631205673758865\n",
            "{1: 849, 0: 22, 2: 2}\n",
            "(NSSDL Strong) Val   loss 3.528659420217522, accuracy 0.6025200458190149, precision 0.6718956347931613, recall 0.3610083145170533, f1 0.3021742962482215\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00010887172789807665\n",
            "PL-Accuracy: 0.96 Precision: 0.3333333333333333 PL-Recall: 0.3333333333333333 PL-F1: 0.3333333333333333 PL-Loss: 0.06691943914743856\n",
            "(NSSDL Strong) Train loss 0.033514155437668315, accuracy 0.98, precision 0.6666666666666666, recall 0.6666666666666666, f1 0.6666666666666666\n",
            "{1: 847, 2: 25, 0: 1}\n",
            "(NSSDL Strong) Val   loss 3.208214090007318, accuracy 0.6059564719358533, precision 0.6964029909484455, recall 0.3507911856475103, f1 0.28752039674440844\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.394190905259166e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.033110717699465955\n",
            "(NSSDL Strong) Train loss 0.016577329804259272, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 654, 2: 133, 0: 86}\n",
            "(NSSDL Strong) Val   loss 2.3567816313797967, accuracy 0.6288659793814433, precision 0.513322278742871, recall 0.47515999700334427, f1 0.4785406693335082\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.608207685530942e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.013035499101273931\n",
            "(NSSDL Strong) Train loss 0.00655079058906462, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 455, 0: 159, 2: 259}\n",
            "(NSSDL Strong) Val   loss 2.633325508465702, accuracy 0.5406643757159221, precision 0.4584643528039754, recall 0.47768110797376173, f1 0.463040190008531\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.326576894731261e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6666666666666666 PL-Recall: 0.6 PL-F1: 0.6296296296296297 PL-Loss: 0.15891475256181106\n",
            "(NSSDL Strong) Train loss 0.07948400916537919, accuracy 0.98, precision 0.8333333333333333, recall 0.8, f1 0.8148148148148149\n",
            "{1: 654, 0: 207, 2: 12}\n",
            "(NSSDL Strong) Val   loss 3.5374961097087683, accuracy 0.5853379152348225, precision 0.45842382070942106, recall 0.4670738847588672, f1 0.3868681042311286\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003247556499991333\n",
            "PL-Accuracy: 0.72 Precision: 0.24 PL-Recall: 0.3333333333333333 PL-F1: 0.27906976744186046 PL-Loss: 3.2766954539895647\n",
            "(NSSDL Strong) Train loss 1.6385101048197819, accuracy 0.86, precision 0.62, recall 0.6666666666666666, f1 0.6395348837209303\n",
            "{1: 872, 0: 1}\n",
            "(NSSDL Strong) Val   loss 2.5480547450327626, accuracy 0.5956471935853379, precision 0.5317278287461774, recall 0.3363914373088685, f1 0.25480251835392026\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0007113347826089012\n",
            "PL-Accuracy: 0.84 Precision: 0.9393939393939394 PL-Recall: 0.6388888888888888 PL-F1: 0.7000000000000001 PL-Loss: 0.9653586063941475\n",
            "(NSSDL Strong) Train loss 0.4830349705883782, accuracy 0.9199999999999999, precision 0.9696969696969697, recall 0.8194444444444444, f1 0.8500000000000001\n",
            "{1: 611, 0: 175, 2: 87}\n",
            "(NSSDL Strong) Val   loss 2.1532185473563716, accuracy 0.6013745704467354, precision 0.4726672202827742, recall 0.4838372633174109, f1 0.44865709879277804\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003029956434693304\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0003233641733199225\n",
            "(NSSDL Strong) Train loss 0.00031317990839462647, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 699, 2: 21, 0: 153}\n",
            "(NSSDL Strong) Val   loss 3.3206174681770655, accuracy 0.6002290950744559, precision 0.4325320018326835, recall 0.44786972291877625, f1 0.3883975095235676\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.0752109748136716e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.9841269841269842 PL-Recall: 0.8333333333333334 PL-F1: 0.8807588075880758 PL-Loss: 0.24754201216585767\n",
            "(NSSDL Strong) Train loss 0.1238013821378029, accuracy 0.98, precision 0.9920634920634921, recall 0.9166666666666667, f1 0.9403794037940378\n",
            "{1: 517, 0: 202, 2: 154}\n",
            "(NSSDL Strong) Val   loss 2.9267203700629043, accuracy 0.5578465063001146, precision 0.45439543954395445, recall 0.4858762398031635, f1 0.45342274315027736\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.4689951030910015, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "(MixText) Val   loss 1.4605738096502434, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.7848437821865082, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "(MixText) Val   loss 1.489219807553503, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.6983965700864792, accuracy 0.8, precision 0.4, recall 0.5, f1 0.4444444444444445\n",
            "(MixText) Val   loss 1.2755643914092039, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.32706906370818617, accuracy 0.92, precision 0.3066666666666667, recall 0.3333333333333333, f1 0.3194444444444445\n",
            "(MixText) Val   loss 1.9361597589309938, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.13556429609656334, accuracy 0.96, precision 0.5833333333333334, recall 0.6666666666666666, f1 0.6190476190476191\n",
            "(MixText) Val   loss 1.3102905821985429, accuracy 0.6048109965635738, precision 0.3132463951015448, recall 0.3656879555484848, f1 0.30508666591140815\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.19846168234944345, accuracy 0.92, precision 0.6376811594202899, recall 0.6666666666666666, f1 0.6515151515151515\n",
            "(MixText) Val   loss 1.2289226765681447, accuracy 0.6082474226804123, precision 0.31908032248432366, recall 0.40626822930476747, f1 0.34986078135968895\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.1089907318353653, accuracy 0.96, precision 0.6527777777777778, recall 0.6666666666666666, f1 0.6595744680851063\n",
            "(MixText) Val   loss 1.5633986583758328, accuracy 0.6059564719358533, precision 0.3260714754221248, recall 0.45088472892471404, f1 0.3784262048192771\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.053844787366688254, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.256772479870253, accuracy 0.6082474226804123, precision 0.4613927632990733, recall 0.4411362183239243, f1 0.41905439132192795\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.5408307348750532, accuracy 0.8, precision 0.42857142857142855, recall 0.6666666666666666, f1 0.48148148148148145\n",
            "(MixText) Val   loss 1.6889534651814935, accuracy 0.6059564719358533, precision 0.32648087781776614, recall 0.45088472892471404, f1 0.37868840420063155\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.031324880421161654, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.0419483007127066, accuracy 0.5784650630011455, precision 0.45164123685953567, recall 0.45323347206694814, f1 0.4489472226033681\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.03219966636970639, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.610244408056763, accuracy 0.5945017182130584, precision 0.31771076355885824, recall 0.4468779645637046, f1 0.3712307950031111\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.010984025914222002, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.598787796294225, accuracy 0.6002290950744559, precision 0.4879025622943112, recall 0.45152583527268186, f1 0.37970742379647393\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.012310224492102862, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5685313477774043, accuracy 0.6105383734249714, precision 0.5327069992135715, recall 0.44255012994927695, f1 0.3844497961696685\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.013410696424543858, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.8362458492890599, accuracy 0.5624284077892325, precision 0.4717980059101489, recall 0.44652384932690087, f1 0.3607998997013225\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.00722527289763093, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7966968477484935, accuracy 0.5292096219931272, precision 0.4913770369699349, recall 0.4542115699048706, f1 0.3576609215187896\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.008565068077296019, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4355090022031622, accuracy 0.5933562428407789, precision 0.5306754586041512, recall 0.4531575455658214, f1 0.39701588897541956\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.01256263818591833, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6090986743603513, accuracy 0.5498281786941581, precision 0.5261134614254656, recall 0.45800274821706805, f1 0.3785110782974587\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 1, 0, 2], device='cuda:0')\n",
            "tensor([0, 1, 0, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.022085768878459932, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7850613724237643, accuracy 0.586483390607102, precision 0.5573031814312728, recall 0.434873616767419, f1 0.3665025942185645\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.00842581626959145, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.555758488601349, accuracy 0.581901489117984, precision 0.527145467597729, recall 0.4583568634538717, f1 0.3884767090873961\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.027710427325218917, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5318249600917049, accuracy 0.5967926689576174, precision 0.4803204753738984, recall 0.4351039936930739, f1 0.3689077505968003\n",
            "\n",
            "{1: 355, 0: 107, 2: 23}\n",
            "{1: 481, 2: 4}\n",
            "{1: 275, 2: 89, 0: 121}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 1, 0, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Fold 4/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.8275145292282104, accuracy 0.64, precision 0.2807017543859649, recall 0.25396825396825395, f1 0.26666666666666666\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.0309096452308029, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.5675400218793324, accuracy 0.76, precision 0.25333333333333335, recall 0.3333333333333333, f1 0.2878787878787879\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.195915168210796, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.4597868892763342, accuracy 0.8, precision 0.4, recall 0.5, f1 0.4444444444444445\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.4774234318876103, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.6246943181114537, accuracy 0.76, precision 0.5833333333333334, recall 0.3888888888888889, f1 0.38095238095238093\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3996943665676873, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.34679101234568016, accuracy 0.84, precision 0.6060606060606061, recall 0.5, f1 0.5222222222222223\n",
            "{2: 41, 1: 832}\n",
            "(traditional) Val   loss 1.1235249408235832, accuracy 0.6093928980526919, precision 0.3677298311444653, recall 0.356048392382001, f1 0.2992724028993459\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.13266003890229122, accuracy 0.96, precision 0.9772727272727273, recall 0.875, f1 0.9169435215946844\n",
            "{2: 184, 1: 689}\n",
            "(traditional) Val   loss 1.0977349909507248, accuracy 0.6506300114547537, precision 0.3876574956353463, recall 0.4258582045534977, f1 0.3986464234808606\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.15837459292794978, accuracy 0.92, precision 0.5404040404040403, recall 0.6666666666666666, f1 0.592248062015504\n",
            "{2: 91, 1: 782}\n",
            "(traditional) Val   loss 1.5382988609747799, accuracy 0.6334478808705613, precision 0.3910841928744742, recall 0.3896478051721652, f1 0.3540133962885692\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.13678000651166908, accuracy 0.96, precision 0.65, recall 0.6666666666666666, f1 0.6581196581196581\n",
            "{2: 107, 1: 766}\n",
            "(traditional) Val   loss 1.7275607675945464, accuracy 0.6391752577319587, precision 0.38754544789048584, recall 0.39716880972041996, f1 0.36375132649451714\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.16851219455046312, accuracy 0.96, precision 0.6111111111111112, recall 0.6666666666666666, f1 0.6363636363636364\n",
            "{2: 253, 1: 620}\n",
            "(traditional) Val   loss 1.474624696430074, accuracy 0.6391752577319587, precision 0.38266819669344215, recall 0.43451954963102773, f1 0.4064604922470097\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.13341433662987714, accuracy 0.96, precision 0.6111111111111112, recall 0.6666666666666666, f1 0.6363636363636364\n",
            "{2: 374, 1: 499}\n",
            "(traditional) Val   loss 1.462116293158365, accuracy 0.6105383734249714, precision 0.3881952854014625, recall 0.4457578021574719, f1 0.4098367246324374\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.32222222222222224 Initial loss: 1.0750427501542228\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.006068264174027718\n",
            "PL-Accuracy: 0.6 Precision: 0.19999999999999998 PL-Recall: 0.3333333333333333 PL-F1: 0.24999999999999997 PL-Loss: 2.2515900092465535\n",
            "(NSSDL) Train loss 1.1288291367102905, accuracy 0.8, precision 0.6, recall 0.6666666666666666, f1 0.625\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 0.9521346239194478, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.020538206042620005\n",
            "PL-Accuracy: 0.68 Precision: 0.22666666666666668 PL-Recall: 0.3333333333333333 PL-F1: 0.2698412698412698 PL-Loss: 1.9504959047813568\n",
            "(NSSDL) Train loss 0.9855170554119884, accuracy 0.8400000000000001, precision 0.6133333333333333, recall 0.6666666666666666, f1 0.6349206349206349\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.3795411084692493, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0010315482704754686\n",
            "PL-Accuracy: 0.8 Precision: 0.26666666666666666 PL-Recall: 0.3333333333333333 PL-F1: 0.29629629629629634 PL-Loss: 1.3108669907253767\n",
            "(NSSDL) Train loss 0.6559492694979261, accuracy 0.9, precision 0.6333333333333333, recall 0.6666666666666666, f1 0.6481481481481481\n",
            "{1: 862, 0: 11}\n",
            "(NSSDL) Val   loss 2.5881260960440198, accuracy 0.5967926689576174, precision 0.3510511143921817, recall 0.34669707093740604, f1 0.2768726365757503\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005467230774956988\n",
            "PL-Accuracy: 0.76 Precision: 0.5909090909090909 PL-Recall: 0.4166666666666667 PL-F1: 0.4239316239316239 PL-Loss: 0.7725297178112669\n",
            "(NSSDL) Train loss 0.3865382204443813, accuracy 0.88, precision 0.7954545454545454, recall 0.7083333333333334, f1 0.711965811965812\n",
            "{1: 872, 2: 1}\n",
            "(NSSDL) Val   loss 3.040531312040073, accuracy 0.5956471935853379, precision 0.5317278287461774, recall 0.3346938775510204, f1 0.2514519393935852\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003154418708436424\n",
            "PL-Accuracy: 0.76 Precision: 0.5166666666666667 PL-Recall: 0.5555555555555555 PL-F1: 0.5281385281385281 PL-Loss: 1.4322150621223304\n",
            "(NSSDL) Train loss 0.716265251996587, accuracy 0.88, precision 0.7583333333333333, recall 0.7777777777777777, f1 0.7640692640692641\n",
            "{1: 829, 2: 44}\n",
            "(NSSDL) Val   loss 3.078309087062544, accuracy 0.6025200458190149, precision 0.34779946631575104, recall 0.35147654437497544, f1 0.29457113618706343\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00034127691775211134\n",
            "PL-Accuracy: 0.84 Precision: 0.45652173913043476 PL-Recall: 0.6507936507936508 PL-F1: 0.5252525252525252 PL-Loss: 0.8903862652275295\n",
            "(NSSDL) Train loss 0.4453637710726408, accuracy 0.9199999999999999, precision 0.7282608695652174, recall 0.8253968253968254, f1 0.7626262626262625\n",
            "{1: 838, 2: 35}\n",
            "(NSSDL) Val   loss 3.1225126080414185, accuracy 0.6036655211912944, precision 0.35564268666893967, recall 0.349963954753385, f1 0.28913920763589146\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002649863365877536\n",
            "PL-Accuracy: 0.88 Precision: 0.5 PL-Recall: 0.6666666666666666 PL-F1: 0.5555555555555555 PL-Loss: 0.4643119149571118\n",
            "(NSSDL) Train loss 0.2322884506468498, accuracy 0.94, precision 0.75, recall 0.8333333333333333, f1 0.7777777777777777\n",
            "{1: 865, 2: 8}\n",
            "(NSSDL) Val   loss 3.9301389004802196, accuracy 0.5945017182130584, precision 0.32384393063583816, recall 0.33548818371279143, f1 0.25646005163471863\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.05436860860209e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.9710144927536232 PL-Recall: 0.6666666666666666 PL-F1: 0.7626262626262625 PL-Loss: 0.27578108097752974\n",
            "(NSSDL) Train loss 0.1379208123318079, accuracy 0.96, precision 0.9855072463768115, recall 0.8333333333333333, f1 0.8813131313131313\n",
            "{2: 79, 1: 785, 0: 9}\n",
            "(NSSDL) Val   loss 2.711822885602657, accuracy 0.5990836197021764, precision 0.5669828984027162, recall 0.3741007898448485, f1 0.3492433024338331\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.0030540695444960875\n",
            "PL-Accuracy: 0.84 Precision: 0.6086956521739131 PL-Recall: 0.4666666666666666 PL-F1: 0.4920634920634921 PL-Loss: 1.1707643633370546\n",
            "(NSSDL) Train loss 0.5869092164407753, accuracy 0.91875, precision 0.5543478260869565, recall 0.4827083333333333, f1 0.4957188549178537\n",
            "{1: 869, 2: 4}\n",
            "(NSSDL) Val   loss 4.3313994701248335, accuracy 0.5945017182130584, precision 0.28202915228231684, recall 0.3340516167931527, f1 0.25147660767951735\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003029515143134631\n",
            "PL-Accuracy: 0.92 Precision: 0.6376811594202899 PL-Recall: 0.5555555555555555 PL-F1: 0.5848484848484848 PL-Loss: 0.5970462089348335\n",
            "(NSSDL) Train loss 0.2986745802245735, accuracy 0.96, precision 0.818840579710145, recall 0.7777777777777777, f1 0.7924242424242425\n",
            "{1: 818, 2: 51, 0: 4}\n",
            "(NSSDL) Val   loss 3.0444223944314506, accuracy 0.6013745704467354, precision 0.6743372165492114, recall 0.3619342234074164, f1 0.32021050279633423\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006646943564919638\n",
            "PL-Accuracy: 0.92 Precision: 0.5714285714285715 PL-Recall: 0.6666666666666666 PL-F1: 0.611111111111111 PL-Loss: 0.19459962904095715\n",
            "(NSSDL Strong) Train loss 0.09763216169872456, accuracy 0.96, precision 0.7857142857142858, recall 0.8333333333333333, f1 0.8055555555555556\n",
            "{2: 144, 1: 716, 0: 13}\n",
            "(NSSDL Strong) Val   loss 2.0830424329284956, accuracy 0.5979381443298969, precision 0.49392995272883544, recall 0.3885424817431878, f1 0.37472458494303845\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00010896852078076335\n",
            "PL-Accuracy: 0.96 Precision: 0.975 PL-Recall: 0.9166666666666667 PL-F1: 0.9417249417249417 PL-Loss: 0.044781649778347594\n",
            "(NSSDL Strong) Train loss 0.02244530914956418, accuracy 0.98, precision 0.9875, recall 0.9583333333333334, f1 0.9708624708624709\n",
            "{2: 347, 1: 521, 0: 5}\n",
            "(NSSDL Strong) Val   loss 1.9517832609148937, accuracy 0.6013745704467354, precision 0.6449147339133897, recall 0.4474099551259227, f1 0.425280239753924\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 8.366741032659774e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.6315789473684211 PL-Recall: 0.6666666666666666 PL-F1: 0.6481481481481481 PL-Loss: 0.5607771849541417\n",
            "(NSSDL Strong) Train loss 0.2804304261822342, accuracy 0.96, precision 0.8157894736842106, recall 0.8333333333333333, f1 0.8240740740740741\n",
            "{2: 174, 1: 684, 0: 15}\n",
            "(NSSDL Strong) Val   loss 2.0684188340856218, accuracy 0.6666666666666666, precision 0.6266720440949116, recall 0.46116313771977824, f1 0.4619976947904914\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0001333480739231163\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.007938374486652069\n",
            "(NSSDL Strong) Train loss 0.004035861280287593, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 76, 1: 797}\n",
            "(NSSDL Strong) Val   loss 3.0464025797496084, accuracy 0.6277205040091638, precision 0.38790200092451954, recall 0.3806902337042717, f1 0.3404192193215856\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.808702638205432e-05\n",
            "PL-Accuracy: 0.88 Precision: 0.8851540616246498 PL-Recall: 0.75 PL-F1: 0.7343434343434344 PL-Loss: 0.23203157445407538\n",
            "(NSSDL Strong) Train loss 0.11604483074022871, accuracy 0.94, precision 0.9425770308123249, recall 0.875, f1 0.8671717171717173\n",
            "{2: 36, 1: 751, 0: 86}\n",
            "(NSSDL Strong) Val   loss 2.609815893440698, accuracy 0.6403207331042382, precision 0.5778562834968247, recall 0.46493048227758343, f1 0.4430050062521271\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011810900739874342\n",
            "PL-Accuracy: 0.84 Precision: 0.3043478260869565 PL-Recall: 0.3333333333333333 PL-F1: 0.3181818181818182 PL-Loss: 0.8257533968738114\n",
            "(NSSDL Strong) Train loss 0.4129357529406051, accuracy 0.9199999999999999, precision 0.6521739130434783, recall 0.6666666666666666, f1 0.6590909090909091\n",
            "{1: 864, 0: 8, 2: 1}\n",
            "(NSSDL Strong) Val   loss 1.8948063263892871, accuracy 0.5967926689576174, precision 0.6165123456790124, recall 0.3401678247442231, f1 0.2638047610800376\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00024753499284997817\n",
            "PL-Accuracy: 0.84 Precision: 0.5499999999999999 PL-Recall: 0.6666666666666666 PL-F1: 0.6026026026026026 PL-Loss: 0.6132039304429782\n",
            "(NSSDL Strong) Train loss 0.3067257327179141, accuracy 0.9199999999999999, precision 0.7749999999999999, recall 0.8333333333333333, f1 0.8013013013013013\n",
            "{2: 147, 1: 631, 0: 95}\n",
            "(NSSDL Strong) Val   loss 1.827406664154815, accuracy 0.6643757159221076, precision 0.5813909916289479, recall 0.5476966456095086, f1 0.5533685358201251\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002326358764548786\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0425997557280685\n",
            "(NSSDL Strong) Train loss 0.021416195802261687, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 100, 1: 756, 0: 17}\n",
            "(NSSDL Strong) Val   loss 3.3732158578544484, accuracy 0.6334478808705613, precision 0.6155586679116091, recall 0.41935620724399425, f1 0.41543295579101197\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.84988503922068e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0006452061560204518\n",
            "(NSSDL Strong) Train loss 0.0003468525032063293, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 81, 1: 756, 0: 36}\n",
            "(NSSDL Strong) Val   loss 3.4060975271185185, accuracy 0.6231386025200458, precision 0.5396825396825397, recall 0.42114907796920537, f1 0.4166622282507249\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.9139049058576346e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0007000895758600174\n",
            "(NSSDL Strong) Train loss 0.00036961431245929687, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 86, 1: 745, 0: 42}\n",
            "(NSSDL Strong) Val   loss 3.4377826088105863, accuracy 0.6300114547537228, precision 0.5554755339522001, recall 0.43753914922635895, f1 0.4389657415809569\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.7249621635675431, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "(MixText) Val   loss 1.2444140247975017, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.5127503082156182, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "(MixText) Val   loss 1.1783388702183928, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.9944445845484734, accuracy 0.72, precision 0.24, recall 0.3333333333333333, f1 0.27906976744186046\n",
            "(MixText) Val   loss 1.2721377368611022, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.574797294139862, accuracy 0.76, precision 0.25333333333333335, recall 0.3333333333333333, f1 0.2878787878787879\n",
            "(MixText) Val   loss 1.0648406533372907, accuracy 0.5899198167239404, precision 0.31142970927917163, recall 0.33938369181969513, f1 0.27576571942458633\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6880771714448929, accuracy 0.8, precision 0.4, recall 0.5, f1 0.4444444444444445\n",
            "(MixText) Val   loss 1.5267129047578554, accuracy 0.5899198167239404, precision 0.19754507096279247, recall 0.33076429030186255, f1 0.24735830931796346\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.2797576549649239, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 1.3125923601622433, accuracy 0.5899198167239404, precision 0.25317185697808536, recall 0.33148257376168194, f1 0.24989028973092717\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.9518991786241532, accuracy 0.72, precision 0.5694444444444445, recall 0.38095238095238093, f1 0.35975609756097554\n",
            "(MixText) Val   loss 1.5384783080319222, accuracy 0.5910652920962199, precision 0.293082591004069, recall 0.332843117979369, f1 0.2527037610070101\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.25297023624181747, accuracy 0.92, precision 0.6333333333333333, recall 0.6111111111111112, f1 0.6188197767145135\n",
            "(MixText) Val   loss 1.397267011876505, accuracy 0.5784650630011455, precision 0.25225755791126814, recall 0.32721481656246315, f1 0.2540653057675167\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.11350088298320771, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2314670024458896, accuracy 0.6128293241695304, precision 0.5173699832236417, recall 0.3904251078381185, f1 0.3529033741158589\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03352700725197792, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4025358426697203, accuracy 0.6185567010309279, precision 0.46621857645940284, recall 0.43085333619049787, f1 0.38836444079439736\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.023749640919268133, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7487304776326624, accuracy 0.5853379152348225, precision 0.3868691423435949, recall 0.34504145473925546, f1 0.29240017279986213\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.008512938814237713, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6770575255078752, accuracy 0.6013745704467354, precision 0.4339805448978801, recall 0.42696569250103744, f1 0.3950674042496663\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.02090831018984318, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3781337031042726, accuracy 0.581901489117984, precision 0.42814807300339086, recall 0.40873503490947566, f1 0.39444440825431887\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.01812835793942213, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.8312677231927712, accuracy 0.5899198167239404, precision 0.38887671899720094, recall 0.3512666101246318, f1 0.2969610248693381\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.018465827852487564, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.1354446047394273, accuracy 0.5876288659793815, precision 0.4607490121508813, recall 0.43617167854927236, f1 0.43779191804355994\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.008307743519544601, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7614397850284462, accuracy 0.588774341351661, precision 0.43181894960551087, recall 0.4213373910841316, f1 0.40061331876776496\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.015609753597527743, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4587458651662215, accuracy 0.6013745704467354, precision 0.43434121354895766, recall 0.4047658129204202, f1 0.3758671802143871\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.019801225811243058, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4682983076893315, accuracy 0.5990836197021764, precision 0.43806980056980055, recall 0.39114108247813806, f1 0.36255317225497025\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.010380974784493447, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.422718512139711, accuracy 0.5899198167239404, precision 0.4453789184309864, recall 0.4375855901746249, f1 0.42252751062819716\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.01600531967356801, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5244366682826926, accuracy 0.5922107674684994, precision 0.4259592052888142, recall 0.4280958597930697, f1 0.40251658704019505\n",
            "\n",
            "{1: 271, 2: 214}\n",
            "{1: 457, 2: 28}\n",
            "{1: 420, 2: 41, 0: 24}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Fold 5/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Train loss 0.7153922840952873, accuracy 0.72, precision 0.3, recall 0.2608695652173913, f1 0.27906976744186046\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.2204030513014967, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.4690110808504479, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.9793927582176445, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 1.2141647807189397, accuracy 0.68, precision 0.22666666666666668, recall 0.3333333333333333, f1 0.2698412698412698\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.8133663496079102, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.5167000291070768, accuracy 0.84, precision 0.9166666666666667, recall 0.6, f1 0.6212121212121212\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.6855195179224423, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.44614810882402317, accuracy 0.84, precision 0.6111111111111112, recall 0.4166666666666667, f1 0.43636363636363634\n",
            "{1: 850, 0: 23}\n",
            "(traditional) Val   loss 1.5851042929135228, accuracy 0.6116838487972509, precision 0.43502131287297524, recall 0.3816207361840283, f1 0.33306033306033306\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.058663135527500083, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 762, 0: 111}\n",
            "(traditional) Val   loss 1.596808532801495, accuracy 0.6449026345933563, precision 0.38995294507105527, recall 0.49688002686889043, f1 0.4335533319139877\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.03571587468364409, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 739, 0: 134}\n",
            "(traditional) Val   loss 1.6801944439465257, accuracy 0.6517754868270332, precision 0.3910622799399484, recall 0.5248920235927713, f1 0.44763063717312085\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.13304127766085522, accuracy 0.96, precision 0.5555555555555555, recall 0.6666666666666666, f1 0.6\n",
            "{1: 712, 0: 161}\n",
            "(traditional) Val   loss 1.6891390717860533, accuracy 0.6563573883161512, precision 0.3916300276827878, recall 0.5540353420185843, f1 0.458739757900332\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.10608889488503337, accuracy 0.96, precision 0.6515151515151515, recall 0.6666666666666666, f1 0.6589147286821705\n",
            "{1: 695, 0: 178}\n",
            "(traditional) Val   loss 1.7300694650772652, accuracy 0.6586483390607102, precision 0.3914019346320696, recall 0.5698149228403245, f1 0.46298028995823026\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.003351803303563169, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 680, 0: 193}\n",
            "(traditional) Val   loss 1.7907146092846598, accuracy 0.6597938144329897, precision 0.39243878898709744, recall 0.5849522429041971, f1 0.467249197731001\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.29411764705882354 Initial loss: 0.7364937577928815\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0035603087976778624\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 1.3945170069782762\n",
            "(NSSDL) Train loss 0.699038657887977, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.572272822370627, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006379150113207288\n",
            "PL-Accuracy: 0.88 Precision: 0.29333333333333333 PL-Recall: 0.3333333333333333 PL-F1: 0.3120567375886525 PL-Loss: 0.9178347531997133\n",
            "(NSSDL) Train loss 0.459236334105517, accuracy 0.94, precision 0.6466666666666667, recall 0.6666666666666666, f1 0.6560283687943262\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.1770164063537907, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005637344950810075\n",
            "PL-Accuracy: 0.88 Precision: 0.29333333333333333 PL-Recall: 0.3333333333333333 PL-F1: 0.3120567375886525 PL-Loss: 0.7990433252146302\n",
            "(NSSDL) Train loss 0.39980352985485557, accuracy 0.94, precision 0.6466666666666667, recall 0.6666666666666666, f1 0.6560283687943262\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.948835647689151, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0004879595489910571\n",
            "PL-Accuracy: 0.92 Precision: 0.46 PL-Recall: 0.5 PL-F1: 0.4791666666666667 PL-Loss: 0.4299434164557689\n",
            "(NSSDL) Train loss 0.21521568800237997, accuracy 0.96, precision 0.73, recall 0.75, f1 0.7395833333333334\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.283408678173744, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002980027459489065\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 0.6045924019459302\n",
            "(NSSDL) Train loss 0.3024452023459396, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.7669837827445805, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002280892472117557\n",
            "PL-Accuracy: 0.88 Precision: 0.625 PL-Recall: 0.6666666666666666 PL-F1: 0.6444444444444445 PL-Loss: 0.8396328234864215\n",
            "(NSSDL) Train loss 0.41993045636681664, accuracy 0.94, precision 0.8125, recall 0.8333333333333333, f1 0.8222222222222222\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.436392485795885, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005127317689402844\n",
            "PL-Accuracy: 0.92 Precision: 0.6388888888888888 PL-Recall: 0.5 PL-F1: 0.5410628019323672 PL-Loss: 0.35497085343688795\n",
            "(NSSDL) Train loss 0.17774179260291412, accuracy 0.96, precision 0.8194444444444444, recall 0.75, f1 0.7705314009661837\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.7488731889239366, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002432850627337757\n",
            "PL-Accuracy: 0.88 Precision: 0.9375 PL-Recall: 0.625 PL-F1: 0.6666666666666667 PL-Loss: 0.9858493473314281\n",
            "(NSSDL) Train loss 0.49304631619708095, accuracy 0.94, precision 0.96875, recall 0.8125, f1 0.8333333333333334\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.0627543169613842, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005247279559262097\n",
            "PL-Accuracy: 0.96 Precision: 0.9791666666666667 PL-Recall: 0.75 PL-F1: 0.8226950354609929 PL-Loss: 0.2862238685879025\n",
            "(NSSDL) Train loss 0.14337429827191436, accuracy 0.98, precision 0.9895833333333334, recall 0.875, f1 0.9113475177304964\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.575860873805016, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00018695554746955167\n",
            "PL-Accuracy: 0.8 Precision: 0.5972222222222222 PL-Recall: 0.4444444444444444 PL-F1: 0.46124031007751937 PL-Loss: 1.0890997031736853\n",
            "(NSSDL) Train loss 0.5446433293605775, accuracy 0.9, precision 0.7986111111111112, recall 0.7222222222222222, f1 0.7306201550387597\n",
            "{1: 679, 0: 192, 2: 2}\n",
            "(NSSDL) Val   loss 1.8198610621991675, accuracy 0.6197021764032073, precision 0.3539953567337588, recall 0.5189879384608133, f1 0.4188006219298684\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00016431324578661588\n",
            "PL-Accuracy: 0.92 Precision: 0.6376811594202899 PL-Recall: 0.6666666666666666 PL-F1: 0.6515151515151515 PL-Loss: 0.7124206287569125\n",
            "(NSSDL Strong) Train loss 0.35629247100134953, accuracy 0.96, precision 0.818840579710145, recall 0.8333333333333333, f1 0.8257575757575757\n",
            "{0: 419, 1: 454}\n",
            "(NSSDL Strong) Val   loss 1.9017229584964273, accuracy 0.5246277205040092, precision 0.34258723833755633, recall 0.5429872785231539, f1 0.3732844892916835\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00033886684224853524\n",
            "PL-Accuracy: 0.76 Precision: 0.47979797979797983 PL-Recall: 0.4444444444444444 PL-F1: 0.43874643874643876 PL-Loss: 1.2940730114423786\n",
            "(NSSDL Strong) Train loss 0.6472059391423136, accuracy 0.88, precision 0.73989898989899, recall 0.7222222222222222, f1 0.7193732193732194\n",
            "{1: 750, 0: 123}\n",
            "(NSSDL Strong) Val   loss 1.9988933667015885, accuracy 0.6403207331042382, precision 0.36852032520325206, recall 0.4870634541844172, f1 0.41812496037245367\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.009141480157486512\n",
            "PL-Accuracy: 0.96 Precision: 0.3333333333333333 PL-Recall: 0.3333333333333333 PL-F1: 0.3333333333333333 PL-Loss: 0.1548238417230355\n",
            "(NSSDL Strong) Train loss 0.081982660940261, accuracy 0.97875, precision 0.41666666666666663, recall 0.41604166666666664, f1 0.4163537755527743\n",
            "{1: 751, 0: 122}\n",
            "(NSSDL Strong) Val   loss 2.9095126130509033, accuracy 0.6494845360824743, precision 0.3912561029738127, recall 0.5115282859886986, f1 0.44166297394643855\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.707493400928797e-05\n",
            "PL-Accuracy: 0.84 Precision: 0.3043478260869565 PL-Recall: 0.3333333333333333 PL-F1: 0.3181818181818182 PL-Loss: 0.5890910709084503\n",
            "(NSSDL Strong) Train loss 0.29456907292122975, accuracy 0.9199999999999999, precision 0.6521739130434783, recall 0.6666666666666666, f1 0.6590909090909091\n",
            "{1: 700, 2: 11, 0: 162}\n",
            "(NSSDL Strong) Val   loss 2.018603896310674, accuracy 0.6666666666666666, precision 0.7019250708139597, recall 0.5742460530905898, f1 0.4919166453508259\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0001136159250199853\n",
            "PL-Accuracy: 0.96 Precision: 0.6521739130434783 PL-Recall: 0.6666666666666666 PL-F1: 0.6592592592592593 PL-Loss: 0.36220023067393675\n",
            "(NSSDL Strong) Train loss 0.18115692329947836, accuracy 0.98, precision 0.8260869565217391, recall 0.8333333333333333, f1 0.8296296296296297\n",
            "{1: 774, 0: 79, 2: 20}\n",
            "(NSSDL Strong) Val   loss 2.216202253686954, accuracy 0.6586483390607102, precision 0.6806087070290779, recall 0.5008663485545887, f1 0.4779951832785188\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00018376367501332424\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0005395886333384883\n",
            "(NSSDL Strong) Train loss 0.0003616761541759063, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 757, 0: 116}\n",
            "(NSSDL Strong) Val   loss 2.5661997453812235, accuracy 0.6552119129438717, precision 0.4051648218163046, recall 0.5195712762133721, f1 0.45260188087774295\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.5591825566807527e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6515151515151515 PL-Recall: 0.6666666666666666 PL-F1: 0.6589147286821705 PL-Loss: 0.15499741377945092\n",
            "(NSSDL Strong) Train loss 0.07752650280250886, accuracy 0.98, precision 0.8257575757575757, recall 0.8333333333333333, f1 0.8294573643410852\n",
            "{1: 731, 0: 141, 2: 1}\n",
            "(NSSDL Strong) Val   loss 2.3594619567002364, accuracy 0.6632302405498282, precision 0.7400044629430198, recall 0.5537755035902749, f1 0.47097669376693757\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00021676898868463467\n",
            "PL-Accuracy: 0.88 Precision: 0.7619047619047619 PL-Recall: 0.75 PL-F1: 0.6252032520325203 PL-Loss: 0.30996477997645605\n",
            "(NSSDL Strong) Train loss 0.15509077448257033, accuracy 0.94, precision 0.8809523809523809, recall 0.875, f1 0.8126016260162601\n",
            "{1: 842, 2: 7, 0: 24}\n",
            "(NSSDL Strong) Val   loss 3.2544595093341537, accuracy 0.6208476517754868, precision 0.727288579723259, recall 0.39348436622355365, f1 0.35531088313485165\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.272564012760996e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.8333333333333334 PL-Recall: 0.7777777777777777 PL-F1: 0.7222222222222222 PL-Loss: 0.1055972224125331\n",
            "(NSSDL Strong) Train loss 0.05282997402633035, accuracy 0.96, precision 0.9166666666666667, recall 0.8888888888888888, f1 0.8611111111111112\n",
            "{1: 733, 2: 31, 0: 109}\n",
            "(NSSDL Strong) Val   loss 2.571954511941379, accuracy 0.6529209621993127, precision 0.5080912642769502, recall 0.5175037761830086, f1 0.47199853190618984\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.2133244660362836e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.5833333333333334 PL-Recall: 0.6666666666666666 PL-F1: 0.6190476190476191 PL-Loss: 0.09091016272083964\n",
            "(NSSDL Strong) Train loss 0.04547614798275, accuracy 0.98, precision 0.7916666666666667, recall 0.8333333333333333, f1 0.8095238095238095\n",
            "{1: 758, 0: 115}\n",
            "(NSSDL Strong) Val   loss 3.39621902775219, accuracy 0.6483390607101948, precision 0.39888723184581854, recall 0.5108860252308308, f1 0.4451863556699109\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6046752569079399, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "(MixText) Val   loss 2.1746398307508943, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.289800372980535, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 2.0601476760621673, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.013153546936810017, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.9726175814963032, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6535423026233912, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "(MixText) Val   loss 1.670375987580142, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4171062324941158, accuracy 0.92, precision 0.3066666666666667, recall 0.3333333333333333, f1 0.3194444444444445\n",
            "(MixText) Val   loss 2.4663633261929947, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.2782114354707301, accuracy 0.92, precision 0.6376811594202899, recall 0.6666666666666666, f1 0.6515151515151515\n",
            "(MixText) Val   loss 2.076637676512697, accuracy 0.6048109965635738, precision 0.3690894880812818, recall 0.36327211233081735, f1 0.30338026683541636\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04854983350262046, accuracy 0.96, precision 0.48, recall 0.5, f1 0.4897959183673469\n",
            "(MixText) Val   loss 2.0941445684457305, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "tensor([2, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0685226096212864, accuracy 0.96, precision 0.9833333333333334, recall 0.8333333333333334, f1 0.8803418803418803\n",
            "(MixText) Val   loss 1.5046212024227177, accuracy 0.6231386025200458, precision 0.35498566236537626, recall 0.4532711106397271, f1 0.3939020597012024\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.06178616154938936, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.9655369672527427, accuracy 0.6265750286368843, precision 0.36205381028816025, recall 0.4455345200426603, f1 0.39122615254618326\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.028800441534258425, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4736778025791681, accuracy 0.6277205040091638, precision 0.5089746095963373, recall 0.44826693612517743, f1 0.41960799014551986\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04672899113968015, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7286876663885677, accuracy 0.6162657502863689, precision 0.465475521478847, recall 0.36447265052633276, f1 0.3138935456029456\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.026098060458898543, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2674280878792241, accuracy 0.6380297823596792, precision 0.487784113407777, recall 0.4054069433185941, f1 0.3783361101743994\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 0], device='cuda:0')\n",
            "tensor([1, 1, 2, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.019564147349447013, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5463299198120934, accuracy 0.6311569301260023, precision 0.5194145456791371, recall 0.39854864220351843, f1 0.37655272970077275\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03349151886999607, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.48261034739591, accuracy 0.6471935853379153, precision 0.5922982742924732, recall 0.4216449691718444, f1 0.40647940301767277\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03345878191292286, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6042723250655375, accuracy 0.6219931271477663, precision 0.49068491032776745, recall 0.39641308008947157, f1 0.3723019550130209\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.017000426184386016, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7317330098400514, accuracy 0.6174112256586484, precision 0.34294451725644387, recall 0.44522812041505366, f1 0.3843526218112945\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.015320677310228348, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.574958671774088, accuracy 0.6300114547537228, precision 0.5647269925143109, recall 0.45947582334979215, f1 0.4170348371800277\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 0], device='cuda:0')\n",
            "tensor([1, 0, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.05763305509462953, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4322010592257717, accuracy 0.6380297823596792, precision 0.3802153207927763, recall 0.4245396038955081, f1 0.39781428871663643\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.016355474144220353, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3030989830641402, accuracy 0.6437571592210768, precision 0.5396720674503794, recall 0.44022050672101054, f1 0.44064423549134646\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.01446779167279601, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4194063591127544, accuracy 0.6437571592210768, precision 0.5457882869861826, recall 0.4325156864463981, f1 0.42841428044085855\n",
            "\n",
            "{0: 126, 1: 359}\n",
            "{1: 364, 0: 120, 2: 1}\n",
            "{1: 410, 0: 75}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Fold 6/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 1.0991302217755998, accuracy 0.52, precision 0.2908496732026144, recall 0.293859649122807, f1 0.2888888888888889\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.0005646313298238, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.3757025950721332, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.4092032584493563, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.5480480765524719, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.9479059475308207, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 1.0936554432181376, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.9014525966374332, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.5922629571120653, accuracy 0.76, precision 0.25333333333333335, recall 0.3333333333333333, f1 0.2878787878787879\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3429456554506332, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.4044563514845712, accuracy 0.84, precision 0.27999999999999997, recall 0.3333333333333333, f1 0.30434782608695654\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.5436099612230614, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.2774022465039577, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3192914831196916, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.3170501076882439, accuracy 0.8, precision 0.5972222222222222, recall 0.6666666666666666, f1 0.627906976744186\n",
            "{2: 114, 1: 759}\n",
            "(traditional) Val   loss 0.8347276348841789, accuracy 0.6231386025200458, precision 0.37060536717287285, recall 0.38745887565045284, f1 0.3545509101239024\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.20071280095726252, accuracy 0.96, precision 0.9333333333333332, recall 0.8333333333333334, f1 0.8518518518518517\n",
            "{2: 298, 1: 570, 0: 5}\n",
            "(traditional) Val   loss 0.7972592419889419, accuracy 0.6575028636884307, precision 0.5398956002982849, recall 0.47045762852700784, f1 0.4449654848555163\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.10745028003917209, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{0: 36, 1: 583, 2: 254}\n",
            "(traditional) Val   loss 0.8444753769619314, accuracy 0.6632302405498282, precision 0.5502285520484896, recall 0.4943006422487328, f1 0.49788869935879015\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.48333333333333334 Initial loss: 0.5826484390667507\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0021935788861446783\n",
            "PL-Accuracy: 0.92 Precision: 0.3066666666666667 PL-Recall: 0.3333333333333333 PL-F1: 0.3194444444444445 PL-Loss: 0.7299534024121905\n",
            "(NSSDL) Train loss 0.3660734906491676, accuracy 0.96, precision 0.6533333333333333, recall 0.6666666666666666, f1 0.6597222222222222\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.773387579124694, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003982433318014955\n",
            "PL-Accuracy: 0.68 Precision: 0.22666666666666668 PL-Recall: 0.3333333333333333 PL-F1: 0.2698412698412698 PL-Loss: 2.5562363168657094\n",
            "(NSSDL) Train loss 1.2783172800987554, accuracy 0.8400000000000001, precision 0.6133333333333333, recall 0.6666666666666666, f1 0.6349206349206349\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.7517768330069377, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0009172927824693033\n",
            "PL-Accuracy: 0.8 Precision: 0.26666666666666666 PL-Recall: 0.3333333333333333 PL-F1: 0.29629629629629634 PL-Loss: 1.5646228014832428\n",
            "(NSSDL) Train loss 0.782770047132856, accuracy 0.9, precision 0.6333333333333333, recall 0.6666666666666666, f1 0.6481481481481481\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.3164210015306934, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005750159538001754\n",
            "PL-Accuracy: 0.88 Precision: 0.44 PL-Recall: 0.5 PL-F1: 0.46808510638297873 PL-Loss: 0.7895411600059431\n",
            "(NSSDL) Train loss 0.39505808797987163, accuracy 0.94, precision 0.72, recall 0.75, f1 0.7340425531914894\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.612292132858792, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00041525403641571755\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 1.2292820217511948\n",
            "(NSSDL) Train loss 0.6148486378938053, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.310355479028193, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000703749671120022\n",
            "PL-Accuracy: 0.76 Precision: 0.25333333333333335 PL-Recall: 0.3333333333333333 PL-F1: 0.2878787878787879 PL-Loss: 1.4284608897453706\n",
            "(NSSDL) Train loss 0.7145823197082453, accuracy 0.88, precision 0.6266666666666667, recall 0.6666666666666666, f1 0.6439393939393939\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.966185253903963, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0010852504694776144\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 0.9948051149247996\n",
            "(NSSDL) Train loss 0.4979451826971386, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.2138140769469388, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006205335503182141\n",
            "PL-Accuracy: 0.88 Precision: 0.625 PL-Recall: 0.6666666666666666 PL-F1: 0.6444444444444445 PL-Loss: 0.6824476256234837\n",
            "(NSSDL) Train loss 0.34153407958690096, accuracy 0.94, precision 0.8125, recall 0.8333333333333333, f1 0.8222222222222222\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.732129475247215, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00028775510494597254\n",
            "PL-Accuracy: 0.96 Precision: 0.5555555555555555 PL-Recall: 0.6666666666666666 PL-F1: 0.6 PL-Loss: 0.07089455187087879\n",
            "(NSSDL) Train loss 0.03559115348791238, accuracy 0.98, precision 0.7777777777777777, recall 0.8333333333333333, f1 0.8\n",
            "{1: 858, 2: 15}\n",
            "(NSSDL) Val   loss 3.7126611704677908, accuracy 0.5876288659793815, precision 0.2866355866355866, recall 0.3323529026254047, f1 0.25668584622832985\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011791537159297149\n",
            "PL-Accuracy: 0.92 Precision: 0.9523809523809523 PL-Recall: 0.8333333333333333 PL-F1: 0.875 PL-Loss: 0.6786430537597751\n",
            "(NSSDL) Train loss 0.33938048456568404, accuracy 0.96, precision 0.9761904761904762, recall 0.9166666666666666, f1 0.9375\n",
            "{1: 870, 2: 3}\n",
            "(NSSDL) Val   loss 4.061571184938192, accuracy 0.5933562428407789, precision 0.30919540229885056, recall 0.33340935603528504, f1 0.25082832083110773\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002885253611384542\n",
            "PL-Accuracy: 0.92 Precision: 0.9565217391304348 PL-Recall: 0.75 PL-F1: 0.8106060606060606 PL-Loss: 0.13737611640161568\n",
            "(NSSDL Strong) Train loss 0.06883232088137707, accuracy 0.96, precision 0.9782608695652174, recall 0.875, f1 0.9053030303030303\n",
            "{2: 461, 1: 409, 0: 3}\n",
            "(NSSDL Strong) Val   loss 0.9825356092695232, accuracy 0.5670103092783505, precision 0.49744746576339427, recall 0.44747109075020647, f1 0.4008210762081525\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0008727447997625859\n",
            "PL-Accuracy: 0.84 Precision: 0.27999999999999997 PL-Recall: 0.3333333333333333 PL-F1: 0.30434782608695654 PL-Loss: 1.077210343950615\n",
            "(NSSDL Strong) Train loss 0.5390415443751888, accuracy 0.9199999999999999, precision 0.64, recall 0.6666666666666666, f1 0.6521739130434783\n",
            "{1: 852, 2: 20, 0: 1}\n",
            "(NSSDL Strong) Val   loss 2.0375468139151285, accuracy 0.5945017182130584, precision 0.6336463223787168, recall 0.34005887730991713, f1 0.2701216366630677\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003980419577237626\n",
            "PL-Accuracy: 0.92 Precision: 0.8888888888888888 PL-Recall: 0.7777777777777777 PL-F1: 0.7666666666666666 PL-Loss: 0.16101232331545492\n",
            "(NSSDL Strong) Train loss 0.08070518263658934, accuracy 0.96, precision 0.9444444444444444, recall 0.8888888888888888, f1 0.8833333333333333\n",
            "{2: 251, 1: 618, 0: 4}\n",
            "(NSSDL Strong) Val   loss 1.533302317208616, accuracy 0.6391752577319587, precision 0.554098600205435, recall 0.4429426533654596, f1 0.42191350957461343\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00013272420546854845\n",
            "PL-Accuracy: 0.96 Precision: 0.6515151515151515 PL-Recall: 0.6666666666666666 PL-F1: 0.6589147286821705 PL-Loss: 0.08845541411834087\n",
            "(NSSDL Strong) Train loss 0.04429406916190471, accuracy 0.98, precision 0.8257575757575757, recall 0.8333333333333333, f1 0.8294573643410852\n",
            "{1: 742, 2: 128, 0: 3}\n",
            "(NSSDL Strong) Val   loss 2.9517793071408542, accuracy 0.6059564719358533, precision 0.5796458520515124, recall 0.38481150109723056, f1 0.3610028506452044\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 9.385032185491582e-05\n",
            "PL-Accuracy: 0.88 Precision: 0.9523809523809524 PL-Recall: 0.7000000000000001 PL-F1: 0.7799145299145298 PL-Loss: 0.4477576269340976\n",
            "(NSSDL Strong) Train loss 0.22392573862797627, accuracy 0.94, precision 0.9761904761904763, recall 0.8500000000000001, f1 0.8899572649572649\n",
            "{2: 311, 1: 561, 0: 1}\n",
            "(NSSDL Strong) Val   loss 2.052548801870374, accuracy 0.6288659793814433, precision 0.3897285699820219, recall 0.44813285622534177, f1 0.4162625455191402\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.005636974070312135\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.01436287311194714\n",
            "(NSSDL Strong) Train loss 0.009999923591129637, accuracy 0.99875, precision 0.75, recall 0.749375, f1 0.7496871088861077\n",
            "{0: 85, 1: 670, 2: 118}\n",
            "(NSSDL Strong) Val   loss 2.2143540488593305, accuracy 0.6300114547537228, precision 0.5370996463346279, recall 0.4957156842337791, f1 0.497755327992357\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.0486593611130957e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6666666666666666 PL-Recall: 0.5833333333333334 PL-F1: 0.6190476190476191 PL-Loss: 0.16308182204154167\n",
            "(NSSDL Strong) Train loss 0.0815711543175764, accuracy 0.98, precision 0.8333333333333333, recall 0.7916666666666667, f1 0.8095238095238095\n",
            "{0: 48, 1: 727, 2: 98}\n",
            "(NSSDL Strong) Val   loss 2.9822907862249917, accuracy 0.6403207331042382, precision 0.6251696001646876, recall 0.4790351586357617, f1 0.497490126905165\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.89182541291666e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.7777777777777778 PL-Recall: 0.7777777777777777 PL-F1: 0.6666666666666666 PL-Loss: 0.23040138173421706\n",
            "(NSSDL Strong) Train loss 0.11521514999417312, accuracy 0.96, precision 0.8888888888888888, recall 0.8888888888888888, f1 0.8333333333333333\n",
            "{2: 114, 1: 733, 0: 26}\n",
            "(NSSDL Strong) Val   loss 3.064831133197892, accuracy 0.6219931271477663, precision 0.6028718734254743, recall 0.4298445021889896, f1 0.4377493084554174\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.8186445742903743e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.001094567901548414\n",
            "(NSSDL Strong) Train loss 0.0005613771736456588, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{0: 97, 1: 774, 2: 2}\n",
            "(NSSDL Strong) Val   loss 3.6272901239506474, accuracy 0.6162657502863689, precision 0.521169095252049, recall 0.44530414311700534, f1 0.3911211539097396\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.3936155712362962e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.8888888888888888 PL-Recall: 0.8333333333333334 PL-F1: 0.8222222222222223 PL-Loss: 0.04096418281993205\n",
            "(NSSDL Strong) Train loss 0.020494059487822205, accuracy 0.98, precision 0.9444444444444444, recall 0.9166666666666667, f1 0.9111111111111112\n",
            "{0: 35, 1: 804, 2: 34}\n",
            "(NSSDL Strong) Val   loss 3.7664607480671988, accuracy 0.6139747995418099, precision 0.5811230960045709, recall 0.4164013748540904, f1 0.4018746088638561\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.40842736914753913, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 1.903609400028352, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.5777263408899307, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "(MixText) Val   loss 1.1126744178425678, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.7461889564990998, accuracy 0.84, precision 0.27999999999999997, recall 0.3333333333333333, f1 0.30434782608695654\n",
            "(MixText) Val   loss 1.8665547792768833, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6897572058439255, accuracy 0.76, precision 0.25333333333333335, recall 0.3333333333333333, f1 0.2878787878787879\n",
            "(MixText) Val   loss 1.1550696941790302, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 1.0776408016681671, accuracy 0.72, precision 0.24, recall 0.3333333333333333, f1 0.27906976744186046\n",
            "(MixText) Val   loss 1.7694708358406817, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.43896386712789537, accuracy 0.84, precision 0.27999999999999997, recall 0.3333333333333333, f1 0.30434782608695654\n",
            "(MixText) Val   loss 1.282510726397936, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4034668779559433, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 2.219148551040294, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 0, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.24854879327118395, accuracy 0.88, precision 0.7611111111111111, recall 0.7999999999999999, f1 0.6819291819291818\n",
            "(MixText) Val   loss 1.2356714644434117, accuracy 0.5990836197021764, precision 0.35204198911960766, recall 0.34595834480227544, f1 0.28314290075143667\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.9959831347316503, accuracy 0.8, precision 0.5972222222222222, recall 0.6666666666666666, f1 0.627906976744186\n",
            "(MixText) Val   loss 2.52028232160849, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.051444447338581084, accuracy 0.96, precision 0.6527777777777778, recall 0.6666666666666666, f1 0.6595744680851063\n",
            "(MixText) Val   loss 1.634330672804023, accuracy 0.5853379152348225, precision 0.30045492142266333, recall 0.3910071709297461, f1 0.33471799835872734\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03617360785603523, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.002851600662719, accuracy 0.5922107674684994, precision 0.3305923870216192, recall 0.3385133629559724, f1 0.2704955078089406\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03695173996500671, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.7094657640106217, accuracy 0.5956471935853379, precision 0.4548783068783069, recall 0.4097813150821128, f1 0.36436461790170743\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.003271307093091309, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.3003434862649197, accuracy 0.588774341351661, precision 0.3013638186923386, recall 0.3591121481560046, f1 0.3015925097356865\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0090723492577672, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.712596217428525, accuracy 0.5830469644902635, precision 0.40219092331768386, recall 0.3754238878914065, f1 0.33241565452091765\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 0, 1], device='cuda:0')\n",
            "tensor([2, 1, 0, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.017899936251342296, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6374080474195065, accuracy 0.5853379152348225, precision 0.34698579336466245, recall 0.3748145518658932, f1 0.3229712947087757\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.007269921191036702, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3803451822522306, accuracy 0.5693012600229095, precision 0.40645387438657266, recall 0.4115945322818102, f1 0.3842710625212418\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.006257458142936229, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.8080363648699305, accuracy 0.5899198167239404, precision 0.42319121447028424, recall 0.38554570579358644, f1 0.345348647233409\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.014748685802333057, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.0399325637409267, accuracy 0.5910652920962199, precision 0.42883447016288345, recall 0.36908076624438113, f1 0.31605869283918064\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.007848149118945002, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.691842037700074, accuracy 0.5945017182130584, precision 0.44124541346689833, recall 0.4129914644615513, f1 0.3722757588953642\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0063072902336716655, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4789342519113493, accuracy 0.5933562428407789, precision 0.45857296041502, recall 0.4283477376031107, f1 0.41887416408615724\n",
            "\n",
            "{1: 327, 2: 133, 0: 25}\n",
            "{1: 481, 2: 4}\n",
            "{1: 452, 2: 13, 0: 20}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 0, 1], device='cuda:0')\n",
            "Fold 7/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.847742634160178, accuracy 0.56, precision 0.25925925925925924, recall 0.24561403508771928, f1 0.25225225225225223\n",
            "{1: 873}\n",
            "(traditional) Val   loss 0.9685179916418851, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.6852422143731799, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3873119259159588, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.5070155715303761, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.8325819537792032, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.7926375163452966, accuracy 0.76, precision 0.38, recall 0.5, f1 0.4318181818181818\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.7325427124710524, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.8355872333049774, accuracy 0.68, precision 0.22666666666666668, recall 0.3333333333333333, f1 0.2698412698412698\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3391456796051977, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.1648565171552556, accuracy 0.92, precision 0.9583333333333333, recall 0.6666666666666666, f1 0.7282608695652174\n",
            "{1: 868, 2: 5}\n",
            "(traditional) Val   loss 1.1949904304695185, accuracy 0.5956471935853379, precision 0.3985407066052227, recall 0.33613044447065915, f1 0.25649795722182167\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.21972261641972832, accuracy 0.92, precision 0.9649122807017544, recall 0.9047619047619048, f1 0.9259259259259259\n",
            "{1: 854, 2: 19}\n",
            "(traditional) Val   loss 1.2683121045906793, accuracy 0.6002290950744559, precision 0.4103701877644932, recall 0.3451640386405044, f1 0.2789070604074246\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.09012415512864079, accuracy 0.96, precision 0.9761904761904762, recall 0.9, f1 0.9322493224932249\n",
            "{1: 842, 2: 31}\n",
            "(traditional) Val   loss 1.3983998558394712, accuracy 0.6059564719358533, precision 0.4062013127985084, recall 0.3534033266485785, f1 0.2957100313423988\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.18677899640585696, accuracy 0.96, precision 0.6521739130434783, recall 0.6666666666666666, f1 0.6592592592592593\n",
            "{1: 840, 2: 33}\n",
            "(traditional) Val   loss 1.5461545502852019, accuracy 0.6139747995418099, precision 0.43589466089466095, recall 0.36077228579292986, f1 0.30681150443883304\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.04187031672336161, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 640, 2: 233}\n",
            "(traditional) Val   loss 1.1834225973378034, accuracy 0.6311569301260023, precision 0.39251385908440634, recall 0.43720655892414767, f1 0.41200091937092886\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial acc: 0.2631578947368421 Initial loss: 0.8445894845894405\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0059923000174603655\n",
            "PL-Accuracy: 0.84 Precision: 0.42 PL-Recall: 0.5 PL-F1: 0.4565217391304348 PL-Loss: 1.1050649463764526\n",
            "(NSSDL) Train loss 0.5555286231969565, accuracy 0.9199999999999999, precision 0.71, recall 0.75, f1 0.7282608695652174\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.5089098556035254, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0008146288587886374\n",
            "PL-Accuracy: 0.64 Precision: 0.21333333333333335 PL-Recall: 0.3333333333333333 PL-F1: 0.26016260162601623 PL-Loss: 2.1365541240998676\n",
            "(NSSDL) Train loss 1.0686843764793281, accuracy 0.8200000000000001, precision 0.6066666666666667, recall 0.6666666666666666, f1 0.6300813008130082\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.116171740463267, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0019212017488462153\n",
            "PL-Accuracy: 0.92 Precision: 0.46 PL-Recall: 0.5 PL-F1: 0.4791666666666667 PL-Loss: 0.5064594757235942\n",
            "(NSSDL) Train loss 0.2541903387362202, accuracy 0.96, precision 0.73, recall 0.75, f1 0.7395833333333334\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.3531341013832434, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003765770033351146\n",
            "PL-Accuracy: 0.76 Precision: 0.5797101449275363 PL-Recall: 0.42857142857142855 PL-F1: 0.4314814814814815 PL-Loss: 1.1900430484175948\n",
            "(NSSDL) Train loss 0.5952098127104649, accuracy 0.88, precision 0.7898550724637681, recall 0.7142857142857143, f1 0.7157407407407408\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.7020639908387323, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0005130105583521072\n",
            "PL-Accuracy: 0.84 Precision: 0.6060606060606061 PL-Recall: 0.5333333333333333 PL-F1: 0.5499999999999999 PL-Loss: 0.9521438861494451\n",
            "(NSSDL) Train loss 0.4763284483538986, accuracy 0.9199999999999999, precision 0.803030303030303, recall 0.7666666666666666, f1 0.7749999999999999\n",
            "{1: 867, 2: 6}\n",
            "(NSSDL) Val   loss 3.425426338370315, accuracy 0.5899198167239404, precision 0.25317185697808536, recall 0.33148257376168194, f1 0.24989028973092717\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00043443433016363995\n",
            "PL-Accuracy: 0.72 Precision: 0.5652173913043478 PL-Recall: 0.4666666666666666 PL-F1: 0.46398046398046394 PL-Loss: 1.4059546538296022\n",
            "(NSSDL) Train loss 0.7031945440798829, accuracy 0.86, precision 0.7826086956521738, recall 0.7333333333333333, f1 0.7319902319902319\n",
            "{1: 785, 2: 88}\n",
            "(NSSDL) Val   loss 2.609955125779645, accuracy 0.5555555555555556, precision 0.27657305539471144, recall 0.32658042022203876, f1 0.2792608559479725\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003070515405124752\n",
            "PL-Accuracy: 0.88 Precision: 0.5760233918128655 PL-Recall: 0.6111111111111112 PL-F1: 0.5925925925925926 PL-Loss: 0.6691871854917346\n",
            "(NSSDL) Train loss 0.33474711851612354, accuracy 0.94, precision 0.7880116959064327, recall 0.8055555555555556, f1 0.7962962962962963\n",
            "{1: 812, 2: 61}\n",
            "(NSSDL) Val   loss 2.6556286218596865, accuracy 0.5853379152348225, precision 0.3209574954911303, recall 0.34399748338641817, f1 0.29285890831831524\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00027871040801983325\n",
            "PL-Accuracy: 0.96 Precision: 0.9166666666666666 PL-Recall: 0.9166666666666666 PL-F1: 0.9047619047619048 PL-Loss: 0.14108813168838555\n",
            "(NSSDL) Train loss 0.07068342104820269, accuracy 0.98, precision 0.9583333333333333, recall 0.9583333333333333, f1 0.9523809523809523\n",
            "{1: 850, 2: 16, 0: 7}\n",
            "(NSSDL) Val   loss 3.4435710398648807, accuracy 0.5899198167239404, precision 0.46628851540616245, recall 0.3423215207137813, f1 0.27897562054132413\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.0028632609895430506\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0015013885983665074\n",
            "(NSSDL) Train loss 0.002182324793954779, accuracy 0.99875, precision 0.75, recall 0.749375, f1 0.7496871088861077\n",
            "{1: 858, 2: 9, 0: 6}\n",
            "(NSSDL) Val   loss 4.014065664575891, accuracy 0.5933562428407789, precision 0.5133385133385133, recall 0.34281173606774556, f1 0.27528771653377565\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.571310140818241e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0007496832269160743\n",
            "(NSSDL) Train loss 0.0003976981641621283, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 846, 2: 26, 0: 1}\n",
            "(NSSDL) Val   loss 4.101508730862456, accuracy 0.5830469644902635, precision 0.275110626174456, recall 0.3312204265135727, f1 0.26042559326692166\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.9987277353689568 FT-Recall: 0.9375 FT-F1: 0.9660297239915074 FT-Loss: 0.018478012559062337\n",
            "PL-Accuracy: 0.96 Precision: 0.3333333333333333 PL-Recall: 0.3333333333333333 PL-F1: 0.3333333333333333 PL-Loss: 0.22230504538726695\n",
            "(NSSDL Strong) Train loss 0.12039152897316464, accuracy 0.97875, precision 0.666030534351145, recall 0.6354166666666666, f1 0.6496815286624203\n",
            "{1: 817, 2: 56}\n",
            "(NSSDL Strong) Val   loss 3.1679955512996814, accuracy 0.6334478808705613, precision 0.4473975636766334, recall 0.38390153749361017, f1 0.3445816009177658\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 9.206536966303246e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.9782608695652174 PL-Recall: 0.8333333333333333 PL-F1: 0.8888888888888888 PL-Loss: 0.29597472268090186\n",
            "(NSSDL Strong) Train loss 0.14803339402528246, accuracy 0.98, precision 0.9891304347826086, recall 0.9166666666666666, f1 0.9444444444444444\n",
            "{1: 587, 2: 286}\n",
            "(NSSDL Strong) Val   loss 1.8046000205346098, accuracy 0.6288659793814433, precision 0.3930339961004356, recall 0.4502877066047999, f1 0.41969216588397024\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00021055419634649297\n",
            "PL-Accuracy: 0.88 Precision: 0.9375 PL-Recall: 0.625 PL-F1: 0.6666666666666667 PL-Loss: 0.6313908892529331\n",
            "(NSSDL Strong) Train loss 0.3158007217246398, accuracy 0.94, precision 0.96875, recall 0.8125, f1 0.8333333333333334\n",
            "{1: 791, 2: 82}\n",
            "(NSSDL Strong) Val   loss 2.4307638910179374, accuracy 0.6437571592210768, precision 0.4372308388064918, recall 0.4004561362117101, f1 0.3701457462785287\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0015983591201802483\n",
            "PL-Accuracy: 0.96 Precision: 0.6515151515151515 PL-Recall: 0.6666666666666666 PL-F1: 0.6589147286821705 PL-Loss: 0.30470815610793317\n",
            "(NSSDL Strong) Train loss 0.1531532576140567, accuracy 0.98, precision 0.8257575757575757, recall 0.8333333333333333, f1 0.8294573643410852\n",
            "{1: 791, 2: 82}\n",
            "(NSSDL Strong) Val   loss 3.084137790103366, accuracy 0.6517754868270332, precision 0.45111159076192536, recall 0.40710681189624215, f1 0.3782975776392683\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00010690590108424657\n",
            "PL-Accuracy: 0.88 Precision: 0.4710144927536232 PL-Recall: 0.4444444444444444 PL-F1: 0.4515151515151515 PL-Loss: 0.6989577532406527\n",
            "(NSSDL Strong) Train loss 0.34953232957086844, accuracy 0.94, precision 0.7355072463768116, recall 0.7222222222222222, f1 0.7257575757575757\n",
            "{1: 838, 2: 35}\n",
            "(NSSDL Strong) Val   loss 2.9016875303098555, accuracy 0.6242840778923253, precision 0.47231503579952266, recall 0.37014404991283606, f1 0.3206583149103414\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.0024064919822558294\n",
            "PL-Accuracy: 0.84 Precision: 0.2916666666666667 PL-Recall: 0.3333333333333333 PL-F1: 0.3111111111111111 PL-Loss: 0.5084043866325894\n",
            "(NSSDL Strong) Train loss 0.2554054393074226, accuracy 0.91875, precision 0.39583333333333337, recall 0.41604166666666664, f1 0.40524266444166324\n",
            "{1: 854, 2: 19}\n",
            "(NSSDL Strong) Val   loss 3.4184313732919556, accuracy 0.6071019473081328, precision 0.41271210813920045, recall 0.3490176031877105, f1 0.2818203888852105\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006209419031983998\n",
            "PL-Accuracy: 0.92 Precision: 0.9565217391304348 PL-Recall: 0.75 PL-F1: 0.8106060606060606 PL-Loss: 0.3578473854699301\n",
            "(NSSDL Strong) Train loss 0.17923416368656425, accuracy 0.96, precision 0.9782608695652174, recall 0.875, f1 0.9053030303030303\n",
            "{1: 866, 2: 7}\n",
            "(NSSDL Strong) Val   loss 4.193891456027967, accuracy 0.6002290950744559, precision 0.43786429121302106, recall 0.3401360544217687, f1 0.26304700781235074\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0012318194946055884\n",
            "PL-Accuracy: 0.96 Precision: 0.6521739130434783 PL-Recall: 0.6666666666666666 PL-F1: 0.6592592592592593 PL-Loss: 0.412844644814738\n",
            "(NSSDL Strong) Train loss 0.2070382321546718, accuracy 0.98, precision 0.8260869565217391, recall 0.8333333333333333, f1 0.8296296296296297\n",
            "{1: 857, 2: 16}\n",
            "(NSSDL Strong) Val   loss 3.597634459888846, accuracy 0.6059564719358533, precision 0.4102003111629717, recall 0.3469387755102041, f1 0.27699627253556686\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.5 FT-Recall: 0.49875 FT-F1: 0.4993742177722153 FT-Loss: 0.003002868532494176\n",
            "PL-Accuracy: 0.88 Precision: 0.6190476190476191 PL-Recall: 0.5555555555555555 PL-F1: 0.5743589743589744 PL-Loss: 0.7500214994964024\n",
            "(NSSDL Strong) Train loss 0.3765121840144483, accuracy 0.93875, precision 0.5595238095238095, recall 0.5271527777777778, f1 0.5368665960655948\n",
            "{1: 818, 2: 48, 0: 7}\n",
            "(NSSDL Strong) Val   loss 3.0838640973772073, accuracy 0.6288659793814433, precision 0.5142478751891955, recall 0.37898134629928043, f1 0.33754471308409345\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0013214445241465\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0010726443693524093\n",
            "(NSSDL Strong) Train loss 0.0011970444467494547, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 801, 0: 18, 2: 54}\n",
            "(NSSDL Strong) Val   loss 3.342051853048913, accuracy 0.6334478808705613, precision 0.5353030933555277, recall 0.39434788887890804, f1 0.36596570343574797\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4774989938735962, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "(MixText) Val   loss 1.5847146830394574, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.5252525454759598, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "(MixText) Val   loss 1.2353313021523982, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4032221126556397, accuracy 0.88, precision 0.29333333333333333, recall 0.3333333333333333, f1 0.3120567375886525\n",
            "(MixText) Val   loss 1.0604949700395403, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 1.0800911790132524, accuracy 0.64, precision 0.32, recall 0.5, f1 0.39024390243902435\n",
            "(MixText) Val   loss 1.7408952469759347, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.8129140250757336, accuracy 0.76, precision 0.38, recall 0.5, f1 0.4318181818181818\n",
            "(MixText) Val   loss 2.4389754458957853, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.12135857343673706, accuracy 0.96, precision 0.9782608695652174, recall 0.8333333333333333, f1 0.8888888888888888\n",
            "(MixText) Val   loss 1.1700620631615868, accuracy 0.5967926689576174, precision 0.3380379403794038, recall 0.3683771774605796, f1 0.3341598999554596\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.16739144030958414, accuracy 0.92, precision 0.9545454545454546, recall 0.8, f1 0.851190476190476\n",
            "(MixText) Val   loss 2.271155774699234, accuracy 0.5933562428407789, precision 0.3234585741811175, recall 0.3348459229549238, f1 0.25597835606631786\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.17143207173794509, accuracy 0.96, precision 0.5555555555555555, recall 0.6666666666666666, f1 0.6\n",
            "(MixText) Val   loss 1.3332213983195753, accuracy 0.5899198167239404, precision 0.3275813692480359, recall 0.35231279409644395, f1 0.3079807636812311\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.1372773576481268, accuracy 0.96, precision 0.9772727272727273, recall 0.875, f1 0.9169435215946844\n",
            "(MixText) Val   loss 2.413498116247484, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.03186439782381058, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2385046081564135, accuracy 0.5486827033218786, precision 0.34880354345594994, recall 0.4046111700942419, f1 0.37089641621059455\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.1062480467557907, accuracy 0.96, precision 0.5, recall 0.6666666666666666, f1 0.5555555555555555\n",
            "(MixText) Val   loss 1.1800535832620296, accuracy 0.6013745704467354, precision 0.3736178019857117, recall 0.430563747657059, f1 0.3994769032137842\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.030707144811749458, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2200876450654703, accuracy 0.579610538373425, precision 0.3174693815397333, recall 0.3443775968961766, f1 0.29873965787339896\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.11158791240304708, accuracy 0.96, precision 0.5833333333333334, recall 0.6666666666666666, f1 0.6190476190476191\n",
            "(MixText) Val   loss 1.1115622423836728, accuracy 0.6219931271477663, precision 0.3769958847736626, recall 0.42129422096391544, f1 0.39576169726582683\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.01747304916381836, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2276847576562482, accuracy 0.6242840778923253, precision 0.37387465556046356, recall 0.40964964020290195, f1 0.38394901639934753\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.026749406307935715, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.201878440373269, accuracy 0.6036655211912944, precision 0.38509188556576307, recall 0.4476505052888207, f1 0.4094797810723206\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.016677635740488767, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.126687467337201, accuracy 0.6059564719358533, precision 0.37896965419901196, recall 0.4381607749072654, f1 0.40505707125425433\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.06945513695478439, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3459492874036267, accuracy 0.4547537227949599, precision 0.3915607268548445, recall 0.41299988203373833, f1 0.32339273520542894\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.008761154096573591, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.280925276182498, accuracy 0.6093928980526919, precision 0.38246136237797423, recall 0.442960691020146, f1 0.4088746203790972\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.047173734456300735, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.1555217733566157, accuracy 0.5945017182130584, precision 0.7181716928645167, recall 0.45067453012210174, f1 0.4123946953468065\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([0, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.025130356401205062, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.2749772925625416, accuracy 0.5922107674684994, precision 0.3932819602943802, recall 0.4584667007458089, f1 0.4101291595088122\n",
            "\n",
            "{1: 348, 2: 137}\n",
            "{1: 465, 2: 19, 0: 1}\n",
            "{1: 437, 2: 37, 0: 11}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "Fold 8/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.7371991149016789, accuracy 0.68, precision 0.618421052631579, recall 0.5992647058823529, f1 0.6031746031746031\n",
            "{1: 873}\n",
            "(traditional) Val   loss 0.9152668698193276, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.4461984315088817, accuracy 0.8, precision 0.8958333333333333, recall 0.5833333333333334, f1 0.584717607973422\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.07879618354584, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.4887076759977, accuracy 0.72, precision 0.8541666666666667, recall 0.5625, f1 0.5257452574525745\n",
            "{1: 850, 2: 23}\n",
            "(traditional) Val   loss 1.0418888821041203, accuracy 0.6105383734249714, precision 0.4346291560102302, recall 0.3538175193005911, f1 0.2915662738171814\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.46801857224532534, accuracy 0.88, precision 0.6111111111111112, recall 0.5925925925925926, f1 0.5946969696969697\n",
            "{1: 659, 2: 214}\n",
            "(traditional) Val   loss 0.9681750046063776, accuracy 0.6426116838487973, precision 0.3921002746538463, recall 0.43429148152517266, f1 0.40879695901500884\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.30957577750086784, accuracy 0.92, precision 0.5925925925925926, recall 0.6666666666666666, f1 0.625\n",
            "{1: 695, 2: 178}\n",
            "(traditional) Val   loss 0.9470700883225763, accuracy 0.6586483390607102, precision 0.4053323633228249, recall 0.43610029753712665, f1 0.41126183493599106\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.1124240697494575, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 680, 2: 193}\n",
            "(traditional) Val   loss 0.9917979972207382, accuracy 0.6506300114547537, precision 0.397175657827898, recall 0.43375932261151084, f1 0.4085596444525688\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.046815935522317886, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 233, 1: 640}\n",
            "(traditional) Val   loss 1.0644829266079485, accuracy 0.6449026345933563, precision 0.3960345135908441, recall 0.4427588376391019, f1 0.4164449466488087\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.015963023112687682, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 753, 2: 120}\n",
            "(traditional) Val   loss 1.438013801104538, accuracy 0.6437571592210768, precision 0.40289951305887567, recall 0.4083572542697233, f1 0.3805060164843055\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.0055817139717484155, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 771, 2: 102}\n",
            "(traditional) Val   loss 1.7415018570683811, accuracy 0.6449026345933563, precision 0.4135449251036342, recall 0.40468981426867473, f1 0.37522209562957504\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.0043743665030758295, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 739, 2: 134}\n",
            "(traditional) Val   loss 1.7548386965587004, accuracy 0.6494845360824743, precision 0.40848867974067415, recall 0.41803310919743614, f1 0.3926574409268806\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.2333333333333333 Initial loss: 1.0611537950379508\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.017266900726317546\n",
            "PL-Accuracy: 0.6 Precision: 0.3 PL-Recall: 0.5 PL-F1: 0.37499999999999994 PL-Loss: 2.8174399255076423\n",
            "(NSSDL) Train loss 1.41735341311698, accuracy 0.8, precision 0.65, recall 0.75, f1 0.6875\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.7611609802408075, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0012285912452352932\n",
            "PL-Accuracy: 0.76 Precision: 0.38 PL-Recall: 0.5 PL-F1: 0.4318181818181818 PL-Loss: 1.1465628692115257\n",
            "(NSSDL) Train loss 0.5738957302283805, accuracy 0.88, precision 0.69, recall 0.75, f1 0.7159090909090909\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 2.3525111944553947, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000681746602349449\n",
            "PL-Accuracy: 0.64 Precision: 0.8043478260869565 PL-Recall: 0.5909090909090909 PL-F1: 0.5322245322245323 PL-Loss: 1.5060840707862684\n",
            "(NSSDL) Train loss 0.7533829086943089, accuracy 0.8200000000000001, precision 0.9021739130434783, recall 0.7954545454545454, f1 0.7661122661122661\n",
            "{1: 850, 2: 23}\n",
            "(NSSDL) Val   loss 2.3129743582112705, accuracy 0.6059564719358533, precision 0.4048593350383632, recall 0.3498119093494816, f1 0.2856172025191791\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0015572674888608162\n",
            "PL-Accuracy: 0.88 Precision: 0.9347826086956521 PL-Recall: 0.7 PL-F1: 0.7508305647840532 PL-Loss: 0.4266849105323282\n",
            "(NSSDL) Train loss 0.2141210890105945, accuracy 0.94, precision 0.967391304347826, recall 0.85, f1 0.8754152823920266\n",
            "{1: 870, 2: 3}\n",
            "(NSSDL) Val   loss 3.489633742335758, accuracy 0.5967926689576174, precision 0.42107279693486593, recall 0.33605442176870753, f1 0.25447641608026195\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00029878751342039323\n",
            "PL-Accuracy: 0.92 Precision: 0.9444444444444444 PL-Recall: 0.8888888888888888 PL-F1: 0.9080882352941178 PL-Loss: 0.2826056064472401\n",
            "(NSSDL) Train loss 0.14145219698033024, accuracy 0.96, precision 0.9722222222222222, recall 0.9444444444444444, f1 0.9540441176470589\n",
            "{2: 147, 1: 726}\n",
            "(NSSDL) Val   loss 2.369040344831944, accuracy 0.6563573883161512, precision 0.404143475572047, recall 0.4240415241241005, f1 0.3977133021883452\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0006956156996784557\n",
            "PL-Accuracy: 0.96 Precision: 0.9736842105263157 PL-Recall: 0.9285714285714286 PL-F1: 0.9480249480249481 PL-Loss: 0.2799288773931039\n",
            "(NSSDL) Train loss 0.14031224654639118, accuracy 0.98, precision 0.9868421052631579, recall 0.9642857142857143, f1 0.974012474012474\n",
            "{1: 751, 2: 122}\n",
            "(NSSDL) Val   loss 2.962545148509499, accuracy 0.6597938144329897, precision 0.41584626690823895, recall 0.4202220387191485, f1 0.392773927782188\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002828885180497309\n",
            "PL-Accuracy: 0.96 Precision: 0.975 PL-Recall: 0.9166666666666667 PL-F1: 0.9417249417249417 PL-Loss: 0.12220004969562329\n",
            "(NSSDL) Train loss 0.06124146910683651, accuracy 0.98, precision 0.9875, recall 0.9583333333333334, f1 0.9708624708624709\n",
            "{1: 742, 2: 131}\n",
            "(NSSDL) Val   loss 3.163922399599832, accuracy 0.6563573883161512, precision 0.4061781993511793, recall 0.4190135399053649, f1 0.3912843572308367\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011693489018398396\n",
            "PL-Accuracy: 0.96 Precision: 0.6481481481481481 PL-Recall: 0.6666666666666666 PL-F1: 0.6571428571428571 PL-Loss: 0.40260896880499786\n",
            "(NSSDL) Train loss 0.20136295184759093, accuracy 0.98, precision 0.8240740740740741, recall 0.8333333333333333, f1 0.8285714285714285\n",
            "{1: 858, 2: 15}\n",
            "(NSSDL) Val   loss 4.300270976474685, accuracy 0.6048109965635738, precision 0.42346542346542343, recall 0.3462965147523364, f1 0.2764277600878908\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0001394916570643545\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00025433473560302185\n",
            "(NSSDL) Train loss 0.0001969131963336882, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 823, 2: 50}\n",
            "(NSSDL) Val   loss 3.78771527702971, accuracy 0.6368843069873997, precision 0.46313487241798296, recall 0.3843917528475745, f1 0.3432030782961597\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.640470461505174e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00031520372379288474\n",
            "(NSSDL) Train loss 0.00017580421420396826, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 823, 2: 50}\n",
            "(NSSDL) Val   loss 3.9061851449492164, accuracy 0.6368843069873997, precision 0.46313487241798296, recall 0.3843917528475745, f1 0.3432030782961597\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00019797562143139658\n",
            "PL-Accuracy: 0.96 Precision: 0.975 PL-Recall: 0.9166666666666667 PL-F1: 0.9417249417249417 PL-Loss: 0.21885516527981963\n",
            "(NSSDL Strong) Train loss 0.10952657045062551, accuracy 0.98, precision 0.9875, recall 0.9583333333333334, f1 0.9708624708624709\n",
            "{2: 626, 1: 247}\n",
            "(NSSDL Strong) Val   loss 2.236559567469083, accuracy 0.4845360824742268, precision 0.3919170622550478, recall 0.42898037827847907, f1 0.3451691132607699\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 0.9925 Precision: 0.49625 FT-Recall: 0.5 FT-F1: 0.4981179422835633 FT-Loss: 0.06867947356233345\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.03860217489169112\n",
            "(NSSDL Strong) Train loss 0.05364082422701229, accuracy 0.9962500000000001, precision 0.748125, recall 0.75, f1 0.7490589711417817\n",
            "{2: 251, 1: 622}\n",
            "(NSSDL Strong) Val   loss 2.301318345525776, accuracy 0.6254295532646048, precision 0.3789450984913508, recall 0.4289672709160735, f1 0.40183695682904075\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002668449474367662\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0009798530274045853\n",
            "(NSSDL Strong) Train loss 0.0006233489874206757, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 110, 1: 763}\n",
            "(NSSDL Strong) Val   loss 3.0688549717775278, accuracy 0.6460481099656358, precision 0.4071885301243099, recall 0.4067686419461812, f1 0.3774823669003098\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 7.954646003781818e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00023709084624507732\n",
            "(NSSDL Strong) Train loss 0.00015831865314144776, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 224, 1: 649}\n",
            "(NSSDL Strong) Val   loss 2.930637131321618, accuracy 0.6380297823596792, precision 0.3893765133171913, recall 0.43387728887316007, f1 0.4080952867755158\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.810953577361943e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.6333333333333333 PL-Recall: 0.6666666666666666 PL-F1: 0.6491228070175439 PL-Loss: 0.242563522348064\n",
            "(NSSDL Strong) Train loss 0.1213058159419188, accuracy 0.98, precision 0.8166666666666667, recall 0.8333333333333333, f1 0.8245614035087719\n",
            "{2: 136, 1: 737}\n",
            "(NSSDL Strong) Val   loss 3.114410140420643, accuracy 0.6437571592210768, precision 0.40208715779391807, recall 0.4141035219482783, f1 0.3885071690879405\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.81342796208628e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.007098350160439233\n",
            "(NSSDL Strong) Train loss 0.0035682422200300477, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 47, 1: 826}\n",
            "(NSSDL Strong) Val   loss 3.774811863291539, accuracy 0.6162657502863689, precision 0.42445864028987, recall 0.3678030749872203, f1 0.32207737094939826\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.9274374921660636e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0015078278629516717\n",
            "(NSSDL Strong) Train loss 0.0007735511189366662, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 169, 1: 704}\n",
            "(NSSDL Strong) Val   loss 3.408437265945179, accuracy 0.6013745704467354, precision 0.3654938811188811, recall 0.3932130077464511, f1 0.36926698820118425\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.001747790463259662\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00015735604964512668\n",
            "(NSSDL Strong) Train loss 0.0009525732564523943, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 867, 2: 6}\n",
            "(NSSDL Strong) Val   loss 4.551783569671491, accuracy 0.5979381443298969, precision 0.4213763936947328, recall 0.3381332494462139, f1 0.25978241914496897\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.6815266155608698e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 8.226887673247672e-05\n",
            "(NSSDL Strong) Train loss 5.4542071444042706e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 861, 2: 12}\n",
            "(NSSDL Strong) Val   loss 4.476419490790552, accuracy 0.6036655211912944, precision 0.4505420054200542, recall 0.3449359705346493, f1 0.27358784939566533\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.3547857581434072e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00010384625498513092\n",
            "(NSSDL Strong) Train loss 6.369705628328249e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 861, 2: 12}\n",
            "(NSSDL Strong) Val   loss 4.512468202156201, accuracy 0.6036655211912944, precision 0.4505420054200542, recall 0.3449359705346493, f1 0.27358784939566533\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.4656028574705124, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "(MixText) Val   loss 1.0004205643994155, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.7470835298299789, accuracy 0.56, precision 0.28, recall 0.5, f1 0.358974358974359\n",
            "(MixText) Val   loss 1.2082123145853667, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.9203824258595705, accuracy 0.76, precision 0.25333333333333335, recall 0.3333333333333333, f1 0.2878787878787879\n",
            "(MixText) Val   loss 1.9088941101723043, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.7210570228099823, accuracy 0.8, precision 0.5151515151515151, recall 0.6078431372549019, f1 0.536369386464263\n",
            "(MixText) Val   loss 1.417307429775203, accuracy 0.3768613974799542, precision 0.29572507831214295, recall 0.32766570982921106, f1 0.2684653769200724\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.16044002994894982, accuracy 0.92, precision 0.9375, recall 0.9090909090909092, f1 0.9166666666666667\n",
            "(MixText) Val   loss 1.3728368631292045, accuracy 0.6002290950744559, precision 0.34838550415740954, recall 0.38107821163147343, f1 0.35238068245465576\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.21777723170816898, accuracy 0.96, precision 0.6363636363636364, recall 0.6666666666666666, f1 0.6507936507936508\n",
            "(MixText) Val   loss 1.451778659227924, accuracy 0.4753722794959908, precision 0.32209495561445584, recall 0.3678161823496258, f1 0.3292508463692487\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.024331736862659454, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5311586637677197, accuracy 0.4536082474226804, precision 0.31983914209115283, recall 0.3620777790885141, f1 0.31808469016161556\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.20668301798403263, accuracy 0.96, precision 0.6190476190476191, recall 0.6666666666666666, f1 0.641025641025641\n",
            "(MixText) Val   loss 1.5195469705714393, accuracy 0.46735395189003437, precision 0.3132762874691585, recall 0.3568558059061775, f1 0.32179528022153064\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.016236043199896813, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5733928022925388, accuracy 0.48911798396334477, precision 0.3220646865518957, recall 0.36905876030566365, f1 0.3346724685550638\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.14650611650198697, accuracy 0.96, precision 0.6333333333333333, recall 0.6666666666666666, f1 0.6491228070175439\n",
            "(MixText) Val   loss 1.3456456438389703, accuracy 0.6254295532646048, precision 0.3863576790182295, recall 0.3916165310054658, f1 0.361332425127991\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.26859323035925625, accuracy 0.92, precision 0.5333333333333333, recall 0.6666666666666666, f1 0.5833333333333334\n",
            "(MixText) Val   loss 1.2366231442829179, accuracy 0.5784650630011455, precision 0.34040607152332675, recall 0.3832409264283749, f1 0.3582252991151483\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.027344761416316032, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6368343125162528, accuracy 0.4570446735395189, precision 0.3111201201833639, recall 0.35394859292464576, f1 0.3167510033683967\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.014251210764050483, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.416880743712494, accuracy 0.6036655211912944, precision 0.35651614855154684, recall 0.39593409618182535, f1 0.37016784385205437\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.21178712852299214, accuracy 0.92, precision 0.5833333333333334, recall 0.6666666666666666, f1 0.6190476190476191\n",
            "(MixText) Val   loss 1.4215392597352516, accuracy 0.5601374570446735, precision 0.3321990849689831, recall 0.3779927385212274, f1 0.35323370818966576\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.12265353521332145, accuracy 0.96, precision 0.6333333333333333, recall 0.6666666666666666, f1 0.6491228070175439\n",
            "(MixText) Val   loss 1.5533181157300222, accuracy 0.5773195876288659, precision 0.3431562965901164, recall 0.38619008296960405, f1 0.36156420593387306\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.1514808825403452, accuracy 0.96, precision 0.5833333333333334, recall 0.6666666666666666, f1 0.6190476190476191\n",
            "(MixText) Val   loss 1.443612950082702, accuracy 0.6139747995418099, precision 0.3668787459658829, recall 0.393095041484802, f1 0.3659756619580505\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([0, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0778918593376875, accuracy 0.96, precision 0.6190476190476191, recall 0.6666666666666666, f1 0.641025641025641\n",
            "(MixText) Val   loss 1.3900931121659852, accuracy 0.5223367697594502, precision 0.3285236575250255, recall 0.37762835384635557, f1 0.3483888915638158\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 2, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.07192091092467308, accuracy 0.96, precision 0.6363636363636364, recall 0.6666666666666666, f1 0.6507936507936508\n",
            "(MixText) Val   loss 1.3839638024261318, accuracy 0.572737686139748, precision 0.33843621399176954, recall 0.38146618955867506, f1 0.35679532291685656\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.024336890056729316, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.405098491825189, accuracy 0.5189003436426117, precision 0.4357987672830252, recall 0.37622355724917106, f1 0.35524681853866213\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.04863269053399563, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.438829976566059, accuracy 0.47995418098510884, precision 0.4600188854532445, recall 0.36973163697608574, f1 0.3451585080573491\n",
            "\n",
            "{1: 394, 2: 91}\n",
            "{1: 445, 2: 40}\n",
            "{1: 467, 2: 18}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 2], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Fold 9/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Train loss 0.6870968235390527, accuracy 0.64, precision 0.3333333333333333, recall 0.2222222222222222, f1 0.26666666666666666\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.279733934835212, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.41029838366167887, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.8818778874497164, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.18839640629344753, accuracy 0.96, precision 0.48, recall 0.5, f1 0.4897959183673469\n",
            "{1: 873}\n",
            "(traditional) Val   loss 2.524845701471167, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 1.0328667779187006, accuracy 0.76, precision 0.38, recall 0.5, f1 0.4318181818181818\n",
            "{1: 873}\n",
            "(traditional) Val   loss 2.3674156766121177, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.2825055292674473, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "{1: 861, 2: 12}\n",
            "(traditional) Val   loss 1.662523249332202, accuracy 0.5990836197021764, precision 0.31204026325977546, recall 0.33877551020408164, f1 0.26110077257091296\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.32169022943292347, accuracy 0.88, precision 0.9375, recall 0.625, f1 0.6666666666666667\n",
            "{1: 860, 2: 13}\n",
            "(traditional) Val   loss 1.8529194775515887, accuracy 0.6025200458190149, precision 0.38064997018485397, recall 0.3428571428571428, f1 0.26899430924929685\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.10919884585642389, accuracy 0.96, precision 0.9791666666666667, recall 0.75, f1 0.8226950354609929\n",
            "{1: 832, 2: 41}\n",
            "(traditional) Val   loss 1.6929881229590558, accuracy 0.6105383734249714, precision 0.36813047217010625, recall 0.35669065313986864, f1 0.29976586453270393\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.03435264965186694, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 814, 2: 59}\n",
            "(traditional) Val   loss 1.7717701878843055, accuracy 0.6231386025200458, precision 0.3956953872208109, recall 0.37309320645406524, f1 0.32793231202537476\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.025490772012355074, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 788, 2: 85}\n",
            "(traditional) Val   loss 1.8833538557373022, accuracy 0.6391752577319587, precision 0.40746989150990337, recall 0.39357739242132306, f1 0.3586175449985703\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.010310476113642966, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 760, 2: 113}\n",
            "(traditional) Val   loss 1.9252641811157836, accuracy 0.6460481099656358, precision 0.40306629405371835, recall 0.4067686419461812, f1 0.3771189957237891\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.3 Initial loss: 0.6922941122736249\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0033520312119071603\n",
            "PL-Accuracy: 0.84 Precision: 0.42 PL-Recall: 0.5 PL-F1: 0.4565217391304348 PL-Loss: 1.2929156242420763\n",
            "(NSSDL) Train loss 0.6481338277269917, accuracy 0.9199999999999999, precision 0.71, recall 0.75, f1 0.7282608695652174\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.3039565190450144, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000704532322997693\n",
            "PL-Accuracy: 0.92 Precision: 0.46 PL-Recall: 0.5 PL-F1: 0.4791666666666667 PL-Loss: 0.4035039100812615\n",
            "(NSSDL) Train loss 0.20210422120212962, accuracy 0.96, precision 0.73, recall 0.75, f1 0.7395833333333334\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.6622421868155035, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003302728622657014\n",
            "PL-Accuracy: 0.88 Precision: 0.9375 PL-Recall: 0.625 PL-F1: 0.6666666666666667 PL-Loss: 0.5197826991872196\n",
            "(NSSDL) Train loss 0.2600564860247427, accuracy 0.94, precision 0.96875, recall 0.8125, f1 0.8333333333333334\n",
            "{1: 863, 2: 10}\n",
            "(NSSDL) Val   loss 3.113586818622361, accuracy 0.5979381443298969, precision 0.3993047508690614, recall 0.3395698163658527, f1 0.2646008910076332\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00036163402619422414\n",
            "PL-Accuracy: 0.92 Precision: 0.46 PL-Recall: 0.5 PL-F1: 0.4791666666666667 PL-Loss: 0.4469543290324509\n",
            "(NSSDL) Train loss 0.22365798152932256, accuracy 0.96, precision 0.73, recall 0.75, f1 0.7395833333333334\n",
            "{1: 864, 2: 9}\n",
            "(NSSDL) Val   loss 3.5877834469478054, accuracy 0.5967926689576174, precision 0.38425925925925924, recall 0.3382092721481656, f1 0.26185799443182406\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0002507130720186979\n",
            "PL-Accuracy: 0.84 Precision: 0.9166666666666667 PL-Recall: 0.6 PL-F1: 0.6212121212121212 PL-Loss: 0.5734666485971373\n",
            "(NSSDL) Train loss 0.286858680834578, accuracy 0.9199999999999999, precision 0.9583333333333334, recall 0.8, f1 0.8106060606060606\n",
            "{1: 844, 2: 29}\n",
            "(NSSDL) Val   loss 1.888342639443052, accuracy 0.6048109965635738, precision 0.37502042817453835, recall 0.34988793205143326, f1 0.28741344500913074\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0007784319589882216\n",
            "PL-Accuracy: 0.96 Precision: 0.48 PL-Recall: 0.5 PL-F1: 0.4897959183673469 PL-Loss: 0.24497795675311604\n",
            "(NSSDL) Train loss 0.12287819435605213, accuracy 0.98, precision 0.74, recall 0.75, f1 0.7448979591836735\n",
            "{1: 870, 2: 3}\n",
            "(NSSDL) Val   loss 4.1942526887727185, accuracy 0.5910652920962199, precision 0.19770114942528735, recall 0.3314065510597302, f1 0.24766018718502514\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0008499395971193736\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0009654567178846005\n",
            "(NSSDL) Train loss 0.000907698157501987, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 872, 2: 1}\n",
            "(NSSDL) Val   loss 4.447055218411969, accuracy 0.5933562428407789, precision 0.19801223241590216, recall 0.33269107257546565, f1 0.24826264078600527\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.6900319182677775e-05\n",
            "PL-Accuracy: 0.92 Precision: 0.46 PL-Recall: 0.5 PL-F1: 0.4791666666666667 PL-Loss: 0.17637178421552693\n",
            "(NSSDL) Train loss 0.08820434226735481, accuracy 0.96, precision 0.73, recall 0.75, f1 0.7395833333333334\n",
            "{1: 847, 2: 26}\n",
            "(NSSDL) Val   loss 3.8735884937313707, accuracy 0.6071019473081328, precision 0.4074107710471347, recall 0.3518907370269881, f1 0.29021446876255\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.679462902458908e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 3.45354835319865e-05\n",
            "(NSSDL) Train loss 4.0665056278287794e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 818, 2: 55}\n",
            "(NSSDL) Val   loss 3.9211980961662736, accuracy 0.6300114547537228, precision 0.42763577091205446, recall 0.3791016213807296, f1 0.336295188232361\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.1424832282027635e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.018255282797586654\n",
            "(NSSDL) Train loss 0.00914335381493434, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 811, 2: 62}\n",
            "(NSSDL) Val   loss 4.156929625847106, accuracy 0.6219931271477663, precision 0.3920024395741352, recall 0.37316922915601697, f1 0.3289706350566971\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00020719844887935325\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00977443158834441\n",
            "(NSSDL Strong) Train loss 0.004990815018611881, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 673, 2: 200}\n",
            "(NSSDL Strong) Val   loss 2.3847625968842854, accuracy 0.6449026345933563, precision 0.387788509162952, recall 0.42839316844271425, f1 0.40218937737224447\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011305361962513416\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.006038389412424294\n",
            "(NSSDL Strong) Train loss 0.0030757215160247143, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 741, 2: 132}\n",
            "(NSSDL Strong) Val   loss 3.0444036641844363, accuracy 0.6391752577319587, precision 0.3921400237189711, recall 0.40722477815789127, f1 0.3795068277826899\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.747868952516001e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0010485981869611091\n",
            "(NSSDL Strong) Train loss 0.0005580384382431346, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 651, 2: 222}\n",
            "(NSSDL Strong) Val   loss 3.152363035587586, accuracy 0.6242840778923253, precision 0.3740468579178257, recall 0.41898732518055387, f1 0.39288542372054147\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.57846008107299e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 6.636244480822435e-05\n",
            "(NSSDL Strong) Train loss 6.607352280947712e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 845, 2: 28}\n",
            "(NSSDL Strong) Val   loss 4.396255219727732, accuracy 0.6059564719358533, precision 0.40435333896872355, recall 0.35196675972893976, f1 0.2917584207906789\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.7060557731601874e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 4.484131100720593e-05\n",
            "(NSSDL Strong) Train loss 3.5950934369403904e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 845, 2: 28}\n",
            "(NSSDL Strong) Val   loss 4.4618220579910135, accuracy 0.6059564719358533, precision 0.40435333896872355, recall 0.35196675972893976, f1 0.2917584207906789\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.4808461084830924e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 5.3747147051451195e-05\n",
            "(NSSDL Strong) Train loss 3.927780406814106e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 845, 2: 28}\n",
            "(NSSDL Strong) Val   loss 4.5082987541349855, accuracy 0.6059564719358533, precision 0.40435333896872355, recall 0.35196675972893976, f1 0.2917584207906789\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 2.0038772422594775e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 5.190818098656434e-05\n",
            "(NSSDL Strong) Train loss 3.597347670457956e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 845, 2: 28}\n",
            "(NSSDL Strong) Val   loss 4.541987393133851, accuracy 0.6059564719358533, precision 0.40435333896872355, recall 0.35196675972893976, f1 0.2917584207906789\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 1.925023490912281e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0008171665888637238\n",
            "(NSSDL Strong) Train loss 0.0004182084118864233, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 758, 2: 115}\n",
            "(NSSDL Strong) Val   loss 3.9051154961471393, accuracy 0.6345933562428407, precision 0.39361018699093725, recall 0.3996277509076848, f1 0.37033701673482405\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 9.783063607756049e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 7.520365085968348e-05\n",
            "(NSSDL Strong) Train loss 8.651714346862198e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 850, 2: 23}\n",
            "(NSSDL Strong) Val   loss 4.592324339277545, accuracy 0.6093928980526919, precision 0.3919352088661552, recall 0.3510204081632653, f1 0.2850775341699101\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 1.9510368056216975e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 2.6982803839408526e-05\n",
            "(NSSDL Strong) Train loss 2.3246585947812752e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 850, 2: 23}\n",
            "(NSSDL Strong) Val   loss 4.651577421069046, accuracy 0.6093928980526919, precision 0.3919352088661552, recall 0.3510204081632653, f1 0.2850775341699101\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6324462906457484, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "(MixText) Val   loss 2.2812886808093165, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6159719178080558, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "(MixText) Val   loss 1.9116033046481808, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.6118284186720848, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "(MixText) Val   loss 2.403180677652837, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.7790280047245324, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "(MixText) Val   loss 2.396488888135856, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.41722624596208335, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 2.5417898309130096, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.008250945694744587, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.3973639280901193, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 1.2623675682768225, accuracy 0.72, precision 0.36, recall 0.5, f1 0.41860465116279066\n",
            "(MixText) Val   loss 2.4967222742838753, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.08869774335063994, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "(MixText) Val   loss 2.580993088819983, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0017422488192096353, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.747401661275836, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.14558312905021012, accuracy 0.96, precision 0.9772727272727273, recall 0.875, f1 0.9169435215946844\n",
            "(MixText) Val   loss 2.459663207925438, accuracy 0.6048109965635738, precision 0.3776243888409548, recall 0.3462965147523364, f1 0.2767698838347054\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.021657230434939265, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.5003707368793084, accuracy 0.6277205040091638, precision 0.3778128240991827, recall 0.4201958239943376, f1 0.3945171778610194\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0031367175397463143, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.638897522524925, accuracy 0.6162657502863689, precision 0.405648369132856, recall 0.36062024038902646, f1 0.30399164824367475\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.3509272193908691, accuracy 0.88, precision 0.625, recall 0.9375, f1 0.6666666666666667\n",
            "(MixText) Val   loss 1.663751423017539, accuracy 0.4822451317296678, precision 0.3495004391743522, recall 0.3932182506914134, f1 0.34037859549933724\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.2758286905474961, accuracy 0.88, precision 0.44, recall 0.5, f1 0.46808510638297873\n",
            "(MixText) Val   loss 2.754430597727502, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0011028959485702217, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.4172179784342, accuracy 0.6323024054982818, precision 0.4146920418106859, recall 0.38038614289646494, f1 0.3370118845500849\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0016192813217639924, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.345082946804449, accuracy 0.6323024054982818, precision 0.4146920418106859, recall 0.38038614289646494, f1 0.3370118845500849\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.002987332409247756, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.2238164152213953, accuracy 0.6380297823596792, precision 0.40753703581412754, recall 0.3893437143643585, f1 0.3512595201797826\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.0022123554535210133, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.3294781624955068, accuracy 0.6380297823596792, precision 0.4115047656783111, recall 0.3900619978241779, f1 0.35282977776515256\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.005164607097394764, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6997755868840463, accuracy 0.6426116838487973, precision 0.38906285454958023, recall 0.42710864692697886, f1 0.4015607199817726\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 0, 1, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.09668743133544921, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6454141819156185, accuracy 0.5807560137457045, precision 0.36544192291318733, recall 0.421876187854718, f1 0.38876102181906486\n",
            "\n",
            "{1: 427, 2: 58}\n",
            "{1: 455, 2: 30}\n",
            "{1: 470, 2: 15}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 0, 1, 2], device='cuda:0')\n",
            "Fold 10/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.7779465871197837, accuracy 0.76, precision 0.7196969696969697, recall 0.6150793650793651, f1 0.625\n",
            "{1: 873}\n",
            "(traditional) Val   loss 0.9715955296880034, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n",
            "(traditional) Train loss 0.22325401646750315, accuracy 0.92, precision 0.46, recall 0.5, f1 0.4791666666666667\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.3272555182309456, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 3/10\n",
            "----------\n",
            "(traditional) Train loss 0.32017492183617186, accuracy 0.88, precision 0.625, recall 0.4444444444444444, f1 0.4777777777777778\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.6350930025418327, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 4/10\n",
            "----------\n",
            "(traditional) Train loss 0.6447563807346991, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.9705021981613446, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(traditional) Epoch 5/10\n",
            "----------\n",
            "(traditional) Train loss 0.48385458080364124, accuracy 0.8, precision 0.26666666666666666, recall 0.3333333333333333, f1 0.29629629629629634\n",
            "{1: 847, 2: 26}\n",
            "(traditional) Val   loss 1.4048359581273577, accuracy 0.6071019473081328, precision 0.43226470499197767, recall 0.35332730394662687, f1 0.2941584320674832\n",
            "\n",
            "(traditional) Epoch 6/10\n",
            "----------\n",
            "(traditional) Train loss 0.3158646215285574, accuracy 0.92, precision 0.9473684210526316, recall 0.875, f1 0.9007936507936507\n",
            "{1: 760, 2: 113}\n",
            "(traditional) Val   loss 1.0327581023652803, accuracy 0.6002290950744559, precision 0.3654323862754231, recall 0.37533194395291836, f1 0.34554171889992036\n",
            "\n",
            "(traditional) Epoch 7/10\n",
            "----------\n",
            "(traditional) Train loss 0.05210402022515025, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 658, 2: 215}\n",
            "(traditional) Val   loss 1.1258199365818065, accuracy 0.6277205040091638, precision 0.39033953017129663, recall 0.42953350897198955, f1 0.40574292293105785\n",
            "\n",
            "(traditional) Epoch 8/10\n",
            "----------\n",
            "(traditional) Train loss 0.22442456128607904, accuracy 0.96, precision 0.6491228070175438, recall 0.6666666666666666, f1 0.6576576576576577\n",
            "{1: 708, 2: 165}\n",
            "(traditional) Val   loss 1.3392732382824295, accuracy 0.6197021764032073, precision 0.3817582605718199, recall 0.40636231371161174, f1 0.382722086389568\n",
            "\n",
            "(traditional) Epoch 9/10\n",
            "----------\n",
            "(traditional) Train loss 0.14699420850125275, accuracy 0.96, precision 0.6, recall 0.6666666666666666, f1 0.6296296296296297\n",
            "{1: 693, 2: 180}\n",
            "(traditional) Val   loss 1.468161895209881, accuracy 0.6174112256586484, precision 0.3798941798941799, recall 0.4093874929547927, f1 0.3861140231670226\n",
            "\n",
            "(traditional) Epoch 10/10\n",
            "----------\n",
            "(traditional) Train loss 0.16341473188783442, accuracy 0.96, precision 0.6507936507936508, recall 0.6666666666666666, f1 0.6585365853658537\n",
            "{1: 656, 2: 217}\n",
            "(traditional) Val   loss 1.4436897168544092, accuracy 0.6242840778923253, precision 0.386923494810985, recall 0.4268884432385671, f1 0.4029117926990267\n",
            "\n",
            "(NSSDL) Epoch 1/10\n",
            "----------\n",
            "Initial acc: 0.41269841269841273 Initial loss: 0.7367114233119147\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0035638466684758895\n",
            "PL-Accuracy: 0.84 Precision: 0.42 PL-Recall: 0.5 PL-F1: 0.4565217391304348 PL-Loss: 1.1504213439516857\n",
            "(NSSDL) Train loss 0.5769925953100808, accuracy 0.9199999999999999, precision 0.71, recall 0.75, f1 0.7282608695652174\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 3.156084759501617, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000630788035268779\n",
            "PL-Accuracy: 0.76 Precision: 0.38 PL-Recall: 0.5 PL-F1: 0.4318181818181818 PL-Loss: 1.2179795247377894\n",
            "(NSSDL) Train loss 0.609305156386529, accuracy 0.88, precision 0.69, recall 0.75, f1 0.7159090909090909\n",
            "{1: 873}\n",
            "(NSSDL) Val   loss 1.6139589488744464, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(NSSDL) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00082274856933509\n",
            "PL-Accuracy: 0.84 Precision: 0.6086956521739131 PL-Recall: 0.4666666666666666 PL-F1: 0.4920634920634921 PL-Loss: 0.8736111905226218\n",
            "(NSSDL) Train loss 0.43721696954597844, accuracy 0.9199999999999999, precision 0.8043478260869565, recall 0.7333333333333333, f1 0.746031746031746\n",
            "{1: 832, 2: 41}\n",
            "(NSSDL) Val   loss 2.670066373837372, accuracy 0.6082474226804123, precision 0.4059763914946841, recall 0.35899754892323016, f1 0.3079666447542095\n",
            "\n",
            "(NSSDL) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.000541946773919335\n",
            "PL-Accuracy: 0.92 Precision: 0.95 PL-Recall: 0.8571428571428572 PL-F1: 0.8903508771929824 PL-Loss: 0.36932291499605135\n",
            "(NSSDL) Train loss 0.18493243088498534, accuracy 0.96, precision 0.975, recall 0.9285714285714286, f1 0.9451754385964912\n",
            "{1: 804, 2: 69}\n",
            "(NSSDL) Val   loss 2.9534203183886936, accuracy 0.5750286368843069, precision 0.3141178167135338, recall 0.33965370348524765, f1 0.2918221310699321\n",
            "\n",
            "(NSSDL) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00018488328967578127\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.003753571572555562\n",
            "(NSSDL) Train loss 0.0019692274311156714, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 802, 2: 71}\n",
            "(NSSDL) Val   loss 4.185188851181774, accuracy 0.5784650630011455, precision 0.32115134698465103, recall 0.3430170526784895, f1 0.29658837921661446\n",
            "\n",
            "(NSSDL) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 3.9644237931497626e-05\n",
            "PL-Accuracy: 0.88 Precision: 0.6111111111111112 PL-Recall: 0.625 PL-F1: 0.6141414141414141 PL-Loss: 0.8932385717510313\n",
            "(NSSDL) Train loss 0.44663910799448137, accuracy 0.94, precision 0.8055555555555556, recall 0.8125, f1 0.807070707070707\n",
            "{1: 852, 2: 21}\n",
            "(NSSDL) Val   loss 4.129777519429214, accuracy 0.6036655211912944, precision 0.42292644757433484, recall 0.3485273878337462, f1 0.28454067334638566\n",
            "\n",
            "(NSSDL) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.0003285451418196317\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00019781476138242788\n",
            "(NSSDL) Train loss 0.0002631799516010298, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 849, 2: 24}\n",
            "(NSSDL) Val   loss 4.136359358519507, accuracy 0.6048109965635738, precision 0.4232430310168826, recall 0.35060621551125265, f1 0.289165706500866\n",
            "\n",
            "(NSSDL) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 5.47562143583491e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.9782608695652174 PL-Recall: 0.8333333333333333 PL-F1: 0.8888888888888888 PL-Loss: 0.33922004835299696\n",
            "(NSSDL) Train loss 0.16963740228367766, accuracy 0.98, precision 0.9891304347826086, recall 0.9166666666666666, f1 0.9444444444444444\n",
            "{1: 835, 2: 38}\n",
            "(NSSDL) Val   loss 3.8360921248130824, accuracy 0.6002290950744559, precision 0.37663620128164715, recall 0.35091030631905945, f1 0.2952678709681909\n",
            "\n",
            "(NSSDL) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00013591398805147038\n",
            "PL-Accuracy: 0.96 Precision: 0.875 PL-Recall: 0.9772727272727273 PL-F1: 0.9169435215946844 PL-Loss: 0.10734272357616906\n",
            "(NSSDL) Train loss 0.05373931878211027, accuracy 0.98, precision 0.9375, recall 0.9886363636363636, f1 0.9584717607973422\n",
            "{1: 834, 2: 39}\n",
            "(NSSDL) Val   loss 3.9176685246732394, accuracy 0.6002290950744559, precision 0.38052634815224745, recall 0.35162858977887884, f1 0.2971400712726718\n",
            "\n",
            "(NSSDL) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.747938423861342e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.65 PL-Recall: 0.6666666666666666 PL-F1: 0.6581196581196581 PL-Loss: 1.051328810524345\n",
            "(NSSDL) Train loss 0.5256881449542918, accuracy 0.98, precision 0.825, recall 0.8333333333333333, f1 0.829059829059829\n",
            "{1: 784, 2: 89}\n",
            "(NSSDL) Val   loss 4.059628022446176, accuracy 0.5693012600229095, precision 0.314234120614538, recall 0.34147038391464485, f1 0.3003003969038132\n",
            "\n",
            "(NSSDL Strong) Epoch 1/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00044784911242459204\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.008058543749419706\n",
            "(NSSDL Strong) Train loss 0.004253196430922149, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{2: 623, 1: 250}\n",
            "(NSSDL Strong) Val   loss 3.891122752550477, accuracy 0.47995418098510884, precision 0.3862364901016586, recall 0.42425648486755013, f1 0.3418847466836057\n",
            "\n",
            "(NSSDL Strong) Epoch 2/10\n",
            "----------\n",
            "FT-Accuracy: 0.9425 Precision: 0.9459113109918273 FT-Recall: 0.8671364867017042 FT-F1: 0.8998683050533854 FT-Loss: 0.30143789767764245\n",
            "PL-Accuracy: 0.92 Precision: 0.6333333333333333 PL-Recall: 0.6666666666666666 PL-F1: 0.6491228070175439 PL-Loss: 0.5785484406099256\n",
            "(NSSDL Strong) Train loss 0.43999316914378406, accuracy 0.93125, precision 0.7896223221625803, recall 0.7669015766841853, f1 0.7744955560354647\n",
            "{1: 659, 2: 214}\n",
            "(NSSDL Strong) Val   loss 2.1345048311881967, accuracy 0.5876288659793815, precision 0.35204383116115706, recall 0.39268871325023263, f1 0.3683347450783117\n",
            "\n",
            "(NSSDL Strong) Epoch 3/10\n",
            "----------\n",
            "FT-Accuracy: 0.995 Precision: 0.997289972899729 FT-Recall: 0.9696969696969697 FT-F1: 0.983016304347826 FT-Loss: 0.03936784970836015\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.0007876675143571836\n",
            "(NSSDL Strong) Train loss 0.020077758611358666, accuracy 0.9975, precision 0.9986449864498645, recall 0.9848484848484849, f1 0.9915081521739131\n",
            "{1: 651, 2: 222}\n",
            "(NSSDL Strong) Val   loss 3.208500137083569, accuracy 0.5910652920962199, precision 0.3542505639279833, recall 0.3967703459032939, f1 0.37207245130157335\n",
            "\n",
            "(NSSDL Strong) Epoch 4/10\n",
            "----------\n",
            "FT-Accuracy: 0.9975 Precision: 0.9986807387862797 FT-Recall: 0.9772727272727273 FT-F1: 0.9877115910417499 FT-Loss: 0.014309994796694809\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 8.781705972588887e-05\n",
            "(NSSDL Strong) Train loss 0.007198905928210349, accuracy 0.99875, precision 0.9993403693931399, recall 0.9886363636363636, f1 0.9938557955208749\n",
            "{1: 692, 2: 181}\n",
            "(NSSDL Strong) Val   loss 3.6826276363447517, accuracy 0.5922107674684994, precision 0.35103098819446665, recall 0.3859200713040514, f1 0.36069596771379714\n",
            "\n",
            "(NSSDL Strong) Epoch 5/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 8.801777323242277e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 4.7120256827578745e-05\n",
            "(NSSDL Strong) Train loss 6.756901503000075e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 664, 2: 209}\n",
            "(NSSDL Strong) Val   loss 3.7651224924499718, accuracy 0.5979381443298969, precision 0.3582223631367576, recall 0.3984690600710419, f1 0.37379766962959105\n",
            "\n",
            "(NSSDL Strong) Epoch 6/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 6.874383909234894e-05\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 5.668614903697744e-05\n",
            "(NSSDL Strong) Train loss 6.27149940646632e-05, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 699, 2: 174}\n",
            "(NSSDL Strong) Val   loss 3.942364923513956, accuracy 0.5922107674684994, precision 0.35157778764408926, recall 0.38448350438441276, f1 0.3591709089199007\n",
            "\n",
            "(NSSDL Strong) Epoch 7/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 4.633954198652646e-05\n",
            "PL-Accuracy: 0.96 Precision: 0.5833333333333334 PL-Recall: 0.6666666666666666 PL-F1: 0.6190476190476191 PL-Loss: 0.2260431253094534\n",
            "(NSSDL Strong) Train loss 0.11304473242571995, accuracy 0.98, precision 0.7916666666666667, recall 0.8333333333333333, f1 0.8095238095238095\n",
            "{1: 766, 2: 107}\n",
            "(NSSDL Strong) Val   loss 4.301942358630335, accuracy 0.5853379152348225, precision 0.3349316349853184, recall 0.35836315258280577, f1 0.3228658177101757\n",
            "\n",
            "(NSSDL Strong) Epoch 8/10\n",
            "----------\n",
            "FT-Accuracy: 0.995 Precision: 0.9794323323735088 FT-Recall: 0.9794323323735088 FT-F1: 0.9794323323735088 FT-Loss: 0.019532828607561895\n",
            "PL-Accuracy: 0.96 Precision: 0.6507936507936508 PL-Recall: 0.6666666666666666 PL-F1: 0.6585365853658537 PL-Loss: 0.2685531588335185\n",
            "(NSSDL Strong) Train loss 0.1440429937205402, accuracy 0.9775, precision 0.8151129915835797, recall 0.8230494995200877, f1 0.8189844588696813\n",
            "{1: 761, 0: 26, 2: 86}\n",
            "(NSSDL Strong) Val   loss 3.4980580032028494, accuracy 0.5899198167239404, precision 0.4484064764295794, recall 0.37594924059670004, f1 0.35732622570959677\n",
            "\n",
            "(NSSDL Strong) Epoch 9/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00040175700534746285\n",
            "PL-Accuracy: 0.92 Precision: 0.7857142857142857 PL-Recall: 0.7857142857142857 PL-F1: 0.7857142857142857 PL-Loss: 0.2975740312218217\n",
            "(NSSDL Strong) Train loss 0.14898789411358457, accuracy 0.96, precision 0.8928571428571428, recall 0.8928571428571428, f1 0.8928571428571428\n",
            "{1: 640, 2: 189, 0: 44}\n",
            "(NSSDL Strong) Val   loss 3.678877036866731, accuracy 0.5670103092783505, precision 0.43528213684463685, recall 0.39888659570172313, f1 0.4001967794889456\n",
            "\n",
            "(NSSDL Strong) Epoch 10/10\n",
            "----------\n",
            "FT-Accuracy: 1.0 Precision: 1.0 FT-Recall: 1.0 FT-F1: 1.0 FT-Loss: 0.00011565234903173405\n",
            "PL-Accuracy: 1.0 Precision: 1.0 PL-Recall: 1.0 PL-F1: 1.0 PL-Loss: 0.00017141048283519922\n",
            "(NSSDL Strong) Train loss 0.00014353141593346664, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "{1: 627, 2: 246}\n",
            "(NSSDL Strong) Val   loss 3.6459295770665827, accuracy 0.5853379152348225, precision 0.3515320081430479, recall 0.3978687428728717, f1 0.3725417017661716\n",
            "\n",
            "(MixText) Epoch 1/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.9533313295990229, accuracy 0.76, precision 0.38, recall 0.5, f1 0.4318181818181818\n",
            "(MixText) Val   loss 1.67570217674916, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 2/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.48322966173291204, accuracy 0.84, precision 0.42, recall 0.5, f1 0.4565217391304348\n",
            "(MixText) Val   loss 1.5139894025637113, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 3/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 1.0153318119049073, accuracy 0.68, precision 0.34, recall 0.5, f1 0.40476190476190477\n",
            "(MixText) Val   loss 1.635324306384941, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 4/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.3024266701936722, accuracy 0.92, precision 0.6376811594202899, recall 0.5555555555555555, f1 0.5848484848484848\n",
            "(MixText) Val   loss 1.4700573528140681, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "(MixText) Epoch 5/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.2903118731081486, accuracy 0.92, precision 0.6315789473684211, recall 0.6190476190476191, f1 0.6225071225071225\n",
            "(MixText) Val   loss 1.663754040347615, accuracy 0.5956471935853379, precision 0.4206896551724138, recall 0.3354121610108398, f1 0.2539964544771902\n",
            "\n",
            "(MixText) Epoch 6/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.028260408788919448, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 2.006209821609902, accuracy 0.5945017182130584, precision 0.36542201478024117, recall 0.3390796010118884, f1 0.268011746710624\n",
            "\n",
            "(MixText) Epoch 7/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.192357472255826, accuracy 0.96, precision 0.6507936507936508, recall 0.6666666666666666, f1 0.6585365853658537\n",
            "(MixText) Val   loss 2.2271885150341575, accuracy 0.5990836197021764, precision 0.38840579710144923, recall 0.34524006134245605, f1 0.2806947730303922\n",
            "\n",
            "(MixText) Epoch 8/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.21947998709976674, accuracy 0.96, precision 0.65, recall 0.6666666666666666, f1 0.6581196581196581\n",
            "(MixText) Val   loss 1.758695908291916, accuracy 0.5979381443298969, precision 0.37724187475227905, recall 0.34747093442386584, f1 0.2884635803779996\n",
            "\n",
            "(MixText) Epoch 9/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.015616012457758188, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.8284527302889275, accuracy 0.5956471935853379, precision 0.3577260055754679, recall 0.3461864129081305, f1 0.2876281156286195\n",
            "\n",
            "(MixText) Epoch 10/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1], device='cuda:0')\n",
            "tensor([0, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.18843942195177077, accuracy 0.96, precision 0.6, recall 0.6666666666666666, f1 0.6296296296296297\n",
            "(MixText) Val   loss 1.1325333859229552, accuracy 0.561282932416953, precision 0.3502245929253229, recall 0.40449320383259274, f1 0.37425256361772913\n",
            "\n",
            "(MixText) Epoch 11/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.027599056512117387, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3397649533423635, accuracy 0.5738831615120275, precision 0.33815664517418903, recall 0.37061591495943275, f1 0.34539259539259537\n",
            "\n",
            "(MixText) Epoch 12/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "tensor([1, 2, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.018580009639263154, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.3565417646866758, accuracy 0.584192439862543, precision 0.3486527984150569, recall 0.3864522302177133, f1 0.36222004373727473\n",
            "\n",
            "(MixText) Epoch 13/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 1, 0], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.09115340361371636, accuracy 0.96, precision 0.6521739130434783, recall 0.6666666666666666, f1 0.6592592592592593\n",
            "(MixText) Val   loss 1.8390474433307658, accuracy 0.5979381443298969, precision 0.367547018807523, recall 0.349625784803324, f1 0.2943181424962802\n",
            "\n",
            "(MixText) Epoch 14/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.10726671608164906, accuracy 0.96, precision 0.6515151515151515, recall 0.6666666666666666, f1 0.6589147286821705\n",
            "(MixText) Val   loss 1.5197129940266325, accuracy 0.570446735395189, precision 0.332594649003182, recall 0.36797084922601025, f1 0.3423335218666095\n",
            "\n",
            "(MixText) Epoch 15/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.012993562631309033, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.6087425987685327, accuracy 0.5922107674684994, precision 0.34363643705079544, recall 0.3507241817729018, f1 0.302575258336075\n",
            "\n",
            "(MixText) Epoch 16/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.03489353634417057, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.934218273381836, accuracy 0.6013745704467354, precision 0.3975180714311149, recall 0.3465245828581915, f1 0.2816078442093369\n",
            "\n",
            "(MixText) Epoch 17/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.22850180000066758, accuracy 0.96, precision 0.9833333333333334, recall 0.8333333333333334, f1 0.8803418803418803\n",
            "(MixText) Val   loss 1.6465557331148906, accuracy 0.5899198167239404, precision 0.33548331613347093, recall 0.3472848098777083, f1 0.2964655108266984\n",
            "\n",
            "(MixText) Epoch 18/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.018761895149946212, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4698153645534149, accuracy 0.5853379152348225, precision 0.3439816626931882, recall 0.37416538869883215, f1 0.3474315332917062\n",
            "\n",
            "(MixText) Epoch 19/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 2], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.01908090667799115, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4379884557625682, accuracy 0.5876288659793815, precision 0.34052484339840666, recall 0.36323909139763805, f1 0.3304560679304039\n",
            "\n",
            "(MixText) Epoch 20/20\n",
            "----------\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "tensor([1, 1, 2, 1], device='cuda:0')\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "(MixText) Train loss 0.011889668181538582, accuracy 1.0, precision 1.0, recall 1.0, f1 1.0\n",
            "(MixText) Val   loss 1.4398929760679346, accuracy 0.584192439862543, precision 0.3394164315216947, recall 0.36562200988295124, f1 0.33552131138338037\n",
            "\n",
            "{2: 124, 1: 361}\n",
            "{1: 430, 2: 55}\n",
            "{2: 128, 1: 357}\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 1], device='cuda:0')\n",
            "tensor([1, 2, 2, 1], device='cuda:0')\n",
            "Data class of 50 labeled samples per fold\n",
            "--------------------------------------------------\n",
            "\n",
            "Fold 1/10\n",
            "----------\n",
            "\n",
            "(traditional) Epoch 1/10\n",
            "----------\n",
            "(traditional) Train loss 0.6703486049977633, accuracy 0.78, precision 0.29545454545454547, recall 0.2888888888888889, f1 0.29213483146067415\n",
            "{1: 873}\n",
            "(traditional) Val   loss 1.5320321950704268, accuracy 0.5945017182130584, precision 0.1981672394043528, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(traditional) Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-44-008e1d73be3f>\", line 185, in <module>\n",
            "    10)\n",
            "  File \"<ipython-input-42-6589f8072cde>\", line 36, in run_model_training\n",
            "    n_examples=len(training_set))\n",
            "  File \"<ipython-input-36-803c0e96eed9>\", line 42, in traditional_train_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 739, in getmodule\n",
            "    f = getabsfile(module)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
            "    _filename = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
            "    if os.path.exists(filename):\n",
            "  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n",
            "    os.stat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRmOxXl1LYe"
      },
      "source": [
        "from transformers import logging\n",
        "from google.colab import files\n",
        "logging.set_verbosity_error()\n",
        "    \n",
        "FOLDS = 10\n",
        "RANDOM_STATES = ['1210', '505', '2506', '1807', '1402', '107', '1803', '2405', '208', '2209']\n",
        "\n",
        "overall_saving_dict = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "best_models = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "\n",
        "test_txt_string = \"\"\n",
        "\n",
        "total_steps = 0\n",
        "flag = 0\n",
        "\n",
        "# set random_state\n",
        "random_state = int(str(1) + RANDOM_STATES[0])\n",
        "\n",
        "# set labeled- and unlabeled data and train-, val-, test data\n",
        "split_data = split_data_into_train_test_val(data['article'], data['label'], 0.2, random_state)\n",
        "\n",
        "X_train = split_data['train']['X']\n",
        "X_test = split_data['test']['X']\n",
        "X_val = split_data['val']['X']\n",
        "\n",
        "y_train = split_data['train']['y']\n",
        "y_test = split_data['test']['y']\n",
        "y_val = split_data['val']['y']\n",
        "\n",
        "data_dict = get_dataclass_distribution_of_unlabeled_data(data_class_size=50,\n",
        "                                                          X=X_train, \n",
        "                                                          y=y_train, \n",
        "                                                          random_state=random_state)\n",
        "\n",
        "X_train_labeled = data_dict['labeled_data']\n",
        "X_train_unlabeled = data_dict['unlabeled_data']\n",
        "\n",
        "y_train_labeled = data_dict['labeled_data_labels']\n",
        "y_train_unlabeled = data_dict['unlabeled_data_labels'] # for testing purposes (maybe)\n",
        "\n",
        "# initialise BertData\n",
        "training_set = BertData(article=X_train_labeled, label=y_train_labeled)\n",
        "validation_set = BertData(article=X_val, label=y_val)\n",
        "test_set = BertData(article=X_test, label=y_test)\n",
        "\n",
        "# initialize dataloaders\n",
        "train_dataloader, val_dataloader, test_dataloader = get_dataloader(training_set, \n",
        "                                                                    validation_set, \n",
        "                                                                    test_set)\n",
        "\n",
        "mix_val_loader = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_test,\n",
        "            dataset_label=y_test, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         shuffle=True)\n",
        "\n",
        "# for MixText\n",
        "mix_train_loader = DataLoader(\n",
        "    dataset=mixtext_loader_labeled(\n",
        "        dataset_text=X_train_labeled,\n",
        "        dataset_label=y_train_labeled, \n",
        "        tokenizer=tokenizer, \n",
        "        max_seq_len=MAX_LEN\n",
        "        ),\n",
        "      batch_size=BATCH_SIZE, \n",
        "      shuffle=True)\n",
        "\n",
        "mix_train_unlabeled = DataLoader(\n",
        "    dataset=mixtext_loader_unlabeled(\n",
        "        dataset_text=X_train_unlabeled,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_len=MAX_LEN,\n",
        "        aug=Translator(mix_path)),\n",
        "      batch_size=BATCH_SIZE, \n",
        "      shuffle=True)\n",
        "\n",
        "mix_val_loader = DataLoader(\n",
        "    dataset=mixtext_loader_labeled(\n",
        "        dataset_text=X_val,\n",
        "        dataset_label=y_val, \n",
        "        tokenizer=tokenizer, \n",
        "        max_seq_len=MAX_LEN\n",
        "        ),\n",
        "      batch_size=BATCH_SIZE, \n",
        "      shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# execute evaluation with test set\n",
        "# traditional\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Data/Model/'\n",
        "files.download(PATH+'fold_0_traditional_best_model_state.bin')\n",
        "tradi_state_dict = torch.load(PATH+'fold_0_traditional_best_model_state.bin')\n",
        "\n",
        "trad = ClassicalBertClassifier()\n",
        "trad = trad.to(device)\n",
        "trad.load_state_dict(tradi_state_dict)\n",
        "\n",
        "tradi_test_acc, tradi_test_precision, tradi_test_recall, tradi_test_f1, tradi_test_loss = eval_model(\n",
        "  trad,\n",
        "  test_dataloader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(validation_set))\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Data/Model/'\n",
        "files.download(PATH+'fold_0_NSSDL_best_model_state.bin')\n",
        "nssdl_state_dict = torch.load(PATH+'fold_0_NSSDL_best_model_state.bin')\n",
        "\n",
        "\n",
        "nssdl = ClassicalBertClassifier()\n",
        "nssdl = nssdl.to(device)\n",
        "nssdl.load_state_dict(nssdl_state_dict)\n",
        "\n",
        "nssdl_test_acc, nssdl_test_precision, nssdl_test_recall, nssdl_test_f1, nssdl_test_loss = eval_model(\n",
        "  nssdl,\n",
        "  test_dataloader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(validation_set))\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Data/Model/'\n",
        "files.download(PATH+'fold_0_NSSDL Strong_best_model_state.bin')\n",
        "nssdl_s_state_dict = torch.load(PATH+'fold_0_NSSDL Strong_best_model_state.bin')\n",
        "\n",
        "\n",
        "nssdl_strong = ClassicalBertClassifier()\n",
        "nssdl_strong = nssdl_strong.to(device)\n",
        "nssdl_strong.load_state_dict(nssdl_s_state_dict)\n",
        "\n",
        "nssdl_strong_test_acc, nssdl_strong_test_precision, nssdl_strong_test_recall, nssdl_strong_test_f1, nssdl_strong_test_loss = eval_model(\n",
        "  nssdl_strong,\n",
        "  test_dataloader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(validation_set))\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Data/Model/'\n",
        "files.download(PATH+'fold_0_MixText_best_model_state.bin')\n",
        "mix_state_dict = torch.load(PATH+'fold_0_MixText_best_model_state.bin')\n",
        "\n",
        "\n",
        "mixtext = MixText(num_labels=3, mix_option=True)\n",
        "mixtext = nn.DataParallel(mixtext)\n",
        "mixtext = mixtext.to(device)\n",
        "mixtext.load_state_dict(mix_state_dict)\n",
        "\n",
        "mixtext_test_loss, mixtext_test_acc, mixtext_test_precision, mixtext_test_recall, mixtext_test_f1 = mix_validate(\n",
        "          mix_val_loader, mixtext, loss_fn, 0, mode='Test Stats')\n",
        "\n",
        "# build dict for saving\n",
        "saving_dict = {\n",
        "      'tradi': {\n",
        "          'test': {\n",
        "              'acc': tradi_test_acc,\n",
        "              'precision': tradi_test_precision,\n",
        "              'recall': tradi_test_recall, \n",
        "              'f1': tradi_test_f1, \n",
        "              'loss': tradi_test_loss\n",
        "          }\n",
        "      },\n",
        "      'nssdl': {\n",
        "          'test': {\n",
        "              'acc': nssdl_test_acc,\n",
        "              'precision': nssdl_test_precision,\n",
        "              'recall': nssdl_test_recall, \n",
        "              'f1': nssdl_test_f1, \n",
        "              'loss': nssdl_test_loss\n",
        "          }\n",
        "      },\n",
        "      'nssdl_strong': {\n",
        "          'test': {\n",
        "              'acc': nssdl_strong_test_acc,\n",
        "              'precision': nssdl_strong_test_precision,\n",
        "              'recall': nssdl_strong_test_recall, \n",
        "              'f1': nssdl_strong_test_f1, \n",
        "              'loss': nssdl_strong_test_loss\n",
        "          }\n",
        "      },\n",
        "      'mixtext': {\n",
        "          'test': {\n",
        "              'acc': mixtext_test_acc,\n",
        "              'precision': mixtext_test_precision,\n",
        "              'recall': mixtext_test_recall, \n",
        "              'f1': mixtext_test_f1, \n",
        "              'loss': mixtext_test_loss\n",
        "          }\n",
        "      }\n",
        "    }\n",
        "\n",
        "data_test = [{\n",
        "              'fold': 0,\n",
        "              'model': kind,\n",
        "              'test_acc': saving_dict[kind]['test']['acc'],\n",
        "              'test_precision': saving_dict[kind]['test']['precision'],\n",
        "              'test_recall': saving_dict[kind]['test']['recall'],\n",
        "              'test_f1': saving_dict[kind]['test']['f1'],\n",
        "              'test_loss': saving_dict[kind]['test']['loss']\n",
        "          } for kind in list(saving_dict.keys())]\n",
        "\n",
        "df_test = pd.DataFrame(data_test)\n",
        "\n",
        "# to excel\n",
        "df_test.to_excel(f\"/content/drive/My Drive/Colab Notebooks/BA/test_data_from_fold_0.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNXbuBWF-mI_"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcAx_lP9Si2c"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_dataloader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(test_set)\n",
        ")\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkzkVJT2SqWc"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"article\"]\n",
        "      input_ids = d[\"ids\"].to(device)\n",
        "      attention_mask = d[\"mask\"].to(device)\n",
        "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        ids=input_ids,\n",
        "        mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsh1HoYHSuWU"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_dataloader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNbnvjrSuqz"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJfGxOmUYTi"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
        "\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmqT5sToXrcb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crte907NuVqx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}