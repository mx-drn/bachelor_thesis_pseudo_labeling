{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "first_implementation_of_BERT_on_kaggle_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dwPSGTb2bIA_",
        "eHa6eFazbgbx",
        "tZ4SBdmxbpua",
        "0k9CLe9PbyqG",
        "opPjR9d_cJaC",
        "fDI95MW1eize",
        "HYnMDrgtzaWt",
        "GdIM8YECp3Bn",
        "GABnMc4a10tP",
        "gBzofDQZ18Ey"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hZO7R9TMzaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb836dc4-fa03-4a88-a4f6-bb67fdb7681a"
      },
      "source": [
        "#!pip install transformers\n",
        "#!pip install wandb\n",
        "#!pip install easynmt\n",
        "!pip install pydeepl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydeepl in /usr/local/lib/python3.7/dist-packages (0.13)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from pydeepl) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->pydeepl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->pydeepl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->pydeepl) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->pydeepl) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLNtikhwX99e"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCt7gXW-i9yc"
      },
      "source": [
        "#print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMT5LWhkMzal"
      },
      "source": [
        "# needed imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import time\n",
        "#import wandb\n",
        "import pickle\n",
        "from easynmt import EasyNMT\n",
        "import requests\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from textwrap import wrap\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings, BertPooler, BertLayer\n",
        "#import pytorch_lightning as pl\n",
        "\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZ7vslbMzal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7232db6-72d1-43c0-db9d-7488f7862b2c"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# Use GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Aug  6 20:04:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRiZccwMzam"
      },
      "source": [
        "# Get an Overview over the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMmaEOMh1Fdh",
        "outputId": "3ede58a1-592e-480d-cef9-afeaa36fbdaa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MIORSrKMzam",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f796eff0-ffdb-4337-8c16-5ea1952c410c"
      },
      "source": [
        "# import data\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Data/kaggle_data.csv\", names=['label', 'article'], encoding='latin-1')\n",
        "\n",
        "print(\"Overall distribution of the data and labels:\")\n",
        "print(data['label'].value_counts(ascending=True))\n",
        "print()\n",
        "\n",
        "all_lens = {\n",
        "    'positive': [],\n",
        "    'negative': [],\n",
        "    'neutral': []\n",
        "}\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    all_lens[row['label']].append(len(row['article']))\n",
        "print(\"General dataset size: \" + str(len(data)))\n",
        "\n",
        "print(\"\\nPositive Sentiment Mean Article Length: \" + str(sum(all_lens['positive'])/len(all_lens['positive'])))\n",
        "print(\"Positive Sentiment Max Article Length: \" + str(max(all_lens['positive'])))\n",
        "print()\n",
        "print(\"Neutral Sentiment Mean Article Length: \" + str(sum(all_lens['neutral'])/len(all_lens['neutral'])))\n",
        "print(\"Neutral Sentiment Max Article Length: \" + str(max(all_lens['neutral'])))\n",
        "print()\n",
        "print(\"Negative Sentiment Mean Article Length: \" + str(sum(all_lens['negative'])/len(all_lens['negative'])))\n",
        "print(\"Negative Sentiment Max Article Length: \" + str(max(all_lens['negative'])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall distribution of the data and labels:\n",
            "negative     604\n",
            "positive    1363\n",
            "neutral     2879\n",
            "Name: label, dtype: int64\n",
            "\n",
            "General dataset size: 4846\n",
            "\n",
            "Positive Sentiment Mean Article Length: 135.64783565663976\n",
            "Positive Sentiment Max Article Length: 298\n",
            "\n",
            "Neutral Sentiment Mean Article Length: 125.07224730809308\n",
            "Neutral Sentiment Max Article Length: 315\n",
            "\n",
            "Negative Sentiment Mean Article Length: 125.75662251655629\n",
            "Negative Sentiment Max Article Length: 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC3riTPlMzan"
      },
      "source": [
        "## Have a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvpwUFhMzan",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623b7ad8-7094-47ef-fbdc-f94023b671b9"
      },
      "source": [
        "print('Article:')\n",
        "print(data.iloc[13].article + \"\\n\")\n",
        "print('Label:')\n",
        "print(data.iloc[13].label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article:\n",
            "Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\n",
            "\n",
            "Label:\n",
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m91CGvaCWnjc",
        "outputId": "cd901350-fc49-48b5-e746-581d655aa4cb"
      },
      "source": [
        "label_names = list(set(data['label'].values))\n",
        "label_names.sort()\n",
        "\n",
        "print(\"All possible labels: \" + str(label_names))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All possible labels: ['negative', 'neutral', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcQOnkcvMzan"
      },
      "source": [
        "### Test Data if BERT cased or uncased makes sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuqn6Oc2Mzao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f718e3a0-8464-4228-dded-8a314f59889f"
      },
      "source": [
        "uppercased_words_found = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    temp_article = row['article']\n",
        "    \n",
        "    for word in temp_article:\n",
        "        if len(word)>1 and word == word.upper():\n",
        "            uppercased_words_found.append(word)\n",
        "            \n",
        "print(uppercased_words_found)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_89-2sLiMzao"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "606mN-qUMzao"
      },
      "source": [
        "# normalize whitespace\n",
        "data['article'] = data['article'].apply(lambda x: \" \".join(x.split()))\n",
        "\n",
        "# remove punctuations except ?\n",
        "data['article'] = data['article'].apply(lambda x: re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLYjtJIMzap"
      },
      "source": [
        "## Convert labels to more understandable 0, 1, 2 format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIT7oXs_Mzap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "cbe8b7fa-9e47-43b9-8fce-d05bacb3329d"
      },
      "source": [
        "new_labels = {\n",
        "    'positive': 2,\n",
        "    'negative': 0,\n",
        "    'neutral': 1\n",
        "}\n",
        "for index, row in data.iterrows():\n",
        "  data['label'].iloc[index] = new_labels[row['label']]\n",
        "data.head(2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>According to Gran  ,  the company has no plans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            article\n",
              "0     1  According to Gran  ,  the company has no plans...\n",
              "1     1  Technopolis plans to develop in stages an area..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ErARovQHVgA"
      },
      "source": [
        "## Build Pseudo Labeling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgjsgVpfHbI8"
      },
      "source": [
        "def get_dataclass_distribution_of_unlabeled_data(data_class_size, X, y, random_state):\n",
        "  labeled_data_distribution = data_class_size/len(X)\n",
        "\n",
        "  X_unlabeled, X_labeled, y_unlabeled, y_labeled = train_test_split(X, y, test_size=labeled_data_distribution, random_state=random_state)\n",
        "  len(X_labeled)\n",
        "  return {\n",
        "      'labeled_data': X_labeled,\n",
        "      'labeled_data_labels': y_labeled,\n",
        "      'unlabeled_data': X_unlabeled,\n",
        "      'unlabeled_data_labels': y_unlabeled\n",
        "  }"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpqv6yEqMzap"
      },
      "source": [
        "## Set tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc5vTsC1Mzap"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Uncased -> Because case may express the sentiment but no uppercase word are given in the dataset as mentioned earlier"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMinlZ7PMzap"
      },
      "source": [
        "### Choose maximum token length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ3EHTwyMzaq",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c21ceac4-d0eb-4721-dab4-1dee5fddbd62"
      },
      "source": [
        "token_lens = []\n",
        "for txt in data.article:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))\n",
        "    \n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW2I38KQMzaq"
      },
      "source": [
        "MAX_LEN = 100\n",
        "BATCH_SIZE = 8\n",
        "SEED = 1210"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcuqFPNNMzaq"
      },
      "source": [
        "# Split data into training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRjOsFIKMzaq"
      },
      "source": [
        "def split_data_into_train_test_val(X, y, val_size, random_state):\n",
        "    X_reduced, X_test, y_reduced, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_reduced, y_reduced, test_size=val_size, random_state=random_state, stratify=y_reduced)\n",
        "    \n",
        "    return {\n",
        "        'train': {\n",
        "            'X': list(X_train),\n",
        "            'y': list(y_train)\n",
        "        },\n",
        "        'val': {\n",
        "            'X': list(X_val),\n",
        "            'y': list(y_val)\n",
        "        },\n",
        "        'test': {\n",
        "            'X': list(X_test),\n",
        "            'y': list(y_test)\n",
        "        }\n",
        "    }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijpna4dYMzaq",
        "scrolled": true
      },
      "source": [
        "split_data = split_data_into_train_test_val(data['article'].values, data['label'].values, val_size=0.2, random_state=SEED)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpvWmDwhMzar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb0c856-3464-4767-c7d7-cdec6fda29fc"
      },
      "source": [
        "X_train = split_data['train']['X']\n",
        "X_test = split_data['test']['X']\n",
        "X_val = split_data['val']['X']\n",
        "y_train = split_data['train']['y']\n",
        "y_test = split_data['test']['y']\n",
        "y_val = split_data['val']['y']\n",
        "\n",
        "import collections, numpy\n",
        "\n",
        "\n",
        "\n",
        "print(\"Distribution train: \")\n",
        "print(collections.Counter(y_train))\n",
        "print(\"\\nDistribution val: \")\n",
        "print(collections.Counter(y_val))\n",
        "print(\"\\nDistribution test: \")\n",
        "print(collections.Counter(y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution train: \n",
            "Counter({1: 2072, 2: 982, 0: 434})\n",
            "\n",
            "Distribution val: \n",
            "Counter({1: 519, 2: 245, 0: 109})\n",
            "\n",
            "Distribution test: \n",
            "Counter({1: 288, 2: 136, 0: 61})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32Ndw0wMzar"
      },
      "source": [
        "### Build data class for pytorch-Dataloader usage (Map-style dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxuAYV89Mzar"
      },
      "source": [
        "class BertData():\n",
        "    def __init__(self, article, label):\n",
        "        self.article = article\n",
        "        self.label = label\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.article)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        article = str(self.article[idx])\n",
        "        article = ' '.join(article.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            article,\n",
        "            None,\n",
        "            add_special_tokens = True,\n",
        "            max_length = MAX_LEN,\n",
        "            padding='max_length',\n",
        "            truncation='longest_first'\n",
        "        )\n",
        "\n",
        "        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
        "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
        "        labels = torch.tensor(self.label[idx], dtype=torch.long)\n",
        "\n",
        "        return {'article': article,\n",
        "                'ids': ids,\n",
        "                'mask': mask,\n",
        "                'token_type_ids': token_type_ids,\n",
        "                'targets': labels\n",
        "                }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzGprxRZh9"
      },
      "source": [
        "training_set = BertData(article=X_train, label=y_train)\n",
        "validation_set = BertData(article=X_val, label=y_val)\n",
        "test_set = BertData(article=X_test, label=y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzUqxLvuMzas"
      },
      "source": [
        "### Set pytorch data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NQd1PdkMzas"
      },
      "source": [
        "def get_dataloader(training_set, validation_set, test_set):\n",
        "    train_dataloader = DataLoader(\n",
        "            training_set,\n",
        "            shuffle=True,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_workers=4\n",
        "            )\n",
        "    val_dataloader = DataLoader(\n",
        "            validation_set,\n",
        "            shuffle=True,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_workers=4\n",
        "            )\n",
        "    test_dataloader = DataLoader(\n",
        "            test_set,\n",
        "            shuffle=True,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_workers=4\n",
        "            )\n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3dkrCVMzas"
      },
      "source": [
        "# BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYDrusWdMzat"
      },
      "source": [
        "## Build PyTorch Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUL8xhXYMzat"
      },
      "source": [
        "#train_dataloader, val_dataloader, test_dataloader = get_dataloader(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyE_LGsMzat"
      },
      "source": [
        "## Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGsOiF2sMzat"
      },
      "source": [
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.2\n",
        "learning_rate = 3e-5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fa2zJvybBxc"
      },
      "source": [
        "## Define standard BERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFLpR6laMzat"
      },
      "source": [
        "class ClassicalBertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClassicalBertClassifier, self).__init__()\n",
        "        \n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        #self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "        # BERT-base has 768 Output dimensions --> Map to 3 -> negative, neutral, positive\n",
        "        D_in, H, D_out = 768, 100, 3\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            #nn.ReLU(),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        #self.all_targets = []\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids) -> torch.Tensor:\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ezNBumeXSn"
      },
      "source": [
        "# MixTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwPSGTb2bIA_"
      },
      "source": [
        "## Define MixText BERT Classifier (According to publishers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bGpO-XebIX_"
      },
      "source": [
        "class BertModel4Mix(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertModel4Mix, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder4Mix(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _resize_token_embeddings(self, new_num_tokens):\n",
        "        old_embeddings = self.embeddings.word_embeddings\n",
        "        new_embeddings = self._get_resized_embeddings(\n",
        "            old_embeddings, new_num_tokens)\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(self, input_ids,  input_ids2=None, l=None, mix_layer=1000, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            if input_ids2 is not None:\n",
        "                attention_mask2 = torch.ones_like(input_ids2)\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "            if input_ids2 is not None:\n",
        "                token_type_ids2 = torch.zeros_like(input_ids2)\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "\n",
        "            extended_attention_mask2 = attention_mask2.unsqueeze(\n",
        "                1).unsqueeze(2)\n",
        "\n",
        "            extended_attention_mask2 = extended_attention_mask2.to(\n",
        "                dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "            extended_attention_mask2 = (\n",
        "                1.0 - extended_attention_mask2) * -10000.0\n",
        "\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(\n",
        "                    0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand(\n",
        "                    self.config.num_hidden_layers, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                # We can specify head_mask for each layer\n",
        "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "            # switch to fload if need + fp16 compatibility\n",
        "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype)\n",
        "        else:\n",
        "            head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "            embedding_output2 = self.embeddings(\n",
        "                input_ids2, position_ids=position_ids, token_type_ids=token_type_ids2)\n",
        "\n",
        "        if input_ids2 is not None:\n",
        "            encoder_outputs = self.encoder(embedding_output, embedding_output2, l, mix_layer,\n",
        "                                           extended_attention_mask, extended_attention_mask2, head_mask=head_mask)\n",
        "        else:\n",
        "            encoder_outputs = self.encoder(\n",
        "                embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        # add hidden_states and attentions if they are here\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
        "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        return outputs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa6eFazbgbx"
      },
      "source": [
        "## Define BERT Encoder for Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkNgukRjbmOs"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class BertEncoder4Mix(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder4Mix, self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.layer = nn.ModuleList([BertLayer(config)\n",
        "                                    for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, hidden_states2=None, l=None, mix_layer=1000, attention_mask=None, attention_mask2=None, head_mask=None):\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "\n",
        "        # Perform mix at till the mix_layer\n",
        "        if mix_layer == -1:\n",
        "            if hidden_states2 is not None:\n",
        "                hidden_states = l * hidden_states + (1-l)*hidden_states2\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if i <= mix_layer:\n",
        "\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states, attention_mask, head_mask[i])\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "                if hidden_states2 is not None:\n",
        "                    layer_outputs2 = layer_module(\n",
        "                        hidden_states2, attention_mask2, head_mask[i])\n",
        "                    hidden_states2 = layer_outputs2[0]\n",
        "\n",
        "            if i == mix_layer:\n",
        "                if hidden_states2 is not None:\n",
        "                    hidden_states = l * hidden_states + (1-l)*hidden_states2\n",
        "\n",
        "            if i > mix_layer:\n",
        "                if self.output_hidden_states:\n",
        "                    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states, attention_mask, head_mask[i])\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "                if self.output_attentions:\n",
        "                    all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        # last-layer hidden state, (all hidden states), (all attentions)\n",
        "        return outputs\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ4SBdmxbpua"
      },
      "source": [
        "## Define Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOide6Vbrqg"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class MixText(nn.Module):\n",
        "    def __init__(self, num_labels=2, mix_option=False):\n",
        "        super(MixText, self).__init__()\n",
        "\n",
        "        if mix_option:\n",
        "            self.bert = BertModel4Mix.from_pretrained('bert-base-uncased')\n",
        "        else:\n",
        "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(768, 128),\n",
        "                                    nn.Tanh(),\n",
        "                                    nn.Linear(128, num_labels))\n",
        "\n",
        "    def forward(self, x, x2=None, l=None, mix_layer=1000):\n",
        "\n",
        "        if x2 is not None:\n",
        "            all_hidden, pooler = self.bert(x, x2, l, mix_layer)\n",
        "\n",
        "            pooled_output = torch.mean(all_hidden, 1)\n",
        "\n",
        "        else:\n",
        "            all_hidden, pooler = self.bert(x)\n",
        "\n",
        "            pooled_output = torch.mean(all_hidden, 1)\n",
        "\n",
        "        predict = self.linear(pooled_output)\n",
        "\n",
        "        return predict"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9CLe9PbyqG"
      },
      "source": [
        "## Define Mix Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot2qTP6sb2mt"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "def mix_validate(valloader, model, criterion, epoch, mode):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_total = 0\n",
        "        total_sample = 0\n",
        "        acc_total = 0\n",
        "        correct = 0\n",
        "        overall_targets = []\n",
        "        overall_preds = []\n",
        "\n",
        "        for batch_idx, (inputs, targets, length) in enumerate(valloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                print(\"Sample some true labeles and predicted labels\")\n",
        "                print(predicted[:20])\n",
        "                print(targets[:20])\n",
        "\n",
        "            new_targets = [float(target) for target in targets]\n",
        "            new_targets[0:0] = overall_targets\n",
        "            overall_targets = new_targets\n",
        "\n",
        "            new_preds = [float(pred) for pred in predicted]\n",
        "            new_preds[0:0] = overall_preds\n",
        "            overall_preds = new_preds\n",
        "\n",
        "            correct += (np.array(predicted.cpu()) ==\n",
        "                        np.array(targets.cpu())).sum()\n",
        "            loss_total += loss.item() * inputs.shape[0]\n",
        "            total_sample += inputs.shape[0]\n",
        "\n",
        "        acc_total = correct/total_sample\n",
        "        loss_total = loss_total/total_sample\n",
        "\n",
        "        precision = accuracy_score(overall_targets, overall_preds)\n",
        "        recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "        f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "    return loss_total, precision, recall, f1\n",
        "\n",
        "def linear_rampup(current, rampup_length=EPOCHS):\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
        "        return float(current)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W9fZSmS0LIC"
      },
      "source": [
        "## Define translation model for MixText augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCV-ASmL0Th5"
      },
      "source": [
        "from easynmt import EasyNMT\n",
        "TRANSLATION_MODEL = EasyNMT('mbart50_m2m') # von Facebook reasearch"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opPjR9d_cJaC"
      },
      "source": [
        "## Define Loss for Mix Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms_pSP5PcLfg"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "class SemiLoss(object):\n",
        "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, outputs_u_2, epoch, mixed=1):\n",
        "        temp_change = 1000000\n",
        "        T = 0.5\n",
        "        co = False\n",
        "        alpha = 0.75\n",
        "        seperate_mix = False\n",
        "        mix_layers_set = [0, 1, 2, 3]\n",
        "        mix_method = 0\n",
        "        margin = 0.7\n",
        "        lambda_u_hinge = 1\n",
        "        lambda_u = 1\n",
        "\n",
        "        if mix_method == 0 or mix_method == 1:\n",
        "\n",
        "            Lx = - \\\n",
        "                torch.mean(torch.sum(F.log_softmax(\n",
        "                    outputs_x, dim=1) * targets_x, dim=1))\n",
        "\n",
        "            probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "            Lu = F.kl_div(probs_u.log(), targets_u, None, None, 'batchmean')\n",
        "\n",
        "            Lu2 = torch.mean(torch.clamp(torch.sum(-F.softmax(outputs_u, dim=1)\n",
        "                                                   * F.log_softmax(outputs_u, dim=1), dim=1) - margin, min=0))\n",
        "\n",
        "        elif mix_method == 2:\n",
        "            if mixed == 0:\n",
        "                Lx = - \\\n",
        "                    torch.mean(torch.sum(F.logsigmoid(\n",
        "                        outputs_x) * targets_x, dim=1))\n",
        "\n",
        "                probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "                Lu = F.kl_div(probs_u.log(), targets_u,\n",
        "                              None, None, 'batchmean')\n",
        "\n",
        "                Lu2 = torch.mean(torch.clamp(margin - torch.sum(\n",
        "                    F.softmax(outputs_u_2, dim=1) * F.softmax(outputs_u_2, dim=1), dim=1), min=0))\n",
        "            else:\n",
        "                Lx = - \\\n",
        "                    torch.mean(torch.sum(F.log_softmax(\n",
        "                        outputs_x, dim=1) * targets_x, dim=1))\n",
        "\n",
        "                probs_u = torch.softmax(outputs_u, dim=1)\n",
        "                Lu = F.kl_div(probs_u.log(), targets_u,\n",
        "                              None, None, 'batchmean')\n",
        "\n",
        "                Lu2 = torch.mean(torch.clamp(margin - torch.sum(\n",
        "                    F.softmax(outputs_u, dim=1) * F.softmax(outputs_u, dim=1), dim=1), min=0))\n",
        "\n",
        "        return Lx, Lu, lambda_u * linear_rampup(epoch), Lu2, lambda_u_hinge * linear_rampup(epoch)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDI95MW1eize"
      },
      "source": [
        "## Define loaders for MixText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUamR2nkemf0"
      },
      "source": [
        "temp_change = 1000000\n",
        "T = 0.5\n",
        "co = False\n",
        "alpha = 0.75\n",
        "seperate_mix = False\n",
        "mix_layers_set = [0, 1, 2, 3]\n",
        "mix_method = 0\n",
        "margin = 0.7\n",
        "lambda_u_hinge = 1\n",
        "mix_path = '/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/'\n",
        "\n",
        "# define augmentation translator \n",
        "class Translator:\n",
        "    \"\"\"Backtranslation. Here to save time, we pre-processing and save all the translated data into pickle files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, transform_type='BackTranslation'):\n",
        "        # Translator for eng to de to eng\n",
        "        self.translator = TRANSLATION_MODEL\n",
        "\n",
        "    def __call__(self, ori):\n",
        "        out1 = self.translator.translate(self.translator.translate(str(ori), target_lang='de'), target_lang='en')\n",
        "        out2 = self.translator.translate(self.translator.translate(str(ori), target_lang='ru'), target_lang='en')\n",
        "        return out1, out2, ori\n",
        "        \n",
        "class mixtext_loader_labeled():\n",
        "    # Data loader for labeled data\n",
        "    def __init__(self, dataset_text, dataset_label, tokenizer, max_seq_len, aug=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.labels = dataset_label\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.aug = aug\n",
        "        self.trans_dist = {}\n",
        "\n",
        "        if aug:\n",
        "            print('Aug train data by back translation of German')\n",
        "            self.en2de = torch.hub.load(\n",
        "                'pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
        "            self.de2en = torch.hub.load(\n",
        "                'pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def augment(self, text):\n",
        "        if text not in self.trans_dist:\n",
        "            self.trans_dist[text] = self.de2en.translate(self.en2de.translate(\n",
        "                text,  sampling=True, temperature=0.9),  sampling=True, temperature=0.9)\n",
        "        return self.trans_dist[text]\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug:\n",
        "            text = self.text[idx]\n",
        "            text_aug = self.augment(text)\n",
        "            text_result, text_length = self.get_tokenized(text)\n",
        "            text_result2, text_length2 = self.get_tokenized(text_aug)\n",
        "            return ((torch.tensor(text_result), torch.tensor(text_result2)), (self.labels[idx], self.labels[idx]), (text_length, text_length2))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "            if len(tokens) > self.max_seq_len:\n",
        "                tokens = tokens[:self.max_seq_len]\n",
        "            length = len(tokens)\n",
        "            encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "            encode_result += padding\n",
        "            return (torch.tensor(encode_result), self.labels[idx], length)\n",
        "\n",
        "\n",
        "class mixtext_loader_unlabeled():\n",
        "    # Data loader for unlabeled data\n",
        "    def __init__(self, dataset_text, tokenizer, max_seq_len, aug=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.ids = [n for n in range (0, len(dataset_text)+1)]\n",
        "        self.aug = aug\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug is not None:\n",
        "            u, v, ori = self.aug(self.text[idx])\n",
        "            encode_result_u, length_u = self.get_tokenized(u)\n",
        "            encode_result_v, length_v = self.get_tokenized(v)\n",
        "            encode_result_ori, length_ori = self.get_tokenized(ori)\n",
        "            return ((torch.tensor(encode_result_u), torch.tensor(encode_result_v), torch.tensor(encode_result_ori)), (length_u, length_v, length_ori))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            encode_result, length = self.get_tokenized(text)\n",
        "            return (torch.tensor(encode_result), length)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvakzoK-cQuP"
      },
      "source": [
        "# Set up MixTest default parameters setp up by the publishers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shjAaQwccXYb"
      },
      "source": [
        "# recommended default sets by publisher of MixText\n",
        "temp_change = 1000000\n",
        "T = 0.5\n",
        "co = False\n",
        "alpha = 0.75\n",
        "seperate_mix = False\n",
        "mix_layers_set = [0, 1, 2, 3]\n",
        "mix_method = 0\n",
        "margin = 0.7\n",
        "lambda_u_hinge = 1\n",
        "mix_path = '/content/drive/My Drive/Colab Notebooks/Data/MixTextModels/'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzl2n2sgMzau"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYnMDrgtzaWt"
      },
      "source": [
        "## Define traditional training epoch (supervised)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dm68sAkMzau"
      },
      "source": [
        "def traditional_train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples):\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"ids\"].to(device)\n",
        "    attention_mask = d[\"mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "    \n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    new_targets = [float(target) for target in targets]\n",
        "    new_targets[0:0] = overall_targets\n",
        "    overall_targets = new_targets\n",
        "\n",
        "    new_preds = [float(pred) for pred in preds]\n",
        "    new_preds[0:0] = overall_preds\n",
        "    overall_preds = new_preds\n",
        "\n",
        "    # set grad to 0\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # avoid exploding gradient\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  precision = accuracy_score(overall_targets, overall_preds)\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "\n",
        "  return precision, recall, f1, np.mean(losses)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFMWLRx41d-4"
      },
      "source": [
        "## Define traditional eval epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr60XpqfPca-"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  pseudo_labels = []\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"ids\"].to(device)\n",
        "      attention_mask = d[\"mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids)\n",
        "      \n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      new_targets = [float(target) for target in targets]\n",
        "      new_targets[0:0] = overall_targets\n",
        "      overall_targets = new_targets\n",
        "\n",
        "      new_preds = [float(pred) for pred in preds]\n",
        "      new_preds[0:0] = overall_preds\n",
        "      overall_preds = new_preds\n",
        "\n",
        "      for item in preds:\n",
        "        pseudo_labels.append(item.item())\n",
        "  frequency = {}\n",
        "\n",
        "  # iterating over the list\n",
        "  for item in pseudo_labels:\n",
        "  # checking the element in dictionary\n",
        "    if item in frequency:\n",
        "      # incrementing the counr\n",
        "      frequency[item] += 1\n",
        "    else:\n",
        "    # initializing the count\n",
        "      frequency[item] = 1\n",
        "\n",
        "  # printing the frequency\n",
        "  print(frequency)\n",
        "  \n",
        "  precision = accuracy_score(overall_targets, overall_preds)\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "\n",
        "  return precision, recall, f1, np.mean(losses)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdIM8YECp3Bn"
      },
      "source": [
        "## Define Pseudo Labeling Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5IJhJRJp5ui"
      },
      "source": [
        "def generate_pseudo_label(unlabeled_dataset, model):\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "  \n",
        "  pseudo_labels = []\n",
        "\n",
        "  for unlabeled_article in unlabeled_dataset:\n",
        "    # prepare data from model\n",
        "    inputs = tokenizer.encode_plus(\n",
        "            unlabeled_article,\n",
        "            max_length=MAX_LEN,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    token_type_ids = inputs['token_type_ids'].to(device)\n",
        "\n",
        "    # calculate model outputs\n",
        "    output = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "    \n",
        "    # calculate pseudo labels\n",
        "    _, pred = torch.max(output, dim=1)\n",
        "    #print(output)\n",
        "    #print(pred.item())\n",
        "    #print()\n",
        "\n",
        "    pseudo_labels.append(pred.item())\n",
        "  \n",
        "  return pseudo_labels"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GABnMc4a10tP"
      },
      "source": [
        "## Define Naive Semi supervised deep learning training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so8Bu_QR17x8"
      },
      "source": [
        "def nssdl_train_epoch(\n",
        "    model,\n",
        "    pseudo_label_dataloader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    n_examples,\n",
        "    unlabeled_dataset):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  overall_targets = []\n",
        "  overall_preds = []\n",
        "\n",
        "  for d in pseudo_label_dataloader:\n",
        "    input_ids = d[\"ids\"].to(device)\n",
        "    attention_mask = d[\"mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    token_type_ids = d['token_type_ids'].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids)\n",
        "    \n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    new_targets = [float(target) for target in targets]\n",
        "    new_targets[0:0] = overall_targets\n",
        "    overall_targets = new_targets\n",
        "\n",
        "    new_preds = [float(pred) for pred in preds]\n",
        "    new_preds[0:0] = overall_preds\n",
        "    overall_preds = new_preds\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # avoid exploding gradient\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  # create new pseudo labels for next iteration\n",
        "\n",
        "  # 1. guess pseudo label\n",
        "  # 2. BertData()\n",
        "  # 3. DataLoader(BertData(PL))\n",
        "  pseudo_labels = generate_pseudo_label(unlabeled_dataset=unlabeled_dataset, \n",
        "                                        model=model)\n",
        "\n",
        "  # Convert to BertData\n",
        "  pseudo_labeled_dataset = BertData(article=unlabeled_dataset, \n",
        "                                    label=pseudo_labels)\n",
        "\n",
        "  # Create PseudoLabel DataLoader\n",
        "  new_pseudo_label_dataloader = DataLoader(\n",
        "            pseudo_labeled_dataset,\n",
        "            shuffle=True,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_workers=4\n",
        "            )\n",
        "\n",
        "  precision = accuracy_score(overall_targets, overall_preds)\n",
        "  recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "  f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "\n",
        "  return precision, recall, f1, np.mean(losses), new_pseudo_label_dataloader"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBzofDQZ18Ey"
      },
      "source": [
        "## Define Naive Semi supervised deep eval training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzPEI95Y1-Zy"
      },
      "source": [
        "# Sharing same eval loader as traditional approach"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpqoPt5_qk-"
      },
      "source": [
        "## Define MixText Model Class and Train Epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhrIUD4__0y7"
      },
      "source": [
        "# @author: https://github.com/GT-SALT/MixText/blob/master/code\n",
        "def mixtext_train_epoch(labeled_trainloader, unlabeled_trainloader, model, optimizer, scheduler, criterion, epoch, n_labels, train_aug, len_labeled_data):\n",
        "  temp_change = 1000000\n",
        "  T = 0.5\n",
        "  co = False\n",
        "  alpha = 0.75\n",
        "  separate_mix = False\n",
        "  mix_layers_set = [0, 1, 2, 3]\n",
        "  mix_method = 0\n",
        "  margin = 0.7\n",
        "  lambda_u_hinge = 1\n",
        "  lambda_u = 1\n",
        "\n",
        "  train_aug=False\n",
        "  labeled_train_iter = iter(labeled_trainloader)\n",
        "  unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "  model.train()\n",
        "\n",
        "  val_iteration = len_labeled_data\n",
        "\n",
        "  global total_steps\n",
        "  global flag\n",
        "  if flag == 0 and total_steps > temp_change:\n",
        "      print('Change T!')\n",
        "      T = 0.9\n",
        "      flag = 1\n",
        "\n",
        "  for batch_idx in range(val_iteration):\n",
        "\n",
        "      total_steps += 1\n",
        "\n",
        "      if not train_aug:\n",
        "          try:\n",
        "              inputs_x, targets_x, inputs_x_length = labeled_train_iter.next()\n",
        "          except:\n",
        "              labeled_train_iter = iter(labeled_trainloader)\n",
        "              inputs_x, targets_x, inputs_x_length = labeled_train_iter.next()\n",
        "      else:\n",
        "          try:\n",
        "              (inputs_x, inputs_x_aug), (targets_x, _), (inputs_x_length,\n",
        "                                                          inputs_x_length_aug) = labeled_train_iter.next()\n",
        "          except:\n",
        "              labeled_train_iter = iter(labeled_trainloader)\n",
        "              (inputs_x, inputs_x_aug), (targets_x, _), (inputs_x_length,\n",
        "                                                          inputs_x_length_aug) = labeled_train_iter.next()\n",
        "      try:\n",
        "          (inputs_u, inputs_u2,  inputs_ori), (length_u,\n",
        "                                                length_u2,  length_ori) = unlabeled_train_iter.next()\n",
        "      except:\n",
        "          unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "          #print(unlabeled_train_iter.next()[0][0])\n",
        "          #print(unlabeled_train_iter.next())\n",
        "          (inputs_u, inputs_u2, inputs_ori), (length_u,\n",
        "                                              length_u2, length_ori) = unlabeled_train_iter.next()\n",
        "\n",
        "      batch_size = inputs_x.size(0)\n",
        "      batch_size_2 = inputs_ori.size(0)\n",
        "      targets_x = torch.zeros(batch_size, n_labels).scatter_(\n",
        "          1, targets_x.view(-1, 1), 1)\n",
        "\n",
        "      inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
        "      inputs_u = inputs_u.cuda()\n",
        "      inputs_u2 = inputs_u2.cuda()\n",
        "      inputs_ori = inputs_ori.cuda()\n",
        "\n",
        "      mask = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Predict labels for unlabeled data.\n",
        "          outputs_u = model(inputs_u)\n",
        "          outputs_u2 = model(inputs_u2)\n",
        "          outputs_ori = model(inputs_ori)\n",
        "\n",
        "          # Based on translation qualities, choose different weights here.\n",
        "          # For AG News: German: 1, Russian: 0, ori: 1\n",
        "          # For DBPedia: German: 1, Russian: 1, ori: 1\n",
        "          # For IMDB: German: 0, Russian: 0, ori: 1\n",
        "          # For Yahoo Answers: German: 1, Russian: 0, ori: 1 / German: 0, Russian: 0, ori: 1\n",
        "          p = (0 * torch.softmax(outputs_u, dim=1) + 0 * torch.softmax(outputs_u2,\n",
        "                                                                        dim=1) + 1 * torch.softmax(outputs_ori, dim=1)) / (1)\n",
        "          # Do a sharpen here.\n",
        "          pt = p**(1/T)\n",
        "          targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
        "          targets_u = targets_u.detach()\n",
        "\n",
        "      mixed = 1\n",
        "\n",
        "      if co:\n",
        "          mix_ = np.random.choice([0, 1], 1)[0]\n",
        "      else:\n",
        "          mix_ = 1\n",
        "\n",
        "      if mix_ == 1:\n",
        "          l = np.random.beta(alpha, alpha)\n",
        "          if separate_mix:\n",
        "              l = l\n",
        "          else:\n",
        "              l = max(l, 1-l)\n",
        "      else:\n",
        "          l = 1\n",
        "\n",
        "      mix_layer = np.random.choice(mix_layers_set, 1)[0]\n",
        "      mix_layer = mix_layer - 1\n",
        "\n",
        "      if not train_aug:\n",
        "          all_inputs = torch.cat(\n",
        "              [inputs_x, inputs_u, inputs_u2, inputs_ori, inputs_ori], dim=0)\n",
        "\n",
        "          all_lengths = torch.cat(\n",
        "              [inputs_x_length, length_u, length_u2, length_ori, length_ori], dim=0)\n",
        "\n",
        "          all_targets = torch.cat(\n",
        "              [targets_x, targets_u, targets_u, targets_u, targets_u], dim=0)\n",
        "\n",
        "      else:\n",
        "          all_inputs = torch.cat(\n",
        "              [inputs_x, inputs_x_aug, inputs_u, inputs_u2, inputs_ori], dim=0)\n",
        "          all_lengths = torch.cat(\n",
        "              [inputs_x_length, inputs_x_length, length_u, length_u2, length_ori], dim=0)\n",
        "          all_targets = torch.cat(\n",
        "              [targets_x, targets_x, targets_u, targets_u, targets_u], dim=0)\n",
        "\n",
        "      if separate_mix:\n",
        "          idx1 = torch.randperm(batch_size)\n",
        "          idx2 = torch.randperm(all_inputs.size(0) - batch_size) + batch_size\n",
        "          idx = torch.cat([idx1, idx2], dim=0)\n",
        "\n",
        "      else:\n",
        "          idx1 = torch.randperm(all_inputs.size(0) - batch_size_2)\n",
        "          idx2 = torch.arange(batch_size_2) + \\\n",
        "              all_inputs.size(0) - batch_size_2\n",
        "          idx = torch.cat([idx1, idx2], dim=0)\n",
        "\n",
        "      input_a, input_b = all_inputs, all_inputs[idx]\n",
        "      target_a, target_b = all_targets, all_targets[idx]\n",
        "      length_a, length_b = all_lengths, all_lengths[idx]\n",
        "\n",
        "      if mix_method == 0:\n",
        "          # Mix sentences' hidden representations\n",
        "          logits = model(input_a, input_b, l, mix_layer)\n",
        "          mixed_target = l * target_a + (1 - l) * target_b\n",
        "\n",
        "      elif mix_method == 1:\n",
        "          # Concat snippet of two training sentences, the snippets are selected based on l\n",
        "          # For example: \"I lova you so much\" and \"He likes NLP\" could be mixed as \"He likes NLP so much\".\n",
        "          # The corresponding labels are mixed with coefficient as well\n",
        "          mixed_input = []\n",
        "          if l != 1:\n",
        "              for i in range(input_a.size(0)):\n",
        "                  length1 = math.floor(int(length_a[i]) * l)\n",
        "                  idx1 = torch.randperm(int(length_a[i]) - length1 + 1)[0]\n",
        "                  length2 = math.ceil(int(length_b[i]) * (1-l))\n",
        "                  if length1 + length2 > 256:\n",
        "                      length2 = 256-length1 - 1\n",
        "                  idx2 = torch.randperm(int(length_b[i]) - length2 + 1)[0]\n",
        "                  try:\n",
        "                      mixed_input.append(\n",
        "                          torch.cat((input_a[i][idx1: idx1 + length1], torch.tensor([102]).cuda(), input_b[i][idx2:idx2 + length2], torch.tensor([0]*(256-1-length1-length2)).cuda()), dim=0).unsqueeze(0))\n",
        "                  except:\n",
        "                      print(256 - 1 - length1 - length2,\n",
        "                            idx2, length2, idx1, length1)\n",
        "\n",
        "              mixed_input = torch.cat(mixed_input, dim=0)\n",
        "\n",
        "          else:\n",
        "              mixed_input = input_a\n",
        "\n",
        "          logits = model(mixed_input)\n",
        "          mixed_target = l * target_a + (1 - l) * target_b\n",
        "\n",
        "      elif mix_method == 2:\n",
        "          # Concat two training sentences\n",
        "          # The corresponding labels are averaged\n",
        "          if l == 1:\n",
        "              mixed_input = []\n",
        "              for i in range(input_a.size(0)):\n",
        "                  mixed_input.append(\n",
        "                      torch.cat((input_a[i][:length_a[i]], torch.tensor([102]).cuda(), input_b[i][:length_b[i]], torch.tensor([0]*(512-1-int(length_a[i])-int(length_b[i]))).cuda()), dim=0).unsqueeze(0))\n",
        "\n",
        "              mixed_input = torch.cat(mixed_input, dim=0)\n",
        "              logits = model(mixed_input, sent_size=512)\n",
        "\n",
        "              #mixed_target = torch.clamp(target_a + target_b, max = 1)\n",
        "              mixed = 0\n",
        "              mixed_target = (target_a + target_b)/2\n",
        "          else:\n",
        "              mixed_input = input_a\n",
        "              mixed_target = target_a\n",
        "              logits = model(mixed_input, sent_size=256)\n",
        "              mixed = 1\n",
        "\n",
        "      Lx, Lu, w, Lu2, w2 = criterion(logits[:batch_size], mixed_target[:batch_size], logits[batch_size:-batch_size_2],\n",
        "                                      mixed_target[batch_size:-batch_size_2], logits[-batch_size_2:], epoch+batch_idx/val_iteration, mixed)\n",
        "\n",
        "      if mix_ == 1:\n",
        "          loss = Lx + w * Lu\n",
        "      else:\n",
        "          loss = Lx + w * Lu + w2 * Lu2\n",
        "\n",
        "      max_grad_norm = 1.0\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # scheduler.step()\n",
        "          \n",
        "      #precision = accuracy_score(overall_targets, overall_preds)\n",
        "      #recall = recall_score(overall_targets, overall_preds, average='macro')\n",
        "      #f1 = f1_score(overall_targets, overall_preds, average='macro')\n",
        "\n",
        "      #return precision, recall, f1, np.mean(losses), new_pseudo_label_dataloader"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2gpM7HR-YUb"
      },
      "source": [
        "## Generic Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtVBC5SS-b6N"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def run_model_training(kind, model, train_dataloader, val_dataloader, loss_fn, \n",
        "                       optimizer, scheduler, training_set, validation_set, \n",
        "                       unlabeled_train_set, unlabeled_scheduler, train_criterion ): # kind == 'traditional', 'NSSDL', 'MixText'\n",
        "  best_f1 = 0\n",
        "  history = defaultdict(list)\n",
        "  best_model = None\n",
        "\n",
        "  # for early stopping: save val_loss from last epoch\n",
        "  last_val_loss = None\n",
        "\n",
        "  pseudo_label_dataloader = None\n",
        "  ret_str = \"\"\n",
        "\n",
        "  # for precise analysis save pl acc/loss and finetune acc/loss as well\n",
        "  if kind == 'NSSDL':\n",
        "    train_acc_pl = 0\n",
        "    train_acc_fine_tune = 0\n",
        "    train_loss_pl = 0\n",
        "    train_loss_fine_tune = 0\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    if kind == 'traditional':\n",
        "      train_precision, train_recall, train_f1, train_loss = traditional_train_epoch(\n",
        "        model=model,\n",
        "        data_loader=train_dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        scheduler=scheduler,\n",
        "        n_examples=len(training_set))\n",
        "      \n",
        "    if kind == 'NSSDL':\n",
        "      if epoch == 0:\n",
        "        # train initial model with labeled data\n",
        "        temp_precision, temp_recall, temp_f1, temp_loss = traditional_train_epoch(\n",
        "                model=model,\n",
        "                data_loader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device,\n",
        "                scheduler=scheduler,\n",
        "                n_examples=len(training_set)\n",
        "              )\n",
        "        \n",
        "        # initial pseudo labeling\n",
        "        pseudo_labels = generate_pseudo_label(unlabeled_dataset=unlabeled_train_set, \n",
        "                                        model=model)\n",
        "        \n",
        "        frequency = {}\n",
        "\n",
        "        # iterating over the list\n",
        "        for item in pseudo_labels:\n",
        "        # checking the element in dictionary\n",
        "          if item in frequency:\n",
        "            # incrementing the count\n",
        "            frequency[item] += 1\n",
        "          else:\n",
        "          # initializing the count\n",
        "            frequency[item] = 1\n",
        "\n",
        "        # printing the frequency\n",
        "        print(frequency)\n",
        "\n",
        "        # Convert to BertData\n",
        "        pseudo_labeled_dataset = BertData(article=unlabeled_train_set, \n",
        "                                          label=pseudo_labels)\n",
        "\n",
        "        # Create PseudoLabel DataLoader\n",
        "        pseudo_label_dataloader = DataLoader(\n",
        "                  pseudo_labeled_dataset,\n",
        "                  shuffle=False,\n",
        "                  batch_size=BATCH_SIZE,\n",
        "                  num_workers=4\n",
        "                  )\n",
        "        \n",
        "        # TODO: ++++++++++ initial pseudo labeling\n",
        "        print(f'Initial acc: {temp_precision} Initial loss: {temp_loss}' )\n",
        "        \n",
        "      # first train with pseudo labels\n",
        "      train_precision_pl, train_recall_pl, train_f1_pl, train_loss_pl, pseudo_label_dataloader = nssdl_train_epoch(\n",
        "                model=model,\n",
        "                pseudo_label_dataloader=pseudo_label_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device,\n",
        "                scheduler=unlabeled_scheduler,\n",
        "                n_examples=len(unlabeled_train_set),\n",
        "                unlabeled_dataset=unlabeled_train_set\n",
        "              )\n",
        "\n",
        "      # fine tune model with labeled samples\n",
        "      train_precision_fine_tune, train_recall_fine_tune, train_f1_fine_tune, train_loss_fine_tune = traditional_train_epoch(\n",
        "                model=model,\n",
        "                data_loader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device,\n",
        "                scheduler=scheduler,\n",
        "                n_examples=len(training_set)\n",
        "              )\n",
        "\n",
        "      train_loss = float((train_loss_pl + train_loss_fine_tune)/2)\n",
        "      train_precision = float((float(train_precision_pl) + float(train_precision_fine_tune))/2)\n",
        "      train_recall = float((float(train_recall_pl) + float(train_recall_fine_tune))/2)\n",
        "      train_f1 = float((float(train_f1_pl) + float(train_f1_fine_tune))/2)\n",
        "      print(f\"FT-Precision/Accuracy: {train_precision_pl} FT-Recall: {train_recall_pl} FT-F1: {train_f1_pl} FT-Loss: {train_loss_pl}\")\n",
        "      print(f\"PL-Precision/Accuracy: {train_precision_fine_tune} PL-Recall: {train_recall_fine_tune} PL-F1: {train_f1_fine_tune} PL-Loss: {train_loss_fine_tune}\")\n",
        "\n",
        "    if kind == 'MixText':\n",
        "      print(\"Start epoch\")\n",
        "      mixtext_train_epoch(labeled_trainloader=train_dataloader,\n",
        "                          unlabeled_trainloader=unlabeled_train_set,\n",
        "                          model=model,\n",
        "                          optimizer=optimizer,\n",
        "                          scheduler=None,\n",
        "                          criterion=train_criterion,\n",
        "                          epoch=epoch,\n",
        "                          n_labels=3,\n",
        "                          train_aug=False,\n",
        "                          len_labeled_data=len(training_set))\n",
        "      print(\"Start train validate\")\n",
        "      train_loss, train_precision, train_recall, train_f1 = mix_validate(train_dataloader,\n",
        "                              model,  loss_fn, epoch, mode='Train Stats')\n",
        "      print(\"Start val validate\")\n",
        "      val_loss, val_precision, val_recall, val_f1 = mix_validate(\n",
        "          val_dataloader, model, loss_fn, epoch, mode='Valid Stats')\n",
        "    \n",
        "    print(f'({kind}) Train loss {train_loss}, accuracy {train_precision}, recall {train_recall}, f1 {train_f1}')\n",
        "\n",
        "    if kind != 'MixText':\n",
        "      val_precision, val_recall, val_f1, val_loss = eval_model(\n",
        "        model,\n",
        "        val_dataloader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(validation_set))\n",
        "\n",
        "    \n",
        "    print(f'({kind}) Val   loss {val_loss}, accuracy {val_precision}, recall {val_recall}, f1 {val_f1}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_precision)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_recall'].append(train_recall)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_acc'].append(val_precision)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_recall'].append(val_recall)\n",
        "    history['train_f1'].append(val_f1)\n",
        "\n",
        "    ret_str += f\"({kind}) Epoch {epoch+1}: \\n Training(acc: {round(train_precision, 4)}, recall: {train_recall}, f1: {train_f1}, loss: {round(train_loss, 4)}) \\n Validation(acc: {round(val_precision, 4)}, recall: {val_recall}, f1: {val_f1}, loss: {round(val_loss, 4)})\\n\"\n",
        "\n",
        "    if kind == 'NSSDL':\n",
        "      history['train_acc_pl'].append(train_precision_pl)\n",
        "      history['train_acc_fine_tune'].append(train_precision_fine_tune)\n",
        "      history['train_loss_pl'].append(train_loss_pl)\n",
        "      history['train_loss_fine_tune'].append(train_loss_fine_tune)\n",
        "\n",
        "      ret_str += f\"PL-Training(acc: {round(train_precision_pl, 4)}, recall: {train_recall_pl}, f1: {train_f1_pl}, loss: {round(train_loss_pl, 4)}) \\n Finetune-Training(acc: {round(train_precision_fine_tune, 4)}, recall: {train_recall_fine_tune}, f1: {train_f1_fine_tune}, loss: {round(train_loss_fine_tune, 4)})\\n\"\n",
        "\n",
        "    ret_str += \"\\n\"\n",
        "      \n",
        "    # early stopping:\n",
        "#    if (last_val_loss is not None) and (last_val_loss < val_loss):\n",
        " #     print(\"Early stopping executed due to increasing val_loss from \" + str(last_val_loss) + \" to \" + str(val_loss))\n",
        "  #    break\n",
        "\n",
        "    last_val_loss = val_loss\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "      best_model = model.state_dict()\n",
        "      torch.save(model.state_dict(), kind+'_best_model_state.bin')\n",
        "      best_f1 = val_f1\n",
        "  \n",
        "  ret_str += \"-----------------------------------------------------------------\"\n",
        "\n",
        "  return best_model, history, best_f1, ret_str"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfaRm4ZxGtm9"
      },
      "source": [
        "## Overall Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHeiGlkTGs7u",
        "outputId": "73a56702-5db8-4707-843a-1efce1b9f7e0"
      },
      "source": [
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "    \n",
        "FOLDS = 10\n",
        "RANDOM_STATES = ['1210', '505', '2506', '1807', '1402', '107', '1803', '2405', '208', '2209']\n",
        "\n",
        "overall_saving_dict = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "best_models = {\n",
        "    'trad': [],\n",
        "    'nssdl': [],\n",
        "    'mixmatch': []\n",
        "}\n",
        "\n",
        "\n",
        "test_txt_string = \"\"\n",
        "\n",
        "total_steps = 0\n",
        "flag = 0\n",
        "\n",
        "\n",
        "# set reasonable data manipulation classes\n",
        "data_manipultation_classes = [50, 100, 250, 500, 2000]\n",
        "\n",
        "for data_class in data_manipultation_classes:\n",
        "  print(\"Data class of \" + str(data_class) + \" labeled samples per fold\")\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "  for fold in range(FOLDS):\n",
        "    print(f'Fold {fold + 1}/{FOLDS}')\n",
        "    print('-' * 10)\n",
        "    print()\n",
        "\n",
        "    tradi_model = ClassicalBertClassifier()\n",
        "    tradi_model = tradi_model.to(device)\n",
        "\n",
        "    NSSDL_model = ClassicalBertClassifier()\n",
        "    NSSDL_model = NSSDL_model.to(device)\n",
        "\n",
        "    MixText_model = MixText(num_labels=3, mix_option=True)\n",
        "    MixText_model = nn.DataParallel(MixText_model)\n",
        "    MixText_model = MixText_model.to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # set random_state\n",
        "    random_state = int(str(data_manipultation_classes.index(data_class)+1) + RANDOM_STATES[fold])\n",
        "\n",
        "    # set labeled- and unlabeled data and train-, val-, test data\n",
        "    split_data = split_data_into_train_test_val(data['article'], data['label'], 0.2, random_state)\n",
        "\n",
        "    X_train = split_data['train']['X']\n",
        "    X_test = split_data['test']['X']\n",
        "    X_val = split_data['val']['X']\n",
        "\n",
        "    y_train = split_data['train']['y']\n",
        "    y_test = split_data['test']['y']\n",
        "    y_val = split_data['val']['y']\n",
        "\n",
        "    data_dict = get_dataclass_distribution_of_unlabeled_data(data_class_size=data_class,\n",
        "                                                             X=X_train, \n",
        "                                                             y=y_train, \n",
        "                                                             random_state=random_state)\n",
        "    \n",
        "    X_train_labeled = data_dict['labeled_data']\n",
        "    X_train_unlabeled = data_dict['unlabeled_data']\n",
        "\n",
        "    y_train_labeled = data_dict['labeled_data_labels']\n",
        "    y_train_unlabeled = data_dict['unlabeled_data_labels'] # for testing purposes (maybe)\n",
        "\n",
        "    # initialise BertData\n",
        "    training_set = BertData(article=X_train_labeled, label=y_train_labeled)\n",
        "    validation_set = BertData(article=X_val, label=y_val)\n",
        "    test_set = BertData(article=X_test, label=y_test)\n",
        "\n",
        "    # initialize dataloaders\n",
        "    train_dataloader, val_dataloader, test_dataloader = get_dataloader(training_set, \n",
        "                                                                       validation_set, \n",
        "                                                                       test_set)\n",
        "    \n",
        "    # for MixText\n",
        "    mix_train_loader = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_train_labeled,\n",
        "            dataset_label=y_train_labeled, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         shuffle=True)\n",
        "    \n",
        "    mix_train_unlabeled = DataLoader(\n",
        "        dataset=mixtext_loader_unlabeled(\n",
        "            dataset_text=X_train_unlabeled,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_len=MAX_LEN,\n",
        "            aug=Translator(mix_path)),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         shuffle=True)\n",
        "    \n",
        "    mix_val_loader = DataLoader(\n",
        "        dataset=mixtext_loader_labeled(\n",
        "            dataset_text=X_val,\n",
        "            dataset_label=y_val, \n",
        "            tokenizer=tokenizer, \n",
        "            max_seq_len=MAX_LEN\n",
        "            ),\n",
        "         batch_size=BATCH_SIZE, \n",
        "         shuffle=True)\n",
        "\n",
        "    # define optimizers, schedulers and loss function\n",
        "    tradi_optimizer = transformers.AdamW(tradi_model.parameters(), lr=learning_rate)\n",
        "    nssdl_optimizer = transformers.AdamW(NSSDL_model.parameters(), lr=learning_rate)\n",
        "    mix_optimizer = transformers.AdamW(MixText_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "    tradi_labeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      tradi_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    nssdl_labeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    nssdl_unlabeled_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "      nssdl_optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=(len(X_train_unlabeled)/BATCH_SIZE)*EPOCHS\n",
        "    )\n",
        "\n",
        "    train_criterion = SemiLoss()\n",
        "\n",
        "    # train mixtext\n",
        "    mixtext_best_model, mixtext_history, mixtext_best_f1, mixtext_str = run_model_training('MixText', \n",
        "                                                         MixText_model, \n",
        "                                                         mix_train_loader, \n",
        "                                                         mix_val_loader, \n",
        "                                                         loss_fn, \n",
        "                                                         mix_optimizer, \n",
        "                                                         None, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         mix_train_unlabeled,\n",
        "                                                         None,\n",
        "                                                         train_criterion)\n",
        "\n",
        "    # train trad\n",
        "    tradi_best_model, tradi_history, tradi_best_f1, tradi_str = run_model_training('traditional', \n",
        "                                                         tradi_model, \n",
        "                                                         train_dataloader, \n",
        "                                                         val_dataloader, \n",
        "                                                         loss_fn, \n",
        "                                                         tradi_optimizer, \n",
        "                                                         tradi_labeled_scheduler, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         X_train_unlabeled,\n",
        "                                                         None,\n",
        "                                                         None)\n",
        "\n",
        "\n",
        "    # train nssdl\n",
        "    nssdl_best_model, nssdl_history, nssdl_best_f1, nssdl_str = run_model_training('NSSDL', \n",
        "                                                         NSSDL_model, \n",
        "                                                         train_dataloader, \n",
        "                                                         val_dataloader, \n",
        "                                                         loss_fn, \n",
        "                                                         nssdl_optimizer, \n",
        "                                                         nssdl_labeled_scheduler, \n",
        "                                                         training_set, \n",
        "                                                         validation_set,\n",
        "                                                         X_train_unlabeled,\n",
        "                                                         nssdl_unlabeled_scheduler,\n",
        "                                                         None)\n",
        "    \n",
        "\n",
        "\n",
        "    # execute evaluation with test set\n",
        "    # traditional\n",
        "    trad = model.load_state_dict(tradi_best_model)\n",
        "\n",
        "    tradi_test_acc, tradi_test_loss = eval_model(\n",
        "      trad,\n",
        "      test_dataloader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(validation_set))\n",
        "    \n",
        "\n",
        "    \n",
        "    nssdl = model.load_state_dict(nssdl_best_model)\n",
        "\n",
        "    nssdl_test_acc, nssdl_test_loss = eval_model(\n",
        "      nssdl,\n",
        "      test_dataloader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(validation_set))\n",
        "\n",
        "    # ---> Log locally\n",
        "    test_txt_string += f\"Fold: {fold}\\n\\n\" + \\\n",
        "    f\"Traditional: Test(acc: {round(tradi_test_acc, 4)}, loss: {round(tradi_test_loss, 4)})\\n\"+\\\n",
        "    f\"NSSDL: Test(acc: {round(nssdl_test_acc, 4)}, loss: {round(nssdl_test_loss, 4)})\\n\\n+++++++++++++++++++++++++++++++++++\"\n",
        "\n",
        "    overall_saving_dict['trad'].append(tradi_history)\n",
        "    overall_saving_dict['nssdl'].append(nssdl_history)\n",
        "\n",
        "    with open(\"history.txt\", \"w\") as text_file:\n",
        "      append_str=f\"Fold: {fold}\\n\\n\"+\\\n",
        "      \"Traditional:\\n\"+\\\n",
        "      \"-----------\\n\" + tradi_str + \"\\n\\nNSSDL:\\n\"+\\\n",
        "      \"-----------\\n\" + nssdl_str + \"\\n\\n\\n+++++++++++++++++++++++++++++++++++\"\n",
        "      text_file.write(append_str)\n",
        "\n",
        "  with open(\"testing_history.txt\", \"w\") as text_file:\n",
        "        text_file.write(test_txt_string)\n",
        "\n",
        "\n",
        "  # calc mean of logged results\n",
        "\n",
        "  # significance testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data class of 50 labeled samples per fold\n",
            "--------------------------------------------------\n",
            "\n",
            "Fold 1/10\n",
            "----------\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "Start epoch\n",
            "Start train validate\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 1, 2, 2, 2, 1, 1, 2], device='cuda:0')\n",
            "Start val validate\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "(MixText) Train loss 0.8538915491104127, accuracy 0.58, recall 0.411764705882353, f1 0.36172591102168566\n",
            "(MixText) Val   loss 0.8872582107475123, accuracy 0.5945017182130584, recall 0.3333333333333333, f1 0.24856321839080456\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Start epoch\n",
            "Start train validate\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 2, 2, 1, 0, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 2, 2, 1, 0, 1, 2, 2], device='cuda:0')\n",
            "Start val validate\n",
            "Sample some true labeles and predicted labels\n",
            "tensor([2, 1, 2, 1, 1, 1, 2, 2], device='cuda:0')\n",
            "tensor([2, 1, 2, 1, 2, 1, 0, 1], device='cuda:0')\n",
            "(MixText) Train loss 0.28667395770549775, accuracy 0.98, recall 0.9803921568627452, f1 0.9702911467617351\n",
            "(MixText) Val   loss 0.9498887098941606, accuracy 0.5750286368843069, recall 0.4898330037853822, f1 0.4887911692952464\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Start epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRmOxXl1LYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNXbuBWF-mI_"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcAx_lP9Si2c"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_dataloader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(test_set)\n",
        ")\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkzkVJT2SqWc"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"article\"]\n",
        "      input_ids = d[\"ids\"].to(device)\n",
        "      attention_mask = d[\"mask\"].to(device)\n",
        "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        ids=input_ids,\n",
        "        mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsh1HoYHSuWU"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_dataloader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNbnvjrSuqz"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJfGxOmUYTi"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
        "\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmqT5sToXrcb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crte907NuVqx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}